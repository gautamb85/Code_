{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import htkmfc\n",
    "import random\n",
    "\n",
    "def data_dict(audio):\n",
    "\n",
    "    f=open(audio)\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    lines=f.readlines()\n",
    "    Data = {}\n",
    "    maxlen=600 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "\n",
    "    for l in lines:\n",
    "        l=l.strip()\n",
    "        fname = os.path.join(dpth,l+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        pad = np.zeros(((maxlen-data.shape[0]), 20), dtype='float32')\n",
    "        data = np.vstack((data,pad))\n",
    "        Data[l] = data\n",
    "        \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "au= '/misc/data15/reco/bhattgau/Rnn/Lists/all_audio.plst'\n",
    "data = data_dict(au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(au)\n",
    "utts = f.readlines()\n",
    "utts = [u.strip() for u in utts]\n",
    "all_ids = [t.split('_')[0] for t in utts]\n",
    "spk_ids = list(set([t.split('_')[0] for t in utts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we have 98 speaker models\n",
    "r = [u for u,i in enumerate(all_ids) if i == spk_ids[1]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "g = random.sample(r,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spk_ids\n",
    "\n",
    "for idx in spk_ids:\n",
    "\n",
    "    r = [u for u,i in enumerate(all_ids) if i == idx] #select speaker sesseion\n",
    "    g = random.sample(r,3) #select 3 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = random.sample(slot,3) #select 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "slots=[]\n",
    "maxlen=600\n",
    "feat_dim=20\n",
    "nSpk = 98\n",
    "\n",
    "ptr=0\n",
    "\n",
    "X_data1 = np.zeros((nSpk,maxlen,feat_dim))\n",
    "X_data2 = np.zeros((nSpk,maxlen,feat_dim))\n",
    "X_data3 = np.zeros((nSpk,maxlen,feat_dim))\n",
    "\n",
    "for idx in spk_ids:\n",
    "\n",
    "    for k,i in enumerate(data.keys()):\n",
    "        spk = i.split('_')[0]\n",
    "        if spk == idx:\n",
    "            slots.append(k)\n",
    "\n",
    "    slot = np.asarray(slots)          \n",
    "    l = random.sample(slot,3)\n",
    "\n",
    "    enr1 = data.keys()[l[0]]\n",
    "    enr2 = data.keys()[l[1]]\n",
    "    enr3 = data.keys()[l[2]]\n",
    "\n",
    "    en_data1 = data[enr1]\n",
    "    en_data2 = data[enr2]\n",
    "    en_data3 = data[enr3]\n",
    "    \n",
    "    X_data1[ptr,:,:] = en_data1\n",
    "    X_data2[ptr,:,:] = en_data2\n",
    "    X_data3[ptr,:,:] = en_data3\n",
    "    \n",
    "    ptr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.55846369, -1.25439584,  0.57914668, ...,  0.02282327,\n",
       "         0.00939691, -0.54287511],\n",
       "       [-1.54901123, -1.6017952 ,  0.32961532, ...,  0.94102347,\n",
       "         0.56644034, -0.49490228],\n",
       "       [-1.17332339, -0.91362154,  1.97719514, ...,  0.38444424,\n",
       "        -0.6826064 , -0.69916391],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data3[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ones = X_data3[97][:,1][np.nonzero(X_data3[97][:,1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93134129,  1.17644513, -0.79408729,  0.22635125,  0.18693383,\n",
       "       -0.51372385, -0.26633751,  0.50752729,  1.35270739,  0.67814296,\n",
       "        0.34120861,  0.20901446, -0.4524318 ,  0.4889394 , -1.71830893,\n",
       "       -0.22557743,  2.28878689,  0.62691939, -1.095837  , -0.92529982])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data3[97][186,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for idx in all_ids:\n",
    "    labels.append(spk_ids.index(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(Data, trials, batch_size, randomize=True):\n",
    "    \n",
    "    #audio : a text file that loads the corresponding data dictionary (either training or validation)\n",
    "    #trials : list of trials (either training or validation)\n",
    "\n",
    "    #batch_size\n",
    "    #load the data and the list of trials\n",
    "    if randomize:\n",
    "        enroll_s, test_s, label_s =  randomize_trials(trials)\n",
    "    else:\n",
    "        enroll_s, test_s, label_s =  randomize_trials(trials, value=False)\n",
    "    \n",
    "    # isolate the speaker model from the Data keys\n",
    "    spkr_ids = [k.split('_')[0] for k in Data.keys()]\n",
    "    #isolate the speaker model associated with every trial in the training list\n",
    "    enroll_mods = [l.split('_')[0] for l in enroll_s]\n",
    "    \n",
    "    #for testing\n",
    "    #enroll_mods = enroll_mods[:23]\n",
    "    \n",
    "    maxlen=600\n",
    "    f_dim =20\n",
    "    \n",
    "    ptr=0\n",
    "\n",
    "    while ptr < len(enroll_mods):\n",
    "        #keep slicing through the training list in mini-bath sized chunks\n",
    "        #dynamically change batch size if needed\n",
    "                \n",
    "        if len(enroll_mods) - ptr == len(enroll_mods)%batch_size:\n",
    "            batch_size = len(enroll_mods)%batch_size\n",
    "        \n",
    "        chunk = enroll_mods[ptr:ptr+batch_size]\n",
    "        t_chunk = test_s[ptr:ptr+batch_size]\n",
    "        \n",
    "        #remove empty items from this list\n",
    "        #chunk = chunk[:-1]\n",
    "        \n",
    "        #store every training chunk in a numpy matrix\n",
    "        enr_data1 = np.zeros((batch_size,maxlen, f_dim), dtype='float32') #enroll-1\n",
    "        enr_data2 = np.zeros((batch_size,maxlen, f_dim), dtype='float32') #enroll-2\n",
    "        enr_data3 = np.zeros((batch_size,maxlen, f_dim), dtype='float32') #enroll-3\n",
    "\n",
    "        TST_data = np.zeros((batch_size,maxlen, f_dim), dtype='float32') #test\n",
    "\n",
    "        #masks\n",
    "        enr_mask1 = np.zeros((batch_size,maxlen), dtype='float32') #enroll-1\n",
    "        enr_mask2 = np.zeros((batch_size,maxlen), dtype='float32') #enroll-2\n",
    "        enr_mask3 = np.zeros((batch_size,maxlen), dtype='float32') #enroll-3\n",
    "\n",
    "        tst_mask = np.zeros((batch_size,maxlen), dtype='float32') #test\n",
    "        \n",
    "        #labels\n",
    "        labs_l = label_s[ptr:ptr+batch_size]\n",
    "        labs = np.asarray(labs_l,'int32')\n",
    "\n",
    "        \n",
    "        for ind,(e,t) in enumerate(zip(chunk,t_chunk)):\n",
    "\n",
    "            #find the indices corresponding to the utterances of a given speaker model\n",
    "            inds = [i for i,x in enumerate(spkr_ids) if x==e]\n",
    "            #find the dictionary keys corresponding to these indices\n",
    "            c = [Data.keys()[i] for i in inds]\n",
    "            #check which utterance corresponds to the test utterance and remove it from the list\n",
    "            c_mod = [cm for cm in c if cm != test_s[ind]]\n",
    "            #sample 3 utterances to enroll the speaker model from this list\n",
    "            c_enr = random.sample(c_mod,3)\n",
    "                        \n",
    "            #get the corresponding utterances\n",
    "            enr_data = [Data[u] for u in c_enr]\n",
    "\n",
    "            tst_data = Data[t] \n",
    "                    \n",
    "            #generate the masks corresponding to these data points\n",
    "            #enroll\n",
    "            enr_mask1[ind,:enr_data[0].shape[0]] = 1.0\n",
    "            enr_mask2[ind,:enr_data[1].shape[0]] = 1.0       \n",
    "            enr_mask3[ind,:enr_data[2].shape[0]] = 1.0\n",
    "            #test\n",
    "            tst_mask[ind,:tst_data.shape[0]] = 1.0  \n",
    "\n",
    "            #pad the data (as utterances are of different length)\n",
    "            #enroll\n",
    "            pl1 = maxlen - enr_data[0].shape[0]\n",
    "            pl2 = maxlen - enr_data[1].shape[0]\n",
    "            pl3 = maxlen - enr_data[2].shape[0]\n",
    "            #test\n",
    "            plt = maxlen - tst_data.shape[0]\n",
    "\n",
    "            enr_p1 = np.vstack((enr_data[0],np.zeros((pl1,f_dim), dtype='float32')))\n",
    "            enr_p2 = np.vstack((enr_data[1],np.zeros((pl2,f_dim), dtype='float32')))       \n",
    "            enr_p3 = np.vstack((enr_data[2],np.zeros((pl3,f_dim), dtype='float32')))\n",
    "            tst_p = np.vstack((tst_data, np.zeros((plt,f_dim), dtype='float32')))\n",
    "            #build the minibatch\n",
    "            enr_data1[ind,:,:] = enr_p1\n",
    "            enr_data2[ind,:,:] = enr_p2\n",
    "            enr_data3[ind,:,:]= enr_p3\n",
    "            TST_data[ind,:,:] = tst_p\n",
    "                    \n",
    "        ptr+=batch_size\n",
    "        yield enr_data1,enr_data2,enr_data3,TST_data,enr_mask1,enr_mask2,enr_mask3,tst_mask, labs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
