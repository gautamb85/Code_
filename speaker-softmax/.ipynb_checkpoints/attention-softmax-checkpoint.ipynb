{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n"
     ]
    }
   ],
   "source": [
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Train_feats_labs.plst')\n",
    "    lines = f1.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    labelz = [int(l.split()[1]) for l in lines] \n",
    "    #labelz = labelz[:20]\n",
    "    features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    f2 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Valid_feats_labs.plst')\n",
    "    lines = f2.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    val_labelz = [int(l.split()[1]) for l in lines] \n",
    "    #val_labelz = val_labelz[:20]\n",
    "    val_features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    n_samp = len(features)\n",
    "    maxlen=800 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "    nSpk = 98\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    Data = np.zeros((n_samp, maxlen, feat_dim), dtype='float32')\n",
    "    Mask = np.zeros((n_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "    \n",
    "    vn_samp = len(val_features)\n",
    "    val_Data = np.zeros((vn_samp, maxlen, feat_dim), dtype='float32')\n",
    "    val_Mask = np.zeros((vn_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "\n",
    "    for ind,f in enumerate(features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        Data[ind,:,:] = data\n",
    "        \n",
    "\n",
    "    for ind,f in enumerate(val_features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        val_Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        val_Data[ind,:,:] = data\n",
    "\n",
    "\n",
    "    return Data, Mask, np.asarray(labelz, dtype='int32'), val_Data, val_Mask, np.asarray(val_labelz, dtype='int32')\n",
    "\n",
    "def iterate_minibatches(inputs, mask, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    iplen = len(inputs)\n",
    "    pointer = 0\n",
    "    indices=np.arange(iplen)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    while pointer < iplen:\n",
    "    \n",
    "        if pointer <= iplen - batchsize:\n",
    "            excerpt = indices[pointer:pointer+batchsize]\n",
    "        else:\n",
    "            batchsize = iplen-pointer\n",
    "            excerpt = indices[pointer:pointer+batchsize]\n",
    "        \n",
    "        pointer+=batchsize\n",
    "\n",
    "        yield inputs[excerpt], mask[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 600\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 100\n",
    "F_DIM = 20\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 1\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "nSpk = 98\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "target = T.matrix(name='target_values', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None, F_DIM), input_var = X)\n",
    "n_batch,maxlen,_ = l_in.input_var.shape\n",
    "\n",
    "#get the batch size for the weights matrix\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "#initialize the gates\n",
    "\n",
    "#Compute Recurrent Embeddings\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask, backwards=True)\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "#Collect all the recurrent embeddings\n",
    "Recc_emb =  lasagne.layers.get_output(l_sum, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask})\n",
    "\n",
    "#Calculate the last time-step of a recording using the mask.\n",
    "lstep = T.sum(Mask, axis=1)#.eval({Mask : mask})\n",
    "lstep = T.cast(lstep, 'int32')#.eval()\n",
    "lstep = lstep.T\n",
    "\n",
    "#Get the last time-step from the l-sum layer\n",
    "l_last = lasagne.layers.SliceLayer(l_sum, -1, 1)\n",
    "#print lasagne.layers.get_output(l_last, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "#Attention Model\n",
    "l_softmax = lasagne.layers.DenseLayer(l_last, num_units=MAX_LENGTH, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#l_out = lasagne.layers.ReshapeLayer(l_softmax, (n_batch, MAX_LENGTH, MAX_LENGTH))\n",
    "\n",
    "#These are the attention weights over all the timesteps of the padded utterance\n",
    "attn_wts = lasagne.layers.get_output(l_softmax, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask})\n",
    "\n",
    "Wts = T.zeros([n_batch, maxlen], dtype='float32')\n",
    "\n",
    "def weighted_avg(w_t, l_t, W, R_emb):\n",
    "    W = T.set_subtensor(W[:l_t], w_t[:l_t])\n",
    "    W = W[None,:]\n",
    "    w_sum = T.dot(W, R_emb)\n",
    "    \n",
    "    #this W (1,600) is is then multiplied (dot product) with the recurrent embedding (600, 100)\n",
    "    #to produce a feature vector that is a weighted sum of all the embedding vectors of the recording\n",
    "    #also return the weights for analysis\n",
    "    \n",
    "    return w_sum\n",
    "    \n",
    "U_t,_ = theano.scan(fn=weighted_avg, sequences=[attn_wts, lstep, Wts, Recc_emb])\n",
    "#We have calculated the weighted sum of the recurrent embeddings, based on the weights output by the attention model.\n",
    "#This will now be reshaped, and passed to a dense layer, which is a softmax over speakers.\n",
    "\n",
    "l_in2 = lasagne.layers.InputLayer(shape=(None,None, None), input_var = U_t)\n",
    "n_batch1,l1,hdim = l_in2.input_var.shape\n",
    "\n",
    "l_reshape2 = lasagne.layers.ReshapeLayer(l_in2, (n_batch1*l1,N_HIDDEN))\n",
    "#print lasagne.layers.get_output(l_reshape2, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "#Finally this feature vector(s) gets passed to a dense layer which represents a softmax distribution over speakers\n",
    "l_Spk_softmax = lasagne.layers.DenseLayer(l_reshape2, num_units=98, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#print lasagne.layers.get_output(l_Spk_softmax, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W_in_to_updategate, (20, 100))\n",
      "(W_hid_to_updategate, (100, 100))\n",
      "(b_updategate, (100,))\n",
      "(W_in_to_resetgate, (20, 100))\n",
      "(W_hid_to_resetgate, (100, 100))\n",
      "(b_resetgate, (100,))\n",
      "(W_in_to_hidden_update, (20, 100))\n",
      "(W_hid_to_hidden_update, (100, 100))\n",
      "(b_hidden_update, (100,))\n",
      "(W_in_to_updategate, (20, 100))\n",
      "(W_hid_to_updategate, (100, 100))\n",
      "(b_updategate, (100,))\n",
      "(W_in_to_resetgate, (20, 100))\n",
      "(W_hid_to_resetgate, (100, 100))\n",
      "(b_resetgate, (100,))\n",
      "(W_in_to_hidden_update, (20, 100))\n",
      "(W_hid_to_hidden_update, (100, 100))\n",
      "(b_hidden_update, (100,))\n",
      "(W, (100, 600))\n",
      "(b, (600,))\n",
      "----------------------------------------\n",
      "Starting training...\n",
      "Epoch 1 of 500 took 34.946s \n",
      "\n",
      "  training loss:\t\t1.241174 \n",
      "\n",
      "  training accuracy:\t\t73.29 % \n",
      "\n",
      "  validation loss:\t\t2.560574 \n",
      "\n",
      "  validation accuracy:\t\t31.64 % \n",
      "\n",
      "Epoch 2 of 500 took 34.997s \n",
      "\n",
      "  training loss:\t\t0.911082 \n",
      "\n",
      "  training accuracy:\t\t82.55 % \n",
      "\n",
      "  validation loss:\t\t2.426701 \n",
      "\n",
      "  validation accuracy:\t\t39.84 % \n",
      "\n",
      "Epoch 3 of 500 took 35.011s \n",
      "\n",
      "  training loss:\t\t0.679810 \n",
      "\n",
      "  training accuracy:\t\t89.35 % \n",
      "\n",
      "  validation loss:\t\t2.477336 \n",
      "\n",
      "  validation accuracy:\t\t39.84 % \n",
      "\n",
      "Epoch 4 of 500 took 35.035s \n",
      "\n",
      "  training loss:\t\t0.512554 \n",
      "\n",
      "  training accuracy:\t\t93.05 % \n",
      "\n",
      "  validation loss:\t\t2.415199 \n",
      "\n",
      "  validation accuracy:\t\t39.45 % \n",
      "\n",
      "Epoch 5 of 500 took 35.050s \n",
      "\n",
      "  training loss:\t\t0.378759 \n",
      "\n",
      "  training accuracy:\t\t96.25 % \n",
      "\n",
      "  validation loss:\t\t2.442820 \n",
      "\n",
      "  validation accuracy:\t\t39.06 % \n",
      "\n",
      "Epoch 6 of 500 took 35.034s \n",
      "\n",
      "  training loss:\t\t0.274689 \n",
      "\n",
      "  training accuracy:\t\t98.23 % \n",
      "\n",
      "  validation loss:\t\t2.478461 \n",
      "\n",
      "  validation accuracy:\t\t41.80 % \n",
      "\n",
      "Epoch 7 of 500 took 34.935s \n",
      "\n",
      "  training loss:\t\t0.290021 \n",
      "\n",
      "  training accuracy:\t\t97.96 % \n",
      "\n",
      "  validation loss:\t\t2.637290 \n",
      "\n",
      "  validation accuracy:\t\t42.19 % \n",
      "\n",
      "Epoch 8 of 500 took 34.989s \n",
      "\n",
      "  training loss:\t\t0.252453 \n",
      "\n",
      "  training accuracy:\t\t97.84 % \n",
      "\n",
      "  validation loss:\t\t2.472155 \n",
      "\n",
      "  validation accuracy:\t\t42.19 % \n",
      "\n",
      "Epoch 9 of 500 took 35.010s \n",
      "\n",
      "  training loss:\t\t0.176894 \n",
      "\n",
      "  training accuracy:\t\t99.38 % \n",
      "\n",
      "  validation loss:\t\t2.452245 \n",
      "\n",
      "  validation accuracy:\t\t41.41 % \n",
      "\n",
      "Epoch 10 of 500 took 34.634s \n",
      "\n",
      "  training loss:\t\t0.115858 \n",
      "\n",
      "  training accuracy:\t\t99.85 % \n",
      "\n",
      "  validation loss:\t\t2.451912 \n",
      "\n",
      "  validation accuracy:\t\t46.48 % \n",
      "\n",
      "Epoch 11 of 500 took 35.184s \n",
      "\n",
      "  training loss:\t\t0.079418 \n",
      "\n",
      "  training accuracy:\t\t100.00 % \n",
      "\n",
      "  validation loss:\t\t2.442389 \n",
      "\n",
      "  validation accuracy:\t\t50.00 % \n",
      "\n",
      "Epoch 12 of 500 took 35.023s \n",
      "\n",
      "  training loss:\t\t0.057713 \n",
      "\n",
      "  training accuracy:\t\t100.00 % \n",
      "\n",
      "  validation loss:\t\t2.444987 \n",
      "\n",
      "  validation accuracy:\t\t50.00 % \n",
      "\n",
      "Epoch 13 of 500 took 34.996s \n",
      "\n",
      "  training loss:\t\t0.045613 \n",
      "\n",
      "  training accuracy:\t\t100.00 % \n",
      "\n",
      "  validation loss:\t\t2.467373 \n",
      "\n",
      "  validation accuracy:\t\t46.88 % \n",
      "\n",
      "Epoch 14 of 500 took 35.023s \n",
      "\n",
      "  training loss:\t\t0.037199 \n",
      "\n",
      "  training accuracy:\t\t100.00 % \n",
      "\n",
      "  validation loss:\t\t2.489674 \n",
      "\n",
      "  validation accuracy:\t\t47.66 % \n",
      "\n",
      "Epoch 15 of 500 took 34.999s \n",
      "\n",
      "  training loss:\t\t0.031974 \n",
      "\n",
      "  training accuracy:\t\t100.00 % \n",
      "\n",
      "  validation loss:\t\t2.496501 \n",
      "\n",
      "  validation accuracy:\t\t47.66 % \n",
      "\n",
      "Epoch 16 of 500 took 35.022s \n",
      "\n",
      "  training loss:\t\t0.027905 \n",
      "\n",
      "  training accuracy:\t\t100.00 % \n",
      "\n",
      "  validation loss:\t\t2.516821 \n",
      "\n",
      "  validation accuracy:\t\t46.88 % \n",
      "\n",
      "Saving Model ...\n"
     ]
    }
   ],
   "source": [
    "# lasagne.layers.get_output produces a variable for the output of the net\n",
    "network_output = lasagne.layers.get_output(l_Spk_softmax)\n",
    "\n",
    "labels = T.ivector(name='labels')\n",
    "\n",
    "network_output = lasagne.layers.get_output(l_softmax)\n",
    "val_prediction = lasagne.layers.get_output(l_softmax, deterministic=True)\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), labels), dtype=theano.config.floatX)\n",
    "#training accuracy\n",
    "train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), labels), dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "total_cost = lasagne.objectives.categorical_crossentropy(network_output, labels)\n",
    "#total_cost = -(labels*T.log(network_output) + (1-labels)*T.log(1-network_output)) \n",
    "mean_cost = total_cost.mean()\n",
    "#accuracy function\n",
    "val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, labels)\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "#Get parameters of both encoder and decoder\n",
    "all_parameters = lasagne.layers.get_all_params([l_softmax], trainable=True)\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)\n",
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-5,5) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,5)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.001)\n",
    "\n",
    "train_func = theano.function([X, Mask, labels], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, labels], [val_mcost, val_acc])\n",
    "\n",
    "\n",
    "#load the dataset\n",
    "Data, Msk, Targets, val_Data, val_Msk, val_tars = load_dataset()\n",
    "\n",
    "\n",
    "num_epochs=500\n",
    "epoch=0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "val_prev = np.inf\n",
    "\n",
    "#for epoch in range(num_epochs):\n",
    "while('true'):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(Data, Msk, Targets, 64):\n",
    "        t_data, t_mask, t_labs = batch\n",
    "        terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "        train_err += terr\n",
    "        tr_acc += tacc\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(val_Data, val_Msk, val_tars, 64, shuffle=False):\n",
    "        v_data, v_mask, v_tars = batch\n",
    "        err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    \n",
    "    #f_log = open('/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/training.log','a')\n",
    "    epoch+=1\n",
    "\n",
    "# Then we print the results for this epoch:\n",
    "    #flog = open('training.log','a')\n",
    "    print(\"Epoch {} of {} took {:.3f}s \\n\".format(\n",
    "    epoch, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f} \\n\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} % \\n\".format(\n",
    "        tr_acc / train_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f} \\n\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} % \\n\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    #flog.write('\\n')\n",
    "    #flog.close()\n",
    "   \n",
    "    valE = val_err/val_batches\n",
    "    if valE > val_prev:\n",
    "        c+=1\n",
    "        val_prev=valE\n",
    "    else:\n",
    "        c=0\n",
    "        val_prev=valE\n",
    "    \n",
    "    if c==5:\n",
    "        break\n",
    "    \n",
    "    if epoch==num_epochs:\n",
    "        break\n",
    "        \n",
    "#Save the final model\n",
    "spth = '/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax'\n",
    "\n",
    "print('Saving Model ...')\n",
    "model_params = lasagne.layers.get_all_param_values(l_softmax)\n",
    "model_name = 'basic_softmax_505' + '.pkl'\n",
    "vpth = os.path.join(spth, model_name)\n",
    "fsave = open(vpth,'wb')  \n",
    "cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "fsave.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
