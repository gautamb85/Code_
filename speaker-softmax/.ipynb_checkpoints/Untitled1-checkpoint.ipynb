{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask = np.zeros((2,600), dtype='float32')\n",
    "\n",
    "mask[0,:433] = 1.0\n",
    "mask[1,:226] = 1.0\n",
    "\n",
    "#mask = mask[:, None]\n",
    "x_dummy = np.random.random((2,600,20))\n",
    "x_dummy = np.cast['float32'](x_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fuel.datasets import H5PYDataset\n",
    "\n",
    "train_set = H5PYDataset('/misc/data15/reco/bhattgau/Rnn/lists/rsr/rsr_train_Frame.hdf5',\n",
    "                        which_sets=('train',), subset=slice(0,1))\n",
    "handle = train_set.open()\n",
    "data = train_set.get_data(handle, slice(0,1))\n",
    "\n",
    "x = data[0]\n",
    "mask = data[1]\n",
    "targets = data[2]\n",
    "targets = targets.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W_in_to_updategate, (60, 100))\n",
      "(W_hid_to_updategate, (100, 100))\n",
      "(b_updategate, (100,))\n",
      "(W_in_to_resetgate, (60, 100))\n",
      "(W_hid_to_resetgate, (100, 100))\n",
      "(b_resetgate, (100,))\n",
      "(W_in_to_hidden_update, (60, 100))\n",
      "(W_hid_to_hidden_update, (100, 100))\n",
      "(b_hidden_update, (100,))\n",
      "(W_in_to_updategate, (60, 100))\n",
      "(W_hid_to_updategate, (100, 100))\n",
      "(b_updategate, (100,))\n",
      "(W_in_to_resetgate, (60, 100))\n",
      "(W_hid_to_resetgate, (100, 100))\n",
      "(b_resetgate, (100,))\n",
      "(W_in_to_hidden_update, (60, 100))\n",
      "(W_hid_to_hidden_update, (100, 100))\n",
      "(b_hidden_update, (100,))\n",
      "(W, (100, 194))\n",
      "(b, (194,))\n",
      "----------------------------------------\n",
      "Starting training...\n",
      "Epoch 1 of 10 took 104.141s\n",
      "  training loss:\t\t5.209393\n",
      "  training accuracy:\t\t1.40 %\n",
      "  validation loss:\t\t1.073640\n",
      "  validation accuracy:\t\t1.76 %\n",
      "Epoch 2 of 10 took 72.405s\n",
      "  training loss:\t\t4.663408\n",
      "  training accuracy:\t\t8.20 %\n",
      "  validation loss:\t\t1.047558\n",
      "  validation accuracy:\t\t3.36 %\n",
      "Epoch 3 of 10 took 84.978s\n",
      "  training loss:\t\t4.286790\n",
      "  training accuracy:\t\t12.35 %\n",
      "  validation loss:\t\t1.032346\n",
      "  validation accuracy:\t\t5.96 %\n",
      "Epoch 4 of 10 took 55.133s\n",
      "  training loss:\t\t3.921668\n",
      "  training accuracy:\t\t18.36 %\n",
      "  validation loss:\t\t0.985727\n",
      "  validation accuracy:\t\t7.12 %\n",
      "Epoch 5 of 10 took 61.351s\n",
      "  training loss:\t\t3.564409\n",
      "  training accuracy:\t\t25.49 %\n",
      "  validation loss:\t\t0.958277\n",
      "  validation accuracy:\t\t6.78 %\n",
      "Epoch 6 of 10 took 33.967s\n",
      "  training loss:\t\t3.256175\n",
      "  training accuracy:\t\t31.54 %\n",
      "  validation loss:\t\t0.944660\n",
      "  validation accuracy:\t\t7.55 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ed2d210f4263>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mt_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_labs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mt_labs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_labs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mterr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtacc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_labs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mterr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mtr_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtacc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    950\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import ipdb\n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 100\n",
    "F_DIM = 60\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 1\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 5\n",
    "nSpk = 194\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = X)\n",
    "n_batch,_,_ = l_in.input_var.shape\n",
    "#print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "#initialize the gates\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask, backwards=True)\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "l_reshape1 = lasagne.layers.ReshapeLayer(l_sum, (-1, N_HIDDEN))\n",
    "\n",
    "l_softmax = lasagne.layers.DenseLayer(l_reshape1, num_units=nSpk, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#print lasagne.layers.get_output(l_softmax, inputs={l_in: X, l_mask: Mask}).eval({X: d1, Mask: m1}).shape\n",
    "#l_softmax = lasagne.layers.ReshapeLayer(l_dense, (n_batch, MAX_LENGTH, N_HIDDEN))\n",
    "\n",
    "labels = T.ivector(name='labels')\n",
    "\n",
    "network_output = lasagne.layers.get_output(l_softmax)\n",
    "val_prediction = lasagne.layers.get_output(l_softmax, deterministic=True)\n",
    "\n",
    "total_cost = lasagne.objectives.categorical_crossentropy(network_output, labels)\n",
    "masked_cost = total_cost*Mask.flatten()\n",
    "mean_cost = total_cost.mean()\n",
    "\n",
    "val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, labels)\n",
    "val_cost = val_cost*Mask.flatten()\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), labels), dtype=theano.config.floatX)\n",
    "#training accuracy\n",
    "train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), labels), dtype=theano.config.floatX)\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "#total_cost = -(labels*T.log(network_output) + (1-labels)*T.log(1-network_output)) \n",
    "#accuracy function\n",
    "\n",
    "\n",
    "#Get parameters of both encoder and decoder\n",
    "all_parameters = lasagne.layers.get_all_params([l_softmax], trainable=True)\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)\n",
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-3,3) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,3)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.005)\n",
    "\n",
    "train_func = theano.function([X, Mask, labels], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, labels], [val_mcost, val_acc])\n",
    "\n",
    "\n",
    "\n",
    "train_set = H5PYDataset('/misc/data15/reco/bhattgau/Rnn/lists/rsr/rsr_train_Frame.hdf5',\n",
    "                        which_sets=('train',), subset=slice(0,2000))\n",
    "\n",
    "valid_set = H5PYDataset('/misc/data15/reco/bhattgau/Rnn/lists/rsr/rsr_train_Frame.hdf5',\n",
    "                        which_sets=('test',), subset=slice(0,200))\n",
    "\n",
    "num_epochs=10\n",
    "\n",
    "trainerr=[]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    t_state = train_set.open()\n",
    "    v_state = valid_set.open()\n",
    "\n",
    "    scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=256)\n",
    "    scheme1 = ShuffledScheme(examples=valid_set.num_examples, batch_size=256)\n",
    "\n",
    "\n",
    "    train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "    valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in train_stream.get_epoch_iterator():\n",
    "        t_data, t_mask, t_labs = data\n",
    "        t_labs = t_labs.flatten()\n",
    "        terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "        train_err += terr\n",
    "        tr_acc += tacc\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for data in valid_stream.get_epoch_iterator():\n",
    "        v_data, v_mask, v_tars = data\n",
    "        v_tars = v_tars.flatten()\n",
    "        err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "        \n",
    "    trainerr.append(train_err/train_batches)\n",
    "    \n",
    "    train_set.close(state=t_state)\n",
    "    valid_set.close(state=v_state)\n",
    "        \n",
    "# Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "    epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(\n",
    "        tr_acc / train_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spk_softmax_fuel.py\n"
     ]
    }
   ],
   "source": [
    "#Save the final model\n",
    "spth = '/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax'\n",
    "\n",
    "tfile = open('/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax/trainerr.npy','wb')\n",
    "trainerr = np.asarray(trainerr, dtype='float32')\n",
    "np.save(tfile,trainerr)\n",
    "\n",
    "print('Saving Model ...')\n",
    "model_params = lasagne.layers.get_all_param_values(l_softmax)\n",
    "model_name = 'basic_softmax_500' + '.pkl'\n",
    "vpth = os.path.join(spth, model_name)\n",
    "fsave = open(vpth,'wb')  \n",
    "cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "fsave.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /misc/data15/reco/bhattgau/Rnn/code/Code_/frame_softmax.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /misc/data15/reco/bhattgau/Rnn/code/Code_/frame_softmax.py\n",
    "\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cPickle\n",
    "import sys\n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "from utils import load_data\n",
    "\n",
    "cFile = sys.argv[1]\n",
    "\n",
    "f=open(cFile)\n",
    "lines = f.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "values = [v.split()[1] for v in lines]\n",
    "\n",
    "\n",
    "N_HIDDEN = int(values[0])\n",
    "F_DIM = int(values[1])\n",
    "LEARNING_RATE = np.cast['float32'](values[2])\n",
    "NUM_EPOCHS = int(values[3])\n",
    "NUM_SPEAKERS = int(values[4])\n",
    "MODEL_NAME = values[5]\n",
    "SAVE_PATH = values[6]\n",
    "#data\n",
    "TRAIN_FILE = values[7]\n",
    "VALID_FILE = values[8]\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "\n",
    "def build_rnn(net_input=None, mask_input=None):\n",
    "\n",
    "    print(\"Building network ...\")\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = net_input)\n",
    "    n_batch,_,_ = l_in.input_var.shape\n",
    "    #print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "    l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = mask_input)\n",
    "    #print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "    #initialize the gates\n",
    "    l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask)\n",
    "    l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask, backwards=True)\n",
    "    l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "    l_reshape1 = lasagne.layers.ReshapeLayer(l_sum, (-1, N_HIDDEN))\n",
    "\n",
    "    l_softmax = lasagne.layers.DenseLayer(l_reshape1, num_units=NUM_SPEAKERS, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_softmax\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "        \n",
    "    X = T.tensor3(name='input',dtype='float32')\n",
    "    MASK = T.matrix(name = 'mask', dtype='float32')\n",
    "    LABELS = T.ivector(name='labels')\n",
    "    \n",
    "    #load data\n",
    "    train_set = H5PYDataset(TRAIN_FILE,  which_sets=('train',))\n",
    "    valid_set = H5PYDataset(VALID_FILE, which_sets=('test',))\n",
    "    \n",
    "    network = build_rnn(net_input=X, mask_input= MASK)\n",
    "    \n",
    "    network_output = lasagne.layers.get_output(network)\n",
    "\n",
    "    val_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    #needed for accuracy\n",
    "    val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), LABELS), dtype=theano.config.floatX)\n",
    "    #training accuracy\n",
    "    train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), LABELS), dtype=theano.config.floatX)\n",
    "\n",
    "    #T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "    #print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "    #cost function\n",
    "    total_cost = lasagne.objectives.categorical_crossentropy(network_output, LABELS)\n",
    "    #total_cost = -(labels*T.log(network_output) + (1-labels)*T.log(1-network_output)) \n",
    "    masked_cost = total_cost*MASK.flatten()\n",
    "    mean_cost = total_cost.mean()\n",
    "    #accuracy function\n",
    "    val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, LABELS)\n",
    "    val_cost = val_cost*MASK.flatten()\n",
    "    val_mcost = val_cost.mean()\n",
    "\n",
    "    #Get parameters of both encoder and decoder\n",
    "    all_parameters = lasagne.layers.get_all_params([network], trainable=True)\n",
    "\n",
    "    print(\"Trainable Model Parameters\")\n",
    "    print(\"-\"*40)\n",
    "    for param in all_parameters:\n",
    "        print(param, param.get_value().shape)\n",
    "    print(\"-\"*40)\n",
    "    #add grad clipping to avoid exploding gradients\n",
    "    all_grads = [T.clip(g,-3,3) for g in T.grad(mean_cost, all_parameters)]\n",
    "    all_grads = lasagne.updates.total_norm_constraint(all_grads,3)\n",
    "\n",
    "    updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=LEARNING_RATE)\n",
    "\n",
    "    train_func = theano.function([X, MASK, LABELS], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "    val_func = theano.function([X, MASK, LABELS], [val_mcost, val_acc])\n",
    "\n",
    "    trainerr=[]\n",
    "    epoch=0 #set the epoch counter\n",
    "    \n",
    "    val_prev = np.inf\n",
    "    a_prev = -np.inf\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "        # We iterate over epochs:\n",
    "    while 'true':\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        tr_acc = 0\n",
    "        train_batches = 0\n",
    "\n",
    "        h1=train_set.open()\n",
    "        h2=valid_set.open()\n",
    "\n",
    "        scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=64)\n",
    "        scheme1 = SequentialScheme(examples=valid_set.num_examples, batch_size=64)\n",
    "\n",
    "\n",
    "        train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "        valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for data in train_stream.get_epoch_iterator():\n",
    "            t_data, t_mask,t_labs = data\n",
    "            t_labs = t_labs.flatten()\n",
    "            terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "            train_err += terr\n",
    "            tr_acc += tacc\n",
    "            train_batches += 1\n",
    "\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "\n",
    "        for data in valid_stream.get_epoch_iterator():\n",
    "            v_data, v_mask, v_tars = data\n",
    "            v_tars = v_tars.flatten()\n",
    "            \n",
    "            err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        trainerr.append(train_err/train_batches)\n",
    "\n",
    "        epoch+=1\n",
    "        train_set.close(h1)\n",
    "        valid_set.close(h2)\n",
    "        \n",
    "        #Display\n",
    "        if display:\n",
    "            \n",
    "            print(\"Epoch {} of {} took {:.3f}s \".format(\n",
    "            epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "            print(\"  training accuracy:\\t\\t{:.2f} % \".format(\n",
    "                    tr_acc / train_batches * 100))\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "            print(\"  validation accuracy:\\t\\t{:.2f} % \".format(\n",
    "                    val_acc / val_batches * 100))\n",
    "\n",
    "        logfile = os.path.join(SAVE_PATH,MODEL_NAME,'logs',MODEL_NAME+'.log')\n",
    "        flog1 = open(logfile,'ab')\n",
    "\n",
    "        flog1.write(\"Epoch {} of {} took {:.3f}s\\n \".format(\n",
    "        epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "        flog1.write(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "        flog1.write(\"  training accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "            tr_acc / train_batches * 100))\n",
    "        flog1.write(\"  validation loss:\\t\\t{:.6f}\\n\".format(val_err / val_batches))\n",
    "        flog1.write(\"  validation accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "            val_acc / val_batches * 100))\n",
    "        flog1.write(\"\\n\")\n",
    "        flog1.close()\n",
    "\n",
    "        if epoch == NUM_EPOCHS:\n",
    "            break\n",
    "\n",
    "        valE = val_err/val_batches\n",
    "        valA = val_acc / val_batches\n",
    "\n",
    "        #save the max accuracy model\n",
    "        max_val = a_prev\n",
    "\n",
    "        #save model with highest accuracy\n",
    "        if valA > max_val:\n",
    "\n",
    "            model_params1 = lasagne.layers.get_all_param_values(network)\n",
    "\n",
    "            model1_name = MODEL_NAME+'_acc' + '.pkl'\n",
    "\n",
    "            vpth1 = os.path.join(SAVE_PATH, MODEL_NAME, 'weights',model1_name)\n",
    "\n",
    "            fsave = open(vpth1,'wb')  \n",
    "\n",
    "            cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            fsave.close()\n",
    "\n",
    "            max_val = valA\n",
    "\n",
    "        #Patience\n",
    "\n",
    "        if valE > val_prev:\n",
    "            c+=1\n",
    "\n",
    "            #save the model incase\n",
    "            model_params1 = lasagne.layers.get_all_param_values(network)\n",
    "\n",
    "            model1_name = MODEL_NAME + '_ofit'  + '.pkl'\n",
    "\n",
    "            vpth1 = os.path.join(SAVE_PATH, MODEL_NAME, 'weights',model1_name)\n",
    "\n",
    "            fsave = open(vpth1,'wb')  \n",
    "\n",
    "            cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            fsave.close()\n",
    "\n",
    "            val_prev=valE\n",
    "\n",
    "        else:\n",
    "            c=0\n",
    "            val_prev=valE\n",
    "\n",
    "        if c==5:\n",
    "            break\n",
    "\n",
    "    #Save the final model\n",
    "\n",
    "    print('Saving Model ...')\n",
    "    model_params = lasagne.layers.get_all_param_values(network)\n",
    "    model1_name = MODEL_NAME+'_final' + '.pkl'\n",
    "    vpth = os.path.join(SAVE_PATH, MODEL_NAME,'weights',model1_name)\n",
    "    fsave = open(vpth,'wb')  \n",
    "    cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    fsave.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
