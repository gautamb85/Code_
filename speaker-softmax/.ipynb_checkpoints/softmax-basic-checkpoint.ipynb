{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask = np.zeros((20,600), dtype='float32')\n",
    "mask[0,:433] = 1.0\n",
    "mask[1,:226] = 1.0\n",
    "\n",
    "#mask = mask[:, None]\n",
    "x_dummy = np.random.random((20,600,20))\n",
    "x_dummy = np.cast['float32'](x_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W_in_to_updategate, (20, 200))\n",
      "(W_hid_to_updategate, (200, 200))\n",
      "(b_updategate, (200,))\n",
      "(W_in_to_resetgate, (20, 200))\n",
      "(W_hid_to_resetgate, (200, 200))\n",
      "(b_resetgate, (200,))\n",
      "(W_in_to_hidden_update, (20, 200))\n",
      "(W_hid_to_hidden_update, (200, 200))\n",
      "(b_hidden_update, (200,))\n",
      "(W_in_to_updategate, (20, 200))\n",
      "(W_hid_to_updategate, (200, 200))\n",
      "(b_updategate, (200,))\n",
      "(W_in_to_resetgate, (20, 200))\n",
      "(W_hid_to_resetgate, (200, 200))\n",
      "(b_resetgate, (200,))\n",
      "(W_in_to_hidden_update, (20, 200))\n",
      "(W_hid_to_hidden_update, (200, 200))\n",
      "(b_hidden_update, (200,))\n",
      "(W, (400, 98))\n",
      "(b, (98,))\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/scan_module/scan.py:1019: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n"
     ]
    }
   ],
   "source": [
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Train_feats_labs.plst')\n",
    "    lines = f1.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    labelz = [int(l.split()[1]) for l in lines] \n",
    "    #labelz = labelz[:20]\n",
    "    features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    f2 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Valid_feats_labs.plst')\n",
    "    lines = f2.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    val_labelz = [int(l.split()[1]) for l in lines] \n",
    "    #val_labelz = val_labelz[:20]\n",
    "    val_features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    n_samp = len(features)\n",
    "    maxlen=600 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "    nSpk = 98\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    Data = np.zeros((n_samp, maxlen, feat_dim), dtype='float32')\n",
    "    Mask = np.zeros((n_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "    \n",
    "    vn_samp = len(val_features)\n",
    "    val_Data = np.zeros((vn_samp, maxlen, feat_dim), dtype='float32')\n",
    "    val_Mask = np.zeros((vn_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "\n",
    "    for ind,f in enumerate(features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        Data[ind,:,:] = data\n",
    "        \n",
    "\n",
    "    for ind,f in enumerate(val_features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        val_Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        val_Data[ind,:,:] = data\n",
    "\n",
    "\n",
    "    return Data, Mask, np.asarray(labelz, dtype='int32'), val_Data, val_Mask, np.asarray(val_labelz, dtype='int32')\n",
    "\n",
    "\n",
    "def iterate_minibatches(inputs, mask, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], mask[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 600\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 200\n",
    "F_DIM = 20\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 1\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 5\n",
    "nSpk = 98\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "target = T.matrix(name='target_values', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = X)\n",
    "n_batch,_,_ = l_in.input_var.shape\n",
    "#print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "#initialize the gates\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask, only_return_final=True)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask, only_return_final=True,\n",
    "                                    backwards=True)\n",
    "l_sum = lasagne.layers.ConcatLayer([l_forward, l_backward])\n",
    "\n",
    "#l_proj = lasagne.layers.DenseLayer(l_concat, num_units=64, nonlinearity=lasagne.nonlinearities.linear)\n",
    "\n",
    "l_softmax = lasagne.layers.DenseLayer(l_sum, num_units=nSpk, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#print lasagne.layers.get_output(l_softmax, inputs={l_in: X, l_mask: Mask}).eval({X: d1, Mask: m1}).shape\n",
    "labels = T.ivector(name='labels')\n",
    "\n",
    "network_output = lasagne.layers.get_output(l_softmax)\n",
    "val_prediction = lasagne.layers.get_output(l_softmax, deterministic=True)\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), labels), dtype=theano.config.floatX)\n",
    "#training accuracy\n",
    "train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), labels), dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "total_cost = lasagne.objectives.categorical_crossentropy(network_output, labels)\n",
    "#total_cost = -(labels*T.log(network_output) + (1-labels)*T.log(1-network_output)) \n",
    "mean_cost = total_cost.mean()\n",
    "#accuracy function\n",
    "val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, labels)\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "#Get parameters of both encoder and decoder\n",
    "all_parameters = lasagne.layers.get_all_params([l_softmax], trainable=True)\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 20 took 22.430s\n",
      "  training loss:\t\t2.395399\n",
      "  training accuracy:\t\t45.35 %\n",
      "  validation loss:\t\t2.591966\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 2 of 20 took 22.417s\n",
      "  training loss:\t\t0.555487\n",
      "  training accuracy:\t\t88.49 %\n",
      "  validation loss:\t\t2.181450\n",
      "  validation accuracy:\t\t42.00 %\n",
      "Epoch 3 of 20 took 22.406s\n",
      "  training loss:\t\t0.101059\n",
      "  training accuracy:\t\t99.62 %\n",
      "  validation loss:\t\t2.149641\n",
      "  validation accuracy:\t\t44.50 %\n",
      "Epoch 4 of 20 took 22.394s\n",
      "  training loss:\t\t0.024498\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.089419\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 5 of 20 took 22.420s\n",
      "  training loss:\t\t0.012509\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.077608\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 6 of 20 took 22.420s\n",
      "  training loss:\t\t0.009013\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.064588\n",
      "  validation accuracy:\t\t48.00 %\n",
      "Epoch 7 of 20 took 22.441s\n",
      "  training loss:\t\t0.007107\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.061869\n",
      "  validation accuracy:\t\t48.50 %\n",
      "Epoch 8 of 20 took 22.342s\n",
      "  training loss:\t\t0.005792\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.060577\n",
      "  validation accuracy:\t\t48.00 %\n",
      "Epoch 9 of 20 took 22.080s\n",
      "  training loss:\t\t0.004876\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.052712\n",
      "  validation accuracy:\t\t48.50 %\n",
      "Epoch 10 of 20 took 22.018s\n",
      "  training loss:\t\t0.004129\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.055059\n",
      "  validation accuracy:\t\t48.50 %\n",
      "Epoch 11 of 20 took 22.024s\n",
      "  training loss:\t\t0.003565\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.052061\n",
      "  validation accuracy:\t\t48.50 %\n",
      "Epoch 12 of 20 took 21.998s\n",
      "  training loss:\t\t0.003117\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.051523\n",
      "  validation accuracy:\t\t49.00 %\n",
      "Epoch 13 of 20 took 21.988s\n",
      "  training loss:\t\t0.002752\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.051887\n",
      "  validation accuracy:\t\t48.50 %\n",
      "Epoch 14 of 20 took 21.985s\n",
      "  training loss:\t\t0.002450\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.051449\n",
      "  validation accuracy:\t\t49.00 %\n",
      "Epoch 15 of 20 took 21.985s\n",
      "  training loss:\t\t0.002201\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.048524\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 16 of 20 took 21.976s\n",
      "  training loss:\t\t0.001976\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.049886\n",
      "  validation accuracy:\t\t49.00 %\n",
      "Epoch 17 of 20 took 21.969s\n",
      "  training loss:\t\t0.001795\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.050068\n",
      "  validation accuracy:\t\t49.00 %\n",
      "Epoch 18 of 20 took 21.973s\n",
      "  training loss:\t\t0.001629\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.050983\n",
      "  validation accuracy:\t\t49.50 %\n",
      "Epoch 19 of 20 took 21.973s\n",
      "  training loss:\t\t0.001495\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.048589\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 20 of 20 took 21.974s\n",
      "  training loss:\t\t0.001375\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t2.049804\n",
      "  validation accuracy:\t\t50.00 %\n"
     ]
    }
   ],
   "source": [
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-5,5) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,5)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.005)\n",
    "\n",
    "train_func = theano.function([X, Mask, labels], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, labels], [val_mcost, val_acc])\n",
    "\n",
    "\n",
    "num_epochs=20\n",
    "#load the dataset\n",
    "Data, Msk, Targets, val_Data, val_Msk, val_tars = load_dataset()\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(Data, Msk, Targets, 32):\n",
    "        t_data, t_mask, t_labs = batch\n",
    "        terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "        train_err += terr\n",
    "        tr_acc += tacc\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(val_Data, val_Msk, val_tars, 8, shuffle=False):\n",
    "        v_data, v_mask, v_tars = batch\n",
    "        err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "        \n",
    "# Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "    epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(\n",
    "        tr_acc / train_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model ...\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "#Save the final model\n",
    "spth = '/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax'\n",
    "\n",
    "print('Saving Model ...')\n",
    "model_params = lasagne.layers.get_all_param_values(l_softmax)\n",
    "model_name = 'basic_softmax_200' + '.pkl'\n",
    "vpth = os.path.join(spth, model_name)\n",
    "fsave = open(vpth,'wb')  \n",
    "cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "fsave.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6520767824b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     val_err, valacc = [val_func(val_x, val_mask, labs) \n\u001b[1;32m---> 21\u001b[1;33m                     for val_x, val_mask, val_labs in iterate_minibatches(val_Data, val_Msk, val_tars, BATCH_SIZE)]\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "#old training function\n",
    "Data, Msk, Targets, val_Data, val_Msk, val_tars = load_dataset()\n",
    "\n",
    "print(\"Training ...\")\n",
    "\n",
    "pval_loss = np.inf\n",
    "\n",
    "num_epochs=30\n",
    "\n",
    "epoch=0\n",
    "patience = 0\n",
    "#for epoch in range(NUM_EPOCHS):\n",
    "while True:\n",
    "    #epoch_start = time.time()\n",
    "    epoch = epoch+1\n",
    "    #flog = open(os.path.join(spth,'logs','train_err.log'),'a')\n",
    "    \n",
    "    mbatch_costs = [train_func(x, x_mask, labs) \n",
    "                    for x, x_mask, labs in iterate_minibatches(Data, Msk, Targets, BATCH_SIZE)]\n",
    "    \n",
    "    val_err, valacc = [val_func(val_x, val_mask, labs) \n",
    "                    for val_x, val_mask, val_labs in iterate_minibatches(val_Data, val_Msk, val_tars, BATCH_SIZE)]\n",
    "    \n",
    "    \n",
    "    print('Epoch %d , Train Error = %f, Val error = %f, val acc = %f' %\n",
    "          (epoch, np.mean(mbatch_costs), np.mean(val_err), np.mean(valacc)))\n",
    "    \n",
    "    if epoch==num_epochs:\n",
    "        print('Finished Tranining Model')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
