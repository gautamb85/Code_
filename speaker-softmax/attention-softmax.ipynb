{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask = np.zeros((2,600), dtype='float32')\n",
    "mask[0,:433] = 1.0\n",
    "mask[1,:226] = 1.0\n",
    "\n",
    "#mask = mask[:, None]\n",
    "x_dummy = np.random.random((2,600,20))\n",
    "x_dummy = np.cast['float32'](x_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n"
     ]
    }
   ],
   "source": [
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Train_feats_labs.plst')\n",
    "    lines = f1.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    labelz = [int(l.split()[1]) for l in lines] \n",
    "    #labelz = labelz[:20]\n",
    "    features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    f2 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Valid_feats_labs.plst')\n",
    "    lines = f2.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    val_labelz = [int(l.split()[1]) for l in lines] \n",
    "    #val_labelz = val_labelz[:20]\n",
    "    val_features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    n_samp = len(features)\n",
    "    maxlen=800 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "    nSpk = 98\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    Data = np.zeros((n_samp, maxlen, feat_dim), dtype='float32')\n",
    "    Mask = np.zeros((n_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "    \n",
    "    vn_samp = len(val_features)\n",
    "    val_Data = np.zeros((vn_samp, maxlen, feat_dim), dtype='float32')\n",
    "    val_Mask = np.zeros((vn_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "\n",
    "    for ind,f in enumerate(features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        Data[ind,:,:] = data\n",
    "        \n",
    "\n",
    "    for ind,f in enumerate(val_features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        val_Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        val_Data[ind,:,:] = data\n",
    "\n",
    "\n",
    "    return Data, Mask, np.asarray(labelz, dtype='int32'), val_Data, val_Mask, np.asarray(val_labelz, dtype='int32')\n",
    "\n",
    "def iterate_minibatches(inputs, mask, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    iplen = len(inputs)\n",
    "    pointer = 0\n",
    "    indices=np.arange(iplen)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    while pointer < iplen:\n",
    "    \n",
    "        if pointer <= iplen - batchsize:\n",
    "            excerpt = indices[pointer:pointer+batchsize]\n",
    "        else:\n",
    "            batchsize = iplen-pointer\n",
    "            excerpt = indices[pointer:pointer+batchsize]\n",
    "        \n",
    "        pointer+=batchsize\n",
    "\n",
    "        yield inputs[excerpt], mask[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 500\n",
    "F_DIM = 20\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 1\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "nSpk = 98\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "target = T.matrix(name='target_values', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None, F_DIM), input_var = X)\n",
    "n_batch,maxlen,_ = l_in.input_var.shape\n",
    "\n",
    "#get the batch size for the weights matrix\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "#initialize the gates\n",
    "\n",
    "#Compute Recurrent Embeddings\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask, backwards=True)\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "'''\n",
    "l_forward1 = lasagne.layers.GRULayer(l_sum, 200, mask_input=l_mask)\n",
    "l_backward1 = lasagne.layers.GRULayer(l_sum, 200, mask_input=l_mask, backwards=True)\n",
    "l_sum1 = lasagne.layers.ElemwiseSumLayer([l_forward1, l_backward1])\n",
    "\n",
    "l_forward2 = lasagne.layers.GRULayer(l_sum, 200, mask_input=l_mask)\n",
    "l_backward2 = lasagne.layers.GRULayer(l_sum, 200, mask_input=l_mask, backwards=True)\n",
    "l_sum2 = lasagne.layers.ConcatLayer([l_forward1, l_backward1])\n",
    "'''\n",
    "\n",
    "#Collect all the recurrent embeddings\n",
    "Recc_emb =  lasagne.layers.get_output(l_sum, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask})\n",
    "\n",
    "#Calculate the last time-step of a recording using the mask.\n",
    "lstep = T.sum(Mask, axis=1)#.eval({Mask : mask})\n",
    "lstep = T.cast(lstep, 'int32')#.eval()\n",
    "lstep = lstep.T\n",
    "\n",
    "#Get the last time-step from the l-sum layer\n",
    "l_last = lasagne.layers.SliceLayer(l_sum, -1, 1)\n",
    "#print lasagne.layers.get_output(l_last, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "#Attention Model\n",
    "l_attn = lasagne.layers.DenseLayer(l_last, num_units=MAX_LENGTH, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "\n",
    "#attend = lasagne.layers.ReshapeLayer(l_softmax, (n_batch, MAX_LENGTH, MAX_LENGTH))\n",
    "\n",
    "#These are the attention weights over all the timesteps of the padded utterance\n",
    "attn_wts = lasagne.layers.get_output(l_attn, inputs={l_in: X, l_mask: Mask})\n",
    "attn_probs = T.nnet.softmax(attn_wts)#.eval({X: x_dummy, Mask: mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W_in_to_updategate, (20, 500))\n",
      "(W_hid_to_updategate, (500, 500))\n",
      "(b_updategate, (500,))\n",
      "(W_in_to_resetgate, (20, 500))\n",
      "(W_hid_to_resetgate, (500, 500))\n",
      "(b_resetgate, (500,))\n",
      "(W_in_to_hidden_update, (20, 500))\n",
      "(W_hid_to_hidden_update, (500, 500))\n",
      "(b_hidden_update, (500,))\n",
      "(W_in_to_updategate, (20, 500))\n",
      "(W_hid_to_updategate, (500, 500))\n",
      "(b_updategate, (500,))\n",
      "(W_in_to_resetgate, (20, 500))\n",
      "(W_hid_to_resetgate, (500, 500))\n",
      "(b_resetgate, (500,))\n",
      "(W_in_to_hidden_update, (20, 500))\n",
      "(W_hid_to_hidden_update, (500, 500))\n",
      "(b_hidden_update, (500,))\n",
      "(W, (500, 800))\n",
      "(b, (800,))\n",
      "(W, (500, 98))\n",
      "(b, (98,))\n",
      "----------------------------------------\n",
      "Starting training...\n",
      "Epoch 1 of 500 took 39.836s \n",
      "  training loss:\t\t4.354376 \n",
      "  training accuracy:\t\t4.40 % \n",
      "  validation loss:\t\t4.252701 \n",
      "  validation accuracy:\t\t1.56 % \n",
      "Epoch 2 of 500 took 39.841s \n",
      "  training loss:\t\t3.877422 \n",
      "  training accuracy:\t\t6.53 % \n",
      "  validation loss:\t\t4.053114 \n",
      "  validation accuracy:\t\t2.73 % \n",
      "Epoch 3 of 500 took 39.954s \n",
      "  training loss:\t\t3.564427 \n",
      "  training accuracy:\t\t13.63 % \n",
      "  validation loss:\t\t3.657946 \n",
      "  validation accuracy:\t\t7.81 % \n",
      "Epoch 4 of 500 took 39.946s \n",
      "  training loss:\t\t3.216354 \n",
      "  training accuracy:\t\t22.01 % \n",
      "  validation loss:\t\t3.505548 \n",
      "  validation accuracy:\t\t14.84 % \n",
      "Epoch 5 of 500 took 39.976s \n",
      "  training loss:\t\t2.849078 \n",
      "  training accuracy:\t\t33.45 % \n",
      "  validation loss:\t\t3.111261 \n",
      "  validation accuracy:\t\t17.19 % \n",
      "Epoch 6 of 500 took 39.922s \n",
      "  training loss:\t\t2.305709 \n",
      "  training accuracy:\t\t47.89 % \n",
      "  validation loss:\t\t2.750688 \n",
      "  validation accuracy:\t\t25.00 % \n",
      "Epoch 7 of 500 took 39.952s \n",
      "  training loss:\t\t1.701831 \n",
      "  training accuracy:\t\t65.76 % \n",
      "  validation loss:\t\t2.395248 \n",
      "  validation accuracy:\t\t38.67 % \n",
      "Epoch 8 of 500 took 39.918s \n",
      "  training loss:\t\t1.177147 \n",
      "  training accuracy:\t\t79.32 % \n",
      "  validation loss:\t\t2.193213 \n",
      "  validation accuracy:\t\t42.19 % \n",
      "Epoch 9 of 500 took 39.842s \n",
      "  training loss:\t\t0.895437 \n",
      "  training accuracy:\t\t85.03 % \n",
      "  validation loss:\t\t1.824727 \n",
      "  validation accuracy:\t\t56.64 % \n",
      "Epoch 10 of 500 took 39.650s \n",
      "  training loss:\t\t0.616939 \n",
      "  training accuracy:\t\t91.96 % \n",
      "  validation loss:\t\t1.702182 \n",
      "  validation accuracy:\t\t53.52 % \n",
      "Epoch 11 of 500 took 39.771s \n",
      "  training loss:\t\t0.392770 \n",
      "  training accuracy:\t\t96.16 % \n",
      "  validation loss:\t\t1.513910 \n",
      "  validation accuracy:\t\t64.84 % \n",
      "Epoch 12 of 500 took 39.709s \n",
      "  training loss:\t\t0.233170 \n",
      "  training accuracy:\t\t99.08 % \n",
      "  validation loss:\t\t1.476013 \n",
      "  validation accuracy:\t\t63.67 % \n",
      "Epoch 13 of 500 took 39.733s \n",
      "  training loss:\t\t0.138457 \n",
      "  training accuracy:\t\t99.72 % \n",
      "  validation loss:\t\t1.417505 \n",
      "  validation accuracy:\t\t63.67 % \n",
      "Epoch 14 of 500 took 39.676s \n",
      "  training loss:\t\t0.092435 \n",
      "  training accuracy:\t\t99.79 % \n",
      "  validation loss:\t\t1.270632 \n",
      "  validation accuracy:\t\t68.36 % \n",
      "Epoch 15 of 500 took 39.681s \n",
      "  training loss:\t\t0.056642 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.325596 \n",
      "  validation accuracy:\t\t64.45 % \n",
      "Epoch 16 of 500 took 39.627s \n",
      "  training loss:\t\t0.032663 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.239512 \n",
      "  validation accuracy:\t\t70.70 % \n",
      "Epoch 17 of 500 took 39.506s \n",
      "  training loss:\t\t0.020521 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.199629 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 18 of 500 took 39.668s \n",
      "  training loss:\t\t0.014074 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.163348 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 19 of 500 took 39.641s \n",
      "  training loss:\t\t0.011110 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.187292 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 20 of 500 took 39.614s \n",
      "  training loss:\t\t0.008992 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.191042 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 21 of 500 took 39.616s \n",
      "  training loss:\t\t0.007758 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.188510 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 22 of 500 took 39.583s \n",
      "  training loss:\t\t0.006928 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.193775 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 23 of 500 took 39.772s \n",
      "  training loss:\t\t0.006047 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.203632 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 24 of 500 took 39.702s \n",
      "  training loss:\t\t0.005528 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.210390 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 25 of 500 took 39.758s \n",
      "  training loss:\t\t0.005047 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.213315 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 26 of 500 took 39.719s \n",
      "  training loss:\t\t0.004652 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.217774 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 27 of 500 took 39.774s \n",
      "  training loss:\t\t0.004326 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.224651 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 28 of 500 took 39.797s \n",
      "  training loss:\t\t0.004048 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.219970 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 29 of 500 took 39.728s \n",
      "  training loss:\t\t0.003736 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.225628 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 30 of 500 took 39.716s \n",
      "  training loss:\t\t0.003523 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.228553 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 31 of 500 took 39.740s \n",
      "  training loss:\t\t0.003309 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.228794 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 32 of 500 took 39.619s \n",
      "  training loss:\t\t0.003116 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.234051 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 33 of 500 took 39.718s \n",
      "  training loss:\t\t0.002966 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.236700 \n",
      "  validation accuracy:\t\t70.70 % \n",
      "Epoch 34 of 500 took 39.721s \n",
      "  training loss:\t\t0.002784 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.237487 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 35 of 500 took 39.700s \n",
      "  training loss:\t\t0.002632 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.242664 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 36 of 500 took 39.745s \n",
      "  training loss:\t\t0.002486 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.242968 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 37 of 500 took 39.568s \n",
      "  training loss:\t\t0.002384 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.245330 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 38 of 500 took 39.726s \n",
      "  training loss:\t\t0.002278 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.246553 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Saving Model ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'l_softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7178ac39cc80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Saving Model ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m \u001b[0mmodel_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_param_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_softmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Attn_softmax_500'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[0mvpth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l_softmax' is not defined"
     ]
    }
   ],
   "source": [
    "Wts = T.zeros([n_batch, maxlen], dtype='float32')\n",
    "\n",
    "def weighted_avg(w_t, l_t, W, R_emb):\n",
    "    W = T.set_subtensor(W[:l_t], w_t[:l_t])\n",
    "    W = W[None,:]\n",
    "    w_sum = T.dot(W, R_emb)\n",
    "    \n",
    "    #this W (1,600) is is then multiplied (dot product) with the recurrent embedding (600, 100)\n",
    "    #to produce a feature vector that is a weighted sum of all the embedding vectors of the recording\n",
    "    #also return the weights for analysis\n",
    "    \n",
    "    return w_sum\n",
    "    \n",
    "U_t,_ = theano.scan(fn=weighted_avg, sequences=[attn_probs, lstep, Wts, Recc_emb])\n",
    "#We have calculated the weighted sum of the recurrent embeddings, based on the weights output by the attention model.\n",
    "#This will now be reshaped, and passed to a dense layer, which is a softmax over speakers.\n",
    "\n",
    "l_in2 = lasagne.layers.InputLayer(shape=(None,None, None), input_var = U_t)\n",
    "n_batch1,l1,hdim = l_in2.input_var.shape\n",
    "\n",
    "l_reshape2 = lasagne.layers.ReshapeLayer(l_in2, (n_batch1*l1,N_HIDDEN))\n",
    "#print lasagne.layers.get_output(l_reshape2, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "#Finally this feature vector(s) gets passed to a dense layer which represents a softmax distribution over speakers\n",
    "l_Spk_softmax = lasagne.layers.DenseLayer(l_reshape2, num_units=98, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#print lasagne.layers.get_output(l_Spk_softmax, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "# lasagne.layers.get_output produces a variable for the output of the net\n",
    "network_output = lasagne.layers.get_output(l_Spk_softmax)\n",
    "\n",
    "labels = T.ivector(name='labels')\n",
    "\n",
    "#network_output = lasagne.layers.get_output(l_softmax)\n",
    "val_prediction = lasagne.layers.get_output(l_Spk_softmax, deterministic=True)\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), labels), dtype=theano.config.floatX)\n",
    "#training accuracy\n",
    "train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), labels), dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "total_cost = lasagne.objectives.categorical_crossentropy(network_output, labels)\n",
    "#total_cost = -(labels*T.log(network_output) + (1-labels)*T.log(1-network_output)) \n",
    "mean_cost = total_cost.mean()\n",
    "#accuracy function\n",
    "val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, labels)\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "#Get parameters of both encoder and decoder\n",
    "params1 = lasagne.layers.get_all_params([l_attn], trainable=True)\n",
    "params2 = lasagne.layers.get_all_params([l_Spk_softmax], trainable=True)\n",
    "all_parameters = params1 + params2\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)\n",
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-5,5) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,5)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.005)\n",
    "\n",
    "train_func = theano.function([X, Mask, labels], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, labels], [val_mcost, val_acc])\n",
    "\n",
    "\n",
    "#load the dataset\n",
    "Data, Msk, Targets, val_Data, val_Msk, val_tars = load_dataset()\n",
    "\n",
    "\n",
    "num_epochs=500\n",
    "epoch=0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "val_prev = np.inf\n",
    "\n",
    "#for epoch in range(num_epochs):\n",
    "while('true'):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(Data, Msk, Targets, 128):\n",
    "        t_data, t_mask, t_labs = batch\n",
    "        terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "        train_err += terr\n",
    "        tr_acc += tacc\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(val_Data, val_Msk, val_tars, 64, shuffle=False):\n",
    "        v_data, v_mask, v_tars = batch\n",
    "        err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    \n",
    "    #f_log = open('/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/training.log','a')\n",
    "    epoch+=1\n",
    "\n",
    "# Then we print the results for this epoch:\n",
    "    #flog = open('training.log','a')\n",
    "    print(\"Epoch {} of {} took {:.3f}s \".format(\n",
    "    epoch, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} % \".format(\n",
    "        tr_acc / train_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f} \".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} % \".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    #flog.write('\\n')\n",
    "    #flog.close()\n",
    "   \n",
    "    valE = val_err/val_batches\n",
    "    if valE > val_prev:\n",
    "        c+=1\n",
    "        val_prev=valE\n",
    "    else:\n",
    "        c=0\n",
    "        val_prev=valE\n",
    "    \n",
    "    if c==10:\n",
    "        break\n",
    "    \n",
    "    if epoch==num_epochs:\n",
    "        break\n",
    "        \n",
    "#Save the final model\n",
    "\n",
    "spth = '/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax'\n",
    "\n",
    "print('Saving Model ...')\n",
    "model_params = lasagne.layers.get_all_param_values(l_softmax)\n",
    "model_name = 'Attn_softmax_500' + '.pkl'\n",
    "vpth = os.path.join(spth, model_name)\n",
    "fsave = open(vpth,'wb')  \n",
    "cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "fsave.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
