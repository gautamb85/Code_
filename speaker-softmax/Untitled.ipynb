{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rsr_attention.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rsr_attention.py\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cPickle\n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "\n",
    "spth = '/misc/data15/reco/bhattgau/Rnn/projects/Rvector/Weights/basic-softmax'\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 400\n",
    "F_DIM = 60\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 1\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 5\n",
    "nSpk = 194\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "target = T.matrix(name='target_values', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None, F_DIM), input_var = X)\n",
    "n_batch,maxlen,_ = l_in.input_var.shape\n",
    "\n",
    "#get the batch size for the weights matrix\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "#initialize the gates\n",
    "\n",
    "#Compute Recurrent Embeddings\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask, backwards=True)\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "Recc_emb =  lasagne.layers.get_output(l_sum, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask})\n",
    "#print lasagne.layers.get_output(l_res, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "l_reshape1 = lasagne.layers.ReshapeLayer(l_sum, (-1, N_HIDDEN))\n",
    "\n",
    "l_attend = lasagne.layers.DenseLayer(l_reshape1, num_units=1, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "#print lasagne.layers.get_output(l_attend, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "lstep = T.sum(Mask, axis=1)#.eval({Mask : mask})\n",
    "lstep = T.cast(lstep, 'int32')#.eval()\n",
    "lstep = lstep.T\n",
    "\n",
    "l_attention = lasagne.layers.ReshapeLayer(l_attend, (n_batch, maxlen))\n",
    "wts = lasagne.layers.get_output(l_attention, inputs={l_in: X, l_mask: Mask})\n",
    "Wts = T.zeros([n_batch, maxlen], dtype='float32')\n",
    "\n",
    "AWts = T.zeros([n_batch, maxlen], dtype='float32')\n",
    "\n",
    "def weighted_avg(w_t, l_t, W, aW, R_emb):\n",
    "    W = T.set_subtensor(W[:l_t], w_t[:l_t])\n",
    "    W = W[None,:]\n",
    "    attn_probs = T.nnet.softmax(W.nonzero_values())\n",
    "    attn = T.flatten(attn_probs)\n",
    "    attn = attn.T\n",
    "\n",
    "    aW = T.set_subtensor(aW[:l_t], attn)\n",
    "    #aW = aW[None,:]\n",
    "    \n",
    "    w_sum = T.dot(aW, R_emb)\n",
    "    \n",
    "    #this W (1,600) is is then multiplied (dot product) with the recurrent embedding (600, 100)\n",
    "    #to produce a feature vector that is a weighted sum of all the embedding vectors of the recording\n",
    "    #also return the weights for analysis\n",
    "    \n",
    "    return w_sum\n",
    "\n",
    "U_t, _ = theano.scan(fn=weighted_avg, sequences=[wts, lstep, Wts, AWts, Recc_emb])\n",
    "\n",
    "l_in2 = lasagne.layers.InputLayer(shape=(None,N_HIDDEN), input_var = U_t)\n",
    "n_batch1,_ = l_in2.input_var.shape\n",
    "\n",
    "#l_reshape2 = lasagne.layers.ReshapeLayer(l_in2, (n_batch1*l1,N_HIDDEN))\n",
    "#print lasagne.layers.get_output(l_reshape2, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "#Finally this feature vector(s) gets passed to a dense layer which represents a softmax distribution over speakers\n",
    "l_Spk_softmax = lasagne.layers.DenseLayer(l_in2, num_units=98, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#print lasagne.layers.get_output(l_Spk_softmax, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "# lasagne.layers.get_output produces a variable for the output of the net\n",
    "\n",
    "network_output = lasagne.layers.get_output(l_Spk_softmax)\n",
    "\n",
    "labels = T.ivector(name='labels')\n",
    "\n",
    "val_prediction = lasagne.layers.get_output(l_Spk_softmax, deterministic=True)\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), labels), dtype=theano.config.floatX)\n",
    "#training accuracy\n",
    "train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), labels), dtype=theano.config.floatX)\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "total_cost = lasagne.objectives.categorical_crossentropy(network_output, labels)\n",
    "#total_cost = -(labels*T.log(network_MMMmoutput) + (1-labels)*T.log(1-network_output)) \n",
    "mean_cost = total_cost.mean()\n",
    "#accuracy function\n",
    "val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, labels)\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "#Get parameters of both encoder and decoder\n",
    "params1 = lasagne.layers.get_all_params([l_attend], trainable=True)\n",
    "params2 = lasagne.layers.get_all_params([l_Spk_softmax], trainable=True)\n",
    "all_parameters = params1 + params2\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)\n",
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-5,5) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,3)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.005)\n",
    "\n",
    "train_func = theano.function([X, Mask, labels], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, labels], [val_mcost, val_acc])\n",
    "#Get parameters of both encoder and decoder\n",
    "\n",
    "num_epochs=500\n",
    "epoch=0\n",
    "\n",
    "train_set = H5PYDataset('/misc/data15/reco/bhattgau/Rnn/lists/rsr/rsr_train3.hdf5', \n",
    "                        which_sets=('train',), subset=slice(0,30000))\n",
    "valid_set = H5PYDataset('/misc/data15/reco/bhattgau/Rnn/lists/rsr/rsr_train3.hdf5',\n",
    "                        which_sets=('test',), subset=slice(0,2000))\n",
    "\n",
    "trainerr=[]\n",
    "\n",
    "val_prev = np.inf\n",
    "a_prev = -np.inf\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "while 'true':\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    h1=train_set.open()\n",
    "    h2=valid_set.open()\n",
    "\n",
    "    scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=256)\n",
    "    scheme1 = SequentialScheme(examples=valid_set.num_examples, batch_size=128)\n",
    "\n",
    "\n",
    "    train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "    valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in train_stream.get_epoch_iterator():\n",
    "        t_data, t_mask, _, t_labs = data\n",
    "        terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "        train_err += terr\n",
    "        tr_acc += tacc\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for data in valid_stream.get_epoch_iterator():\n",
    "        v_data, v_mask, _, v_tars = data\n",
    "        err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "        \n",
    "    trainerr.append(train_err/train_batches)\n",
    "    \n",
    "    epoch+=1\n",
    "    train_set.close(h1)\n",
    "    valid_set.close(h2)\n",
    "    \n",
    "        \n",
    "    flog1 = open('/misc/data15/reco/bhattgau/Rnn/projects/Rvector/Weights/basic-softmax/Atrain_attn_rsr1.log','ab')\n",
    "\n",
    "    flog1.write(\"Epoch {} of {} took {:.3f}s\\n \".format(\n",
    "    epoch, num_epochs, time.time() - start_time))\n",
    "    flog1.write(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "    flog1.write(\"  training accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "        tr_acc / train_batches * 100))\n",
    "    flog1.write(\"  validation loss:\\t\\t{:.6f}\\n\".format(val_err / val_batches))\n",
    "    flog1.write(\"  validation accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    flog1.write(\"\\n\")\n",
    "    flog1.close()\n",
    "    \n",
    "    max_val = a_prev\n",
    "    valE = val_err/val_batches\n",
    "    valA = val_acc / val_batches\n",
    "    \n",
    "    if epoch == num_epochs:\n",
    "        break\n",
    "    #save model with highest accuracy\n",
    "    if valA > max_val:\n",
    "        \n",
    "        model_params1 = lasagne.layers.get_all_param_values(l_attention)\n",
    "        model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)\n",
    "\n",
    "        model1_name = 'rsr_Attn_acc' + '.pkl'\n",
    "        model2_name = 'rsr_Attn_accB' + '.pkl'\n",
    "\n",
    "        vpth1 = os.path.join(spth, model1_name)\n",
    "        vpth2 = os.path.join(spth, model2_name)\n",
    "\n",
    "        fsave = open(vpth1,'wb')  \n",
    "        fsave2 = open(vpth2,'wb')  \n",
    "\n",
    "        cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        cPickle.dump(model_params2, fsave2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        fsave.close()\n",
    "        fsave2.close()\n",
    "        \n",
    "        max_val = valA\n",
    "\n",
    "    #Patience\n",
    "\n",
    "    if valE > val_prev:\n",
    "        c+=1\n",
    "        \n",
    "        #save the model incase\n",
    "        model_params1 = lasagne.layers.get_all_param_values(l_attention)\n",
    "        model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)\n",
    "\n",
    "        model1_name = 'rsr_Attn_ofit'  + '.pkl'\n",
    "        model2_name = 'rsr_Attn_ofitB' + '.pkl'\n",
    "\n",
    "        vpth1 = os.path.join(spth, model1_name)\n",
    "        vpth2 = os.path.join(spth, model2_name)\n",
    "\n",
    "        fsave = open(vpth1,'wb')  \n",
    "        fsave2 = open(vpth2,'wb')  \n",
    "\n",
    "        cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        cPickle.dump(model_params2, fsave2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        fsave.close()\n",
    "        fsave2.close()\n",
    "        \n",
    "        val_prev=valE\n",
    "        \n",
    "    else:\n",
    "        c=0\n",
    "        val_prev=valE\n",
    "    \n",
    "    if c==5:\n",
    "        break\n",
    "        \n",
    "#Save the final model\n",
    "\n",
    "\n",
    "print('Saving Model ...')\n",
    "model_params1 = lasagne.layers.get_all_param_values(l_attention)\n",
    "model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)\n",
    "\n",
    "model1_name = 'Attn_softmax_rsr' + '.pkl'\n",
    "model2_name = 'Attn_softmax_rsr' + '.pkl'\n",
    "\n",
    "vpth1 = os.path.join(spth, model1_name)\n",
    "vpth2 = os.path.join(spth, model2_name)\n",
    "\n",
    "fsave = open(vpth1,'wb')  \n",
    "fsave2 = open(vpth2,'wb')  \n",
    "\n",
    "cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(model_params2, fsave2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "fsave.close()\n",
    "fsave2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../attention_fuel.py\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "spth = '/misc/data15/reco/bhattgau/Rnn/projects/Rvector/Weights/basic-softmax'\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    f1 = open('/misc/data15/reco/bhattgau/Rnn/lists/spk_softmax/Train_feats_labs.plst')\n",
    "    lines = f1.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    labelz = [int(l.split()[1]) for l in lines] \n",
    "    #labelz = labelz[:20]\n",
    "    features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    f2 = open('/misc/data15/reco/bhattgau/Rnn/lists/spk_softmax/Valid_feats_labs.plst')\n",
    "    lines = f2.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    val_labelz = [int(l.split()[1]) for l in lines] \n",
    "    #val_labelz = val_labelz[:20]\n",
    "    val_features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    n_samp = len(features)\n",
    "    maxlen=800 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "    nSpk = 98\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    Data = np.zeros((n_samp, maxlen, feat_dim), dtype='float32')\n",
    "    Mask = np.zeros((n_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "    \n",
    "    vn_samp = len(val_features)\n",
    "    val_Data = np.zeros((vn_samp, maxlen, feat_dim), dtype='float32')\n",
    "    val_Mask = np.zeros((vn_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "\n",
    "    for ind,f in enumerate(features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        Data[ind,:,:] = data\n",
    "        \n",
    "\n",
    "    for ind,f in enumerate(val_features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        val_Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        val_Data[ind,:,:] = data\n",
    "\n",
    "\n",
    "    return Data, Mask, np.asarray(labelz, dtype='int32'), val_Data, val_Mask, np.asarray(val_labelz, dtype='int32')\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 100\n",
    "F_DIM = 20\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 1\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "nSpk = 98\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "target = T.matrix(name='target_values', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None, F_DIM), input_var = X)\n",
    "n_batch,maxlen,_ = l_in.input_var.shape\n",
    "\n",
    "#get the batch size for the weights matrix\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "#initialize the gates\n",
    "\n",
    "#Compute Recurrent Embeddings\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask, backwards=True)\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "Recc_emb =  lasagne.layers.get_output(l_sum, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask})\n",
    "#print lasagne.layers.get_output(l_res, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "l_reshape1 = lasagne.layers.ReshapeLayer(l_sum, (-1, N_HIDDEN))\n",
    "\n",
    "l_attend = lasagne.layers.DenseLayer(l_reshape1, num_units=1, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "#print lasagne.layers.get_output(l_attend, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "lstep = T.sum(Mask, axis=1)#.eval({Mask : mask})\n",
    "lstep = T.cast(lstep, 'int32')#.eval()\n",
    "lstep = lstep.T\n",
    "\n",
    "l_attention = lasagne.layers.ReshapeLayer(l_attend, (n_batch, maxlen))\n",
    "wts = lasagne.layers.get_output(l_attention, inputs={l_in: X, l_mask: Mask})\n",
    "Wts = T.zeros([n_batch, maxlen], dtype='float32')\n",
    "\n",
    "AWts = T.zeros([n_batch, maxlen], dtype='float32')\n",
    "\n",
    "def weighted_avg(w_t, l_t, W, aW, R_emb):\n",
    "    W = T.set_subtensor(W[:l_t], w_t[:l_t])\n",
    "    W = W[None,:]\n",
    "    attn_probs = T.nnet.softmax(W.nonzero_values())\n",
    "    attn = T.flatten(attn_probs)\n",
    "    attn = attn.T\n",
    "\n",
    "    aW = T.set_subtensor(aW[:l_t], attn)\n",
    "    #aW = aW[None,:]\n",
    "    \n",
    "    w_sum = T.dot(aW, R_emb)\n",
    "    \n",
    "    #this W (1,600) is is then multiplied (dot product) with the recurrent embedding (600, 100)\n",
    "    #to produce a feature vector that is a weighted sum of all the embedding vectors of the recording\n",
    "    #also return the weights for analysis\n",
    "    \n",
    "    return w_sum\n",
    "\n",
    "U_t, _ = theano.scan(fn=weighted_avg, sequences=[wts, lstep, Wts, AWts, Recc_emb])\n",
    "\n",
    "l_in2 = lasagne.layers.InputLayer(shape=(None,N_HIDDEN), input_var = U_t)\n",
    "n_batch1,_ = l_in2.input_var.shape\n",
    "\n",
    "#l_reshape2 = lasagne.layers.ReshapeLayer(l_in2, (n_batch1*l1,N_HIDDEN))\n",
    "#print lasagne.layers.get_output(l_reshape2, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "#Finally this feature vector(s) gets passed to a dense layer which represents a softmax distribution over speakers\n",
    "l_Spk_softmax = lasagne.layers.DenseLayer(l_in2, num_units=98, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#print lasagne.layers.get_output(l_Spk_softmax, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "# lasagne.layers.get_output produces a variable for the output of the net\n",
    "\n",
    "network_output = lasagne.layers.get_output(l_Spk_softmax)\n",
    "\n",
    "labels = T.ivector(name='labels')\n",
    "\n",
    "val_prediction = lasagne.layers.get_output(l_Spk_softmax, deterministic=True)\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), labels), dtype=theano.config.floatX)\n",
    "#training accuracy\n",
    "train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), labels), dtype=theano.config.floatX)\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "total_cost = lasagne.objectives.categorical_crossentropy(network_output, labels)\n",
    "#total_cost = -(labels*T.log(network_MMMmoutput) + (1-labels)*T.log(1-network_output)) \n",
    "mean_cost = total_cost.mean()\n",
    "#accuracy function\n",
    "val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, labels)\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "#Get parameters of both encoder and decoder\n",
    "params1 = lasagne.layers.get_all_params([l_attend], trainable=True)\n",
    "params2 = lasagne.layers.get_all_params([l_Spk_softmax], trainable=True)\n",
    "all_parameters = params1 + params2\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)\n",
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-5,5) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,5)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.001)\n",
    "\n",
    "train_func = theano.function([X, Mask, labels], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, labels], [val_mcost, val_acc])\n",
    "\n",
    "\n",
    "#load the dataset\n",
    "Data, Msk, Targets, val_Data, val_Msk, val_tars = load_dataset()\n",
    "\n",
    "train_set = IndexableDataset(\n",
    "    indexables = OrderedDict([('features', Data), ('mask',Msk), ('targets', Targets)]), \n",
    "    axis_labels={'features':('batch','maxlen','feat_dim'),'mask':('batch','maxlen'), 'targets':('batch','index')})\n",
    "\n",
    "valid_set = IndexableDataset(\n",
    "    indexables = OrderedDict([('features', val_Data), ('mask', val_Msk), ('targets', val_tars)]), \n",
    "    axis_labels={'features':('batch','maxlen','feat_dim'),'mask':('batch','maxlen'), 'targets':('batch','index')})\n",
    "\n",
    "num_epochs=5\n",
    "epoch=0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "val_prev = np.inf\n",
    "a_prev = -np.inf\n",
    "\n",
    "while 'true':\n",
    "#for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    t_state = train_set.open()\n",
    "    v_state = valid_set.open()\n",
    "\n",
    "    scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=128)\n",
    "    scheme1 = SequentialScheme(examples=valid_set.num_examples, batch_size=128)\n",
    "\n",
    "\n",
    "    train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "    valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in train_stream.get_epoch_iterator():\n",
    "        t_data, t_mask, t_labs = data\n",
    "        terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "        train_err += terr\n",
    "        tr_acc += tacc\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for data in valid_stream.get_epoch_iterator():\n",
    "        v_data, v_mask, v_tars = data\n",
    "        err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    \n",
    "    epoch+=1\n",
    "    #f_log = open('/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/training.log','a')\n",
    "\n",
    "# Then we print the results for this epoch:\n",
    "    flog1 = open('/misc/data15/reco/bhattgau/Rnn/projects/Rvector/Weights/basic-softmax/Atrain1.log','ab')\n",
    "\n",
    "    flog1.write(\"Epoch {} of {} took {:.3f}s\\n \".format(\n",
    "    epoch, num_epochs, time.time() - start_time))\n",
    "    flog1.write(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "    flog1.write(\"  training accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "        tr_acc / train_batches * 100))\n",
    "    flog1.write(\"  validation loss:\\t\\t{:.6f}\\n\".format(val_err / val_batches))\n",
    "    flog1.write(\"  validation accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    flog1.write(\"\\n\")\n",
    "    flog1.close()\n",
    "   \n",
    "    valE = val_err/val_batches\n",
    "    valA = val_acc / val_batches\n",
    "    \n",
    "    #save the max accuracy model\n",
    "    max_val = a_prev\n",
    "    \n",
    "    if epoch == num_epochs:\n",
    "        break\n",
    "    #save model with highest accuracy\n",
    "    if valA > max_val:\n",
    "        \n",
    "        model_params1 = lasagne.layers.get_all_param_values(l_attention)\n",
    "        model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)\n",
    "\n",
    "        model1_name = 'Attn_acc' + '.pkl'\n",
    "        model2_name = 'Attn_accB' + '.pkl'\n",
    "\n",
    "        vpth1 = os.path.join(spth, model1_name)\n",
    "        vpth2 = os.path.join(spth, model2_name)\n",
    "\n",
    "        fsave = open(vpth1,'wb')  \n",
    "        fsave2 = open(vpth2,'wb')  \n",
    "\n",
    "        cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        cPickle.dump(model_params2, fsave2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        fsave.close()\n",
    "        fsave2.close()\n",
    "        \n",
    "        max_val = valA\n",
    "\n",
    "    #Patience\n",
    "\n",
    "    if valE > val_prev:\n",
    "        c+=1\n",
    "        \n",
    "        #save the model incase\n",
    "        model_params1 = lasagne.layers.get_all_param_values(l_attention)\n",
    "        model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)\n",
    "\n",
    "        model1_name = 'Attn_ofit'  + '.pkl'\n",
    "        model2_name = 'Attn_ofitB' + '.pkl'\n",
    "\n",
    "        vpth1 = os.path.join(spth, model1_name)\n",
    "        vpth2 = os.path.join(spth, model2_name)\n",
    "\n",
    "        fsave = open(vpth1,'wb')  \n",
    "        fsave2 = open(vpth2,'wb')  \n",
    "\n",
    "        cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        cPickle.dump(model_params2, fsave2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        fsave.close()\n",
    "        fsave2.close()\n",
    "        \n",
    "        val_prev=valE\n",
    "        \n",
    "    else:\n",
    "        c=0\n",
    "        val_prev=valE\n",
    "    \n",
    "    if c==8:\n",
    "        break\n",
    "    \n",
    "    if epoch==num_epochs:\n",
    "        break\n",
    "        \n",
    "#Save the final model\n",
    "\n",
    "\n",
    "print('Saving Model ...')\n",
    "model_params1 = lasagne.layers.get_all_param_values(l_attention)\n",
    "model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)\n",
    "\n",
    "model1_name = 'Attn_softmax_final' + '.pkl'\n",
    "model2_name = 'Attn_softmax_final' + '.pkl'\n",
    "\n",
    "vpth1 = os.path.join(spth, model1_name)\n",
    "vpth2 = os.path.join(spth, model2_name)\n",
    "\n",
    "fsave = open(vpth1,'wb')  \n",
    "fsave2 = open(vpth2,'wb')  \n",
    "\n",
    "cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(model_params2, fsave2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "fsave.close()\n",
    "fsave2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import htkmfc\n",
    "import random\n",
    "\n",
    "def data_dict(audio):\n",
    "\n",
    "    f=open(audio)\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    lines=f.readlines()\n",
    "    Data = {}\n",
    "    maxlen=600 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "\n",
    "    for l in lines:\n",
    "        l=l.strip()\n",
    "        fname = os.path.join(dpth,l+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        pad = np.zeros(((maxlen-data.shape[0]), 20), dtype='float32')\n",
    "        data = np.vstack((data,pad))\n",
    "        Data[l] = data\n",
    "        \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "au= '/misc/data15/reco/bhattgau/Rnn/Lists/Validspks.plst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(au)\n",
    "utts = f.readlines()\n",
    "utts = [u.strip() for u in utts]\n",
    "all_ids = [t.split('_')[0] for t in utts]\n",
    "spk_ids = list(set([t.split('_')[0] for t in utts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for ids in all_ids:\n",
    "    labels.append(spk_ids.index(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/Validspks_labs.plst','wb')\n",
    "snl = zip(utts,labels)\n",
    "random.shuffle(snl)\n",
    "\n",
    "for utt,lab in snl:\n",
    "    f1.write('%s %s\\n'%(utt,lab))\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we have 98 speaker models\n",
    "labels=[]\n",
    "for idx in spk_ids:\n",
    "    labels.append(spk_ids.index(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1d0f4de110c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "g = random.sample(r,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this will be a generator()\n",
    "slots=[]\n",
    "maxlen=600\n",
    "feat_dim=20\n",
    "nSpk = 98\n",
    "labels=[]\n",
    "ptr=0\n",
    "\n",
    "X_data1 = np.zeros((nSpk,maxlen,feat_dim), dtype='float32')\n",
    "X_data2 = np.zeros((nSpk,maxlen,feat_dim), dtype='float32')\n",
    "X_data3 = np.zeros((nSpk,maxlen,feat_dim), dtype='float32')\n",
    "\n",
    "for lab,idx in enumerate(spk_ids):\n",
    "    labels.append(lab)    \n",
    "    for k,i in enumerate(data.keys()):\n",
    "        spk = i.split('_')[0]\n",
    "        if spk == idx:\n",
    "            slots.append(k)\n",
    "            \n",
    "        \n",
    "    slot = np.asarray(slots)          \n",
    "    l = random.sample(slot,3)\n",
    "\n",
    "    enr1 = data.keys()[l[0]]\n",
    "    enr2 = data.keys()[l[1]]\n",
    "    enr3 = data.keys()[l[2]]\n",
    "\n",
    "    en_data1 = data[enr1]\n",
    "    en_data2 = data[enr2]\n",
    "    en_data3 = data[enr3]\n",
    "    \n",
    "    X_data1[ptr,:,:] = en_data1\n",
    "    X_data2[ptr,:,:] = en_data2\n",
    "    X_data3[ptr,:,:] = en_data3\n",
    "      \n",
    "    ptr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate the corresponding masks\n",
    "mask1 = np.zeros((nSpk,maxlen), dtype='float32')\n",
    "mask2 = np.zeros((nSpk,maxlen), dtype='float32')\n",
    "mask3 = np.zeros((nSpk,maxlen), dtype='float32')\n",
    "\n",
    "for i in range(nSpk):\n",
    "    \n",
    "    ones1 = X_data1[i][:,1][np.nonzero(X_data1[i][:,1])]\n",
    "    ones2 = X_data2[i][:,1][np.nonzero(X_data2[i][:,1])]\n",
    "    ones3 = X_data3[i][:,1][np.nonzero(X_data3[i][:,1])]\n",
    "    \n",
    "    mask1[i,:ones1.shape[0]] = 1.0\n",
    "    mask2[i,:ones2.shape[0]] = 1.0\n",
    "    mask3[i,:ones3.shape[0]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = zip(X_data1, mask1,labels, X_data2, mask2,labels, X_data3, mask3,labels)\n",
    "random.shuffle(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1[2][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA = np.vstack((X_data1, X_data2, X_data3))\n",
    "MASK = np.vstack((mask1, mask2, mask3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data = np.asarray(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 600, 20)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 680 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = theano.shared(np.asarray(DATA, dtype=theano.config.floatX), borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = data1[:98,:,:]\n",
    "d2 = data1[98:196,:,:]\n",
    "d3 = data1[196:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "for i in range(98):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the data matrices will be loaded as theano shared variables (data, masks and labels)\n",
    "Shuffle and load them, and then slice mini-batches out of each\n",
    "e.g. 16,16,16 so a batch size of 48\n",
    "After forward propogating throught the RNN, we need to do some grouping, before decision making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import htkmfc\n",
    "import random\n",
    "\n",
    "dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "feat_dim = 20\n",
    "maxlen=600 #pad all utterances to this length\n",
    "\n",
    "f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/Allspks_labs.plst')\n",
    "lines = f1.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "\n",
    "audio = [l.split()[0] for l in lines]\n",
    "labs = [l.split()[1] for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = np.zeros((len(audio), maxlen, feat_dim), dtype='float32')\n",
    "\n",
    "for ind,l in enumerate(audio):\n",
    "    fname = os.path.join(dpth,l+'.fea')\n",
    "    fi = htkmfc.HTKFeat_read(fname)\n",
    "    data = fi.getall()[:,:20]\n",
    "    pad = np.zeros(((maxlen-data.shape[0]), 20), dtype='float32')\n",
    "    data = np.vstack((data,pad))\n",
    "    Data[ind,:,:] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = theano.shared(Data, borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labs = theano.shared(np.asarray(labs, dtype=theano.config.floatX), borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
