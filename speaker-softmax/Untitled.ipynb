{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/all_audio.plst')\n",
    "lines = f1.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "all_ids = [l.split('_')[0] for l in lines]\n",
    "spk_ids = list(set(all_ids))\n",
    "\n",
    "labs = []\n",
    "\n",
    "for id in all_ids:\n",
    "    labs.append(spk_ids.index(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "#f2 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/All_feats_labs.plst','wb')\n",
    "mix = zip(lines, labs)\n",
    "VALID=[]\n",
    "ALL=[]\n",
    "\n",
    "for id in spk_ids:\n",
    "    tmp=[]\n",
    "    for l in lines:\n",
    "        if l.split('_')[0] == id:\n",
    "            tmp.append(l)\n",
    "    ALL.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a in ALL[:50]:\n",
    "    val = itertools.islice(a,4)\n",
    "    val = list(val)\n",
    "    VALID.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the list of validation files\n",
    "V2=[]\n",
    "for v in VALID:\n",
    "    for vv in v:\n",
    "        V2.append(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALL2 = []\n",
    "for a in ALL:\n",
    "    for aa in a:\n",
    "        ALL2.append(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the list of training files\n",
    "TRAIN=[]\n",
    "\n",
    "for a in ALL2:\n",
    "    if a not in V2:\n",
    "        TRAIN.append(a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we need to give them labels\n",
    "train_labs=[]\n",
    "for t in TRAIN:\n",
    "    tlab = t.split('_')[0]\n",
    "    train_labs.append(spk_ids.index(tlab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_labs = []\n",
    "for v in V2:\n",
    "    vlab = v.split('_')[0]\n",
    "    valid_labs.append(spk_ids.index(vlab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zz = zip(V2, valid_labs)\n",
    "random.shuffle(zz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#randomize and write these to files\n",
    "f2 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Valid_feats_labs.plst','wb')\n",
    "#t_mix = \n",
    "for T,TL in zz:\n",
    "    f2.write('%s %s\\n'%(T,TL))\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        \n",
    "        #now select 3 from this list for the validation list\n",
    "        valid = itertools.islice(tmp,4)\n",
    "        valid = list(valid)\n",
    "        VALID.append(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "top3 = itertools.islice(tmp,3)\n",
    "top3 = list(top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1327"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "# %load ../mnist.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Usage example employing Lasagne for digit recognition using the MNIST dataset.\n",
    "\n",
    "This example is deliberately structured as a long flat file, focusing on how\n",
    "to use Lasagne, instead of focusing on writing maximally modular and reusable\n",
    "code. It is used as the foundation for the introductory Lasagne tutorial:\n",
    "http://lasagne.readthedocs.org/en/latest/user/tutorial.html\n",
    "\n",
    "More in-depth examples and reproductions of paper results are maintained in\n",
    "a separate repository: https://github.com/Lasagne/Recipes\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "\n",
    "# ################## Download and prepare the MNIST dataset ##################\n",
    "# This is just some way of getting the MNIST dataset from an online location\n",
    "# and loading it into numpy arrays. It doesn't involve Lasagne at all.\n",
    "\n",
    "def load_dataset():\n",
    "    # We first define some helper functions for supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "        import cPickle as pickle\n",
    "\n",
    "        def pickle_load(f, encoding):\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "        import pickle\n",
    "\n",
    "        def pickle_load(f, encoding):\n",
    "            return pickle.load(f, encoding=encoding)\n",
    "\n",
    "    # We'll now download the MNIST dataset if it is not yet available.\n",
    "    url = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
    "    filename = 'mnist.pkl.gz'\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading MNIST dataset...\")\n",
    "        urlretrieve(url, filename)\n",
    "\n",
    "    # We'll then load and unpickle the file.\n",
    "    import gzip\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = pickle_load(f, encoding='latin-1')\n",
    "\n",
    "    # The MNIST dataset we have here consists of six numpy arrays:\n",
    "    # Inputs and targets for the training set, validation set and test set.\n",
    "    X_train, y_train = data[0]\n",
    "    X_val, y_val = data[1]\n",
    "    X_test, y_test = data[2]\n",
    "\n",
    "    # The inputs come as vectors, we reshape them to monochrome 2D images,\n",
    "    # according to the shape convention: (examples, channels, rows, columns)\n",
    "    X_train = X_train.reshape((-1, 1, 28, 28))\n",
    "    X_val = X_val.reshape((-1, 1, 28, 28))\n",
    "    X_test = X_test.reshape((-1, 1, 28, 28))\n",
    "\n",
    "    # The targets are int64, we cast them to int8 for GPU compatibility.\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_val = y_val.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##################### Build the neural network model #######################\n",
    "# This script supports three types of models. For each one, we define a\n",
    "# function that takes a Theano variable representing the input and returns\n",
    "# the output layer of a neural network model build in Lasagne.\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
    "    # a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "    # data and 50% dropout to the hidden layers.\n",
    "\n",
    "    # Input layer, specifying the expected input shape of the network\n",
    "    # (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
    "    # linking it to the given Theano variable `input_var`, if any:\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                     input_var=input_var)\n",
    "\n",
    "    # Apply 20% dropout to the input data:\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "\n",
    "    # Add a fully-connected layer of 800 units, using the linear rectifier, and\n",
    "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in_drop, num_units=800,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # We'll now add dropout of 50%:\n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    # Another 800-unit layer:\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1_drop, num_units=800,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # 50% dropout again:\n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "\n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid2_drop, num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
    "    # the output layer to give access to a network in Lasagne:\n",
    "    return l_out\n",
    "\n",
    "\n",
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
    "                     drop_hidden=.5):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "                network, width, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "# ############################## Main program ################################\n",
    "# Everything else will be handled in our main program now. We could pull out\n",
    "# more functions to better separate the code, but it wouldn't make it any\n",
    "# easier to read.\n",
    "\n",
    "def main(model='mlp', num_epochs=500):\n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    if model == 'mlp':\n",
    "        network = build_mlp(input_var)\n",
    "    elif model.startswith('custom_mlp:'):\n",
    "        depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "        network = build_custom_mlp(input_var, int(depth), int(width),\n",
    "                                   float(drop_in), float(drop_hid))\n",
    "    elif model == 'cnn':\n",
    "        network = build_cnn(input_var)\n",
    "    else:\n",
    "        print(\"Unrecognized model type %r.\" % model)\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "\n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', lasagne.layers.get_all_param_values(network))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if ('--help' in sys.argv) or ('-h' in sys.argv):\n",
    "        print(\"Trains a neural network on MNIST using Lasagne.\")\n",
    "        print(\"Usage: %s [MODEL [EPOCHS]]\" % sys.argv[0])\n",
    "        print()\n",
    "        print(\"MODEL: 'mlp' for a simple Multi-Layer Perceptron (MLP),\")\n",
    "        print(\"       'custom_mlp:DEPTH,WIDTH,DROP_IN,DROP_HID' for an MLP\")\n",
    "        print(\"       with DEPTH hidden layers of WIDTH units, DROP_IN\")\n",
    "        print(\"       input dropout and DROP_HID hidden dropout,\")\n",
    "        print(\"       'cnn' for a simple Convolutional Neural Network (CNN).\")\n",
    "        print(\"EPOCHS: number of training epochs to perform (default: 500)\")\n",
    "    else:\n",
    "        kwargs = {}\n",
    "        if len(sys.argv) > 1:\n",
    "            kwargs['model'] = sys.argv[1]\n",
    "        if len(sys.argv) > 2:\n",
    "            kwargs['num_epochs'] = int(sys.argv[2])\n",
    "        main(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import htkmfc\n",
    "import random\n",
    "\n",
    "def data_dict(audio):\n",
    "\n",
    "    f=open(audio)\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    lines=f.readlines()\n",
    "    Data = {}\n",
    "    maxlen=600 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "\n",
    "    for l in lines:\n",
    "        l=l.strip()\n",
    "        fname = os.path.join(dpth,l+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        pad = np.zeros(((maxlen-data.shape[0]), 20), dtype='float32')\n",
    "        data = np.vstack((data,pad))\n",
    "        Data[l] = data\n",
    "        \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "au= '/misc/data15/reco/bhattgau/Rnn/Lists/Validspks.plst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(au)\n",
    "utts = f.readlines()\n",
    "utts = [u.strip() for u in utts]\n",
    "all_ids = [t.split('_')[0] for t in utts]\n",
    "spk_ids = list(set([t.split('_')[0] for t in utts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for ids in all_ids:\n",
    "    labels.append(spk_ids.index(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/Validspks_labs.plst','wb')\n",
    "snl = zip(utts,labels)\n",
    "random.shuffle(snl)\n",
    "\n",
    "for utt,lab in snl:\n",
    "    f1.write('%s %s\\n'%(utt,lab))\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we have 98 speaker models\n",
    "labels=[]\n",
    "for idx in spk_ids:\n",
    "    labels.append(spk_ids.index(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1d0f4de110c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "g = random.sample(r,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this will be a generator()\n",
    "slots=[]\n",
    "maxlen=600\n",
    "feat_dim=20\n",
    "nSpk = 98\n",
    "labels=[]\n",
    "ptr=0\n",
    "\n",
    "X_data1 = np.zeros((nSpk,maxlen,feat_dim), dtype='float32')\n",
    "X_data2 = np.zeros((nSpk,maxlen,feat_dim), dtype='float32')\n",
    "X_data3 = np.zeros((nSpk,maxlen,feat_dim), dtype='float32')\n",
    "\n",
    "for lab,idx in enumerate(spk_ids):\n",
    "    labels.append(lab)    \n",
    "    for k,i in enumerate(data.keys()):\n",
    "        spk = i.split('_')[0]\n",
    "        if spk == idx:\n",
    "            slots.append(k)\n",
    "            \n",
    "        \n",
    "    slot = np.asarray(slots)          \n",
    "    l = random.sample(slot,3)\n",
    "\n",
    "    enr1 = data.keys()[l[0]]\n",
    "    enr2 = data.keys()[l[1]]\n",
    "    enr3 = data.keys()[l[2]]\n",
    "\n",
    "    en_data1 = data[enr1]\n",
    "    en_data2 = data[enr2]\n",
    "    en_data3 = data[enr3]\n",
    "    \n",
    "    X_data1[ptr,:,:] = en_data1\n",
    "    X_data2[ptr,:,:] = en_data2\n",
    "    X_data3[ptr,:,:] = en_data3\n",
    "      \n",
    "    ptr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate the corresponding masks\n",
    "mask1 = np.zeros((nSpk,maxlen), dtype='float32')\n",
    "mask2 = np.zeros((nSpk,maxlen), dtype='float32')\n",
    "mask3 = np.zeros((nSpk,maxlen), dtype='float32')\n",
    "\n",
    "for i in range(nSpk):\n",
    "    \n",
    "    ones1 = X_data1[i][:,1][np.nonzero(X_data1[i][:,1])]\n",
    "    ones2 = X_data2[i][:,1][np.nonzero(X_data2[i][:,1])]\n",
    "    ones3 = X_data3[i][:,1][np.nonzero(X_data3[i][:,1])]\n",
    "    \n",
    "    mask1[i,:ones1.shape[0]] = 1.0\n",
    "    mask2[i,:ones2.shape[0]] = 1.0\n",
    "    mask3[i,:ones3.shape[0]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = zip(X_data1, mask1,labels, X_data2, mask2,labels, X_data3, mask3,labels)\n",
    "random.shuffle(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1[2][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA = np.vstack((X_data1, X_data2, X_data3))\n",
    "MASK = np.vstack((mask1, mask2, mask3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data = np.asarray(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 600, 20)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 680 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = theano.shared(np.asarray(DATA, dtype=theano.config.floatX), borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = data1[:98,:,:]\n",
    "d2 = data1[98:196,:,:]\n",
    "d3 = data1[196:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "for i in range(98):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the data matrices will be loaded as theano shared variables (data, masks and labels)\n",
    "Shuffle and load them, and then slice mini-batches out of each\n",
    "e.g. 16,16,16 so a batch size of 48\n",
    "After forward propogating throught the RNN, we need to do some grouping, before decision making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import htkmfc\n",
    "import random\n",
    "\n",
    "dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "feat_dim = 20\n",
    "maxlen=600 #pad all utterances to this length\n",
    "\n",
    "f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/Allspks_labs.plst')\n",
    "lines = f1.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "\n",
    "audio = [l.split()[0] for l in lines]\n",
    "labs = [l.split()[1] for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = np.zeros((len(audio), maxlen, feat_dim), dtype='float32')\n",
    "\n",
    "for ind,l in enumerate(audio):\n",
    "    fname = os.path.join(dpth,l+'.fea')\n",
    "    fi = htkmfc.HTKFeat_read(fname)\n",
    "    data = fi.getall()[:,:20]\n",
    "    pad = np.zeros(((maxlen-data.shape[0]), 20), dtype='float32')\n",
    "    data = np.vstack((data,pad))\n",
    "    Data[ind,:,:] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = theano.shared(Data, borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labs = theano.shared(np.asarray(labs, dtype=theano.config.floatX), borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
