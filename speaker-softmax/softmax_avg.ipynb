{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask = np.zeros((20,600), dtype='float32')\n",
    "mask[0,:433] = 1.0\n",
    "mask[1,:226] = 1.0\n",
    "\n",
    "#mask = mask[:, None]\n",
    "x_dummy = np.random.random((20,600,20))\n",
    "x_dummy = np.cast['float32'](x_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Train_feats_labs.plst')\n",
    "    lines = f1.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    labelz = [int(l.split()[1]) for l in lines] \n",
    "    #labelz = labelz[:20]\n",
    "    features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    f2 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Valid_feats_labs.plst')\n",
    "    lines = f2.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    val_labelz = [int(l.split()[1]) for l in lines] \n",
    "    #val_labelz = val_labelz[:20]\n",
    "    val_features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    n_samp = len(features)\n",
    "    maxlen=600 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "    nSpk = 98\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    Data = np.zeros((n_samp, maxlen, feat_dim), dtype='float32')\n",
    "    Mask = np.zeros((n_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "    \n",
    "    vn_samp = len(val_features)\n",
    "    val_Data = np.zeros((vn_samp, maxlen, feat_dim), dtype='float32')\n",
    "    val_Mask = np.zeros((vn_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "\n",
    "    for ind,f in enumerate(features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        Data[ind,:,:] = data\n",
    "        \n",
    "\n",
    "    for ind,f in enumerate(val_features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        val_Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        val_Data[ind,:,:] = data\n",
    "\n",
    "\n",
    "    return Data, Mask, np.asarray(labelz, dtype='int32'), val_Data, val_Mask, np.asarray(val_labelz, dtype='int32')\n",
    "\n",
    "def iterate_minibatches(inputs, mask, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], mask[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 600\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 400\n",
    "F_DIM = 20\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 20\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 5\n",
    "nSpk = 98\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "target = T.matrix(name='target_values', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None,None, F_DIM), input_var = X)\n",
    "n_batch,_,_ = l_in.input_var.shape\n",
    "#print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "#initialize the gates\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask,\n",
    "                                    backwards=True)\n",
    "l_sum = lasagne.layers.ConcatLayer([l_forward, l_backward], axis=2)\n",
    "\n",
    "#l_avg = lasagne.layers.FeaturePoolLayer(l_sum, pool_size=600)\n",
    "#print lasagne.layers.get_output(l_sum, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "#l_reshape = lasagne.layers.ReshapeLayer(l_avg,(n_batch, N_HIDDEN))\n",
    "#print lasagne.layers.get_output(l_reshape, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "sum_op = lasagne.layers.get_output(l_sum, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask}).shape\n",
    "avg_emb = T.mean(sum_op, axis=1)\n",
    "#print avg_emb.eval({X: x_dummy, Mask: mask}).shape\n",
    "#l_proj = lasagne.layers.DenseLayer(l_concat, num_units=64, nonlinearity=lasagne.nonlinearities.linear)\n",
    "\n",
    "l_in2 = lasagne.layers.InputLayer(shape=(None, 2*N_HIDDEN), input_var = avg_emb)\n",
    "\n",
    "l_softmax = lasagne.layers.DenseLayer(l_in2, num_units=nSpk, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#print lasagne.layers.get_output(l_softmax, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W_in_to_updategate, (20, 400))\n",
      "(W_hid_to_updategate, (400, 400))\n",
      "(b_updategate, (400,))\n",
      "(W_in_to_resetgate, (20, 400))\n",
      "(W_hid_to_resetgate, (400, 400))\n",
      "(b_resetgate, (400,))\n",
      "(W_in_to_hidden_update, (20, 400))\n",
      "(W_hid_to_hidden_update, (400, 400))\n",
      "(b_hidden_update, (400,))\n",
      "(W_in_to_updategate, (20, 400))\n",
      "(W_hid_to_updategate, (400, 400))\n",
      "(b_updategate, (400,))\n",
      "(W_in_to_resetgate, (20, 400))\n",
      "(W_hid_to_resetgate, (400, 400))\n",
      "(b_resetgate, (400,))\n",
      "(W_in_to_hidden_update, (20, 400))\n",
      "(W_hid_to_hidden_update, (400, 400))\n",
      "(b_hidden_update, (400,))\n",
      "(W, (800, 98))\n",
      "(b, (98,))\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "labels = T.ivector(name='labels')\n",
    "\n",
    "network_output = lasagne.layers.get_output(l_softmax)\n",
    "val_prediction = lasagne.layers.get_output(l_softmax, deterministic=True)\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), labels), dtype=theano.config.floatX)\n",
    "#training accuracy\n",
    "train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), labels), dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "total_cost = lasagne.objectives.categorical_crossentropy(network_output, labels)\n",
    "#total_cost = -(labels*T.log(network_output) + (1-labels)*T.log(1-network_output)) \n",
    "mean_cost = total_cost.mean()\n",
    "#accuracy function\n",
    "val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, labels)\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "params1 = lasagne.layers.get_all_params([l_sum], trainable=True)\n",
    "params2 = lasagne.layers.get_all_params([l_softmax], trainable=True)\n",
    "\n",
    "all_parameters = params1 + params2\n",
    "\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 of 20 took 58.871s\n",
      "  training loss:\t\t4.002770\n",
      "  training accuracy:\t\t14.63 %\n",
      "  validation loss:\t\t3.752385\n",
      "  validation accuracy:\t\t12.50 %\n",
      "Epoch 2 of 20 took 58.871s\n",
      "  training loss:\t\t2.665549\n",
      "  training accuracy:\t\t41.84 %\n",
      "  validation loss:\t\t2.971425\n",
      "  validation accuracy:\t\t33.00 %\n",
      "Epoch 3 of 20 took 58.593s\n",
      "  training loss:\t\t1.713792\n",
      "  training accuracy:\t\t65.40 %\n",
      "  validation loss:\t\t2.583419\n",
      "  validation accuracy:\t\t36.50 %\n",
      "Epoch 4 of 20 took 58.529s\n",
      "  training loss:\t\t1.015449\n",
      "  training accuracy:\t\t82.09 %\n",
      "  validation loss:\t\t2.087509\n",
      "  validation accuracy:\t\t47.00 %\n",
      "Epoch 5 of 20 took 58.771s\n",
      "  training loss:\t\t0.596873\n",
      "  training accuracy:\t\t91.92 %\n",
      "  validation loss:\t\t1.835692\n",
      "  validation accuracy:\t\t51.50 %\n",
      "Epoch 6 of 20 took 58.570s\n",
      "  training loss:\t\t0.355473\n",
      "  training accuracy:\t\t96.65 %\n",
      "  validation loss:\t\t1.733034\n",
      "  validation accuracy:\t\t54.50 %\n",
      "Epoch 7 of 20 took 58.036s\n",
      "  training loss:\t\t0.188827\n",
      "  training accuracy:\t\t99.01 %\n",
      "  validation loss:\t\t1.548486\n",
      "  validation accuracy:\t\t56.50 %\n",
      "Epoch 8 of 20 took 58.036s\n",
      "  training loss:\t\t0.085207\n",
      "  training accuracy:\t\t99.92 %\n",
      "  validation loss:\t\t1.416046\n",
      "  validation accuracy:\t\t61.50 %\n",
      "Epoch 9 of 20 took 58.048s\n",
      "  training loss:\t\t0.045203\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.468191\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 10 of 20 took 58.031s\n",
      "  training loss:\t\t0.032096\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.306638\n",
      "  validation accuracy:\t\t65.50 %\n",
      "Epoch 11 of 20 took 58.036s\n",
      "  training loss:\t\t0.021087\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.305337\n",
      "  validation accuracy:\t\t64.00 %\n",
      "Epoch 12 of 20 took 58.040s\n",
      "  training loss:\t\t0.016371\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.260904\n",
      "  validation accuracy:\t\t65.50 %\n",
      "Epoch 13 of 20 took 58.023s\n",
      "  training loss:\t\t0.013427\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.251338\n",
      "  validation accuracy:\t\t66.50 %\n",
      "Epoch 14 of 20 took 58.023s\n",
      "  training loss:\t\t0.011319\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.248848\n",
      "  validation accuracy:\t\t66.50 %\n",
      "Epoch 15 of 20 took 58.029s\n",
      "  training loss:\t\t0.009623\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.246738\n",
      "  validation accuracy:\t\t67.00 %\n",
      "Epoch 16 of 20 took 58.031s\n",
      "  training loss:\t\t0.008394\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.240690\n",
      "  validation accuracy:\t\t67.00 %\n",
      "Epoch 17 of 20 took 58.038s\n",
      "  training loss:\t\t0.007311\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.234229\n",
      "  validation accuracy:\t\t67.00 %\n",
      "Epoch 18 of 20 took 58.031s\n",
      "  training loss:\t\t0.006457\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.232349\n",
      "  validation accuracy:\t\t68.00 %\n",
      "Epoch 19 of 20 took 58.033s\n",
      "  training loss:\t\t0.005724\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.232325\n",
      "  validation accuracy:\t\t67.00 %\n",
      "Epoch 20 of 20 took 58.052s\n",
      "  training loss:\t\t0.005145\n",
      "  training accuracy:\t\t100.00 %\n",
      "  validation loss:\t\t1.231022\n",
      "  validation accuracy:\t\t67.00 %\n"
     ]
    }
   ],
   "source": [
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-5,5) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,5)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.001)\n",
    "\n",
    "train_func = theano.function([X, Mask, labels], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, labels], [val_mcost, val_acc])\n",
    "\n",
    "\n",
    "num_epochs=20\n",
    "#load the dataset\n",
    "Data, Msk, Targets, val_Data, val_Msk, val_tars = load_dataset()\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(Data, Msk, Targets, 16):\n",
    "        t_data, t_mask, t_labs = batch\n",
    "        terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "        train_err += terr\n",
    "        tr_acc += tacc\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(val_Data, val_Msk, val_tars, 8, shuffle=False):\n",
    "        v_data, v_mask, v_tars = batch\n",
    "        err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "        \n",
    "# Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "    epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(\n",
    "        tr_acc / train_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
