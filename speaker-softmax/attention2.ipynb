{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask = np.zeros((4,600), dtype='float32')\n",
    "mask[0,:433] = 1.0\n",
    "mask[1,:226] = 1.0\n",
    "mask[2,:115] = 1.0\n",
    "mask[3,:545] = 1.0\n",
    "\n",
    "#mask = mask[:, None]\n",
    "x_dummy = np.random.random((4,600,20))\n",
    "x_dummy = np.cast['float32'](x_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled)\n",
      "/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/scan_module/scan.py:1019: Warning: In the strict mode, all neccessary shared variables must be passed as a part of non_sequences\n",
      "  'must be passed as a part of non_sequences', Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W_in_to_updategate, (20, 600))\n",
      "(W_hid_to_updategate, (600, 600))\n",
      "(b_updategate, (600,))\n",
      "(W_in_to_resetgate, (20, 600))\n",
      "(W_hid_to_resetgate, (600, 600))\n",
      "(b_resetgate, (600,))\n",
      "(W_in_to_hidden_update, (20, 600))\n",
      "(W_hid_to_hidden_update, (600, 600))\n",
      "(b_hidden_update, (600,))\n",
      "(W_in_to_updategate, (20, 600))\n",
      "(W_hid_to_updategate, (600, 600))\n",
      "(b_updategate, (600,))\n",
      "(W_in_to_resetgate, (20, 600))\n",
      "(W_hid_to_resetgate, (600, 600))\n",
      "(b_resetgate, (600,))\n",
      "(W_in_to_hidden_update, (20, 600))\n",
      "(W_hid_to_hidden_update, (600, 600))\n",
      "(b_hidden_update, (600,))\n",
      "(W, (600, 1))\n",
      "(b, (1,))\n",
      "(W, (600, 98))\n",
      "(b, (98,))\n",
      "----------------------------------------\n",
      "Starting training...\n",
      "Epoch 1 of 300 took 27.339s \n",
      "  training loss:\t\t4.468714 \n",
      "  training accuracy:\t\t3.51 % \n",
      "  validation loss:\t\t4.316285 \n",
      "  validation accuracy:\t\t0.78 % \n",
      "Epoch 2 of 300 took 27.276s \n",
      "  training loss:\t\t4.041635 \n",
      "  training accuracy:\t\t6.79 % \n",
      "  validation loss:\t\t4.171183 \n",
      "  validation accuracy:\t\t1.17 % \n",
      "Epoch 3 of 300 took 27.382s \n",
      "  training loss:\t\t3.709581 \n",
      "  training accuracy:\t\t11.18 % \n",
      "  validation loss:\t\t3.887392 \n",
      "  validation accuracy:\t\t10.94 % \n",
      "Epoch 4 of 300 took 27.523s \n",
      "  training loss:\t\t3.345374 \n",
      "  training accuracy:\t\t18.69 % \n",
      "  validation loss:\t\t3.494806 \n",
      "  validation accuracy:\t\t14.45 % \n",
      "Epoch 5 of 300 took 27.464s \n",
      "  training loss:\t\t3.009670 \n",
      "  training accuracy:\t\t32.35 % \n",
      "  validation loss:\t\t3.288712 \n",
      "  validation accuracy:\t\t19.14 % \n",
      "Epoch 6 of 300 took 27.517s \n",
      "  training loss:\t\t2.685077 \n",
      "  training accuracy:\t\t36.27 % \n",
      "  validation loss:\t\t3.091725 \n",
      "  validation accuracy:\t\t21.88 % \n",
      "Epoch 7 of 300 took 27.439s \n",
      "  training loss:\t\t2.266902 \n",
      "  training accuracy:\t\t50.72 % \n",
      "  validation loss:\t\t2.984791 \n",
      "  validation accuracy:\t\t29.30 % \n",
      "Epoch 8 of 300 took 27.436s \n",
      "  training loss:\t\t1.881581 \n",
      "  training accuracy:\t\t61.82 % \n",
      "  validation loss:\t\t2.648568 \n",
      "  validation accuracy:\t\t33.59 % \n",
      "Epoch 9 of 300 took 27.376s \n",
      "  training loss:\t\t1.509053 \n",
      "  training accuracy:\t\t71.75 % \n",
      "  validation loss:\t\t2.371267 \n",
      "  validation accuracy:\t\t42.19 % \n",
      "Epoch 10 of 300 took 27.416s \n",
      "  training loss:\t\t1.190069 \n",
      "  training accuracy:\t\t78.22 % \n",
      "  validation loss:\t\t2.229951 \n",
      "  validation accuracy:\t\t42.19 % \n",
      "Epoch 11 of 300 took 27.528s \n",
      "  training loss:\t\t0.911929 \n",
      "  training accuracy:\t\t88.46 % \n",
      "  validation loss:\t\t1.908273 \n",
      "  validation accuracy:\t\t51.95 % \n",
      "Epoch 12 of 300 took 27.520s \n",
      "  training loss:\t\t0.635183 \n",
      "  training accuracy:\t\t93.05 % \n",
      "  validation loss:\t\t1.929469 \n",
      "  validation accuracy:\t\t48.44 % \n",
      "Epoch 13 of 300 took 27.256s \n",
      "  training loss:\t\t0.464342 \n",
      "  training accuracy:\t\t96.88 % \n",
      "  validation loss:\t\t1.725776 \n",
      "  validation accuracy:\t\t52.34 % \n",
      "Epoch 14 of 300 took 27.264s \n",
      "  training loss:\t\t0.324533 \n",
      "  training accuracy:\t\t97.85 % \n",
      "  validation loss:\t\t1.523749 \n",
      "  validation accuracy:\t\t59.38 % \n",
      "Epoch 15 of 300 took 27.182s \n",
      "  training loss:\t\t0.278208 \n",
      "  training accuracy:\t\t97.60 % \n",
      "  validation loss:\t\t1.540184 \n",
      "  validation accuracy:\t\t61.72 % \n",
      "Epoch 16 of 300 took 27.182s \n",
      "  training loss:\t\t0.308257 \n",
      "  training accuracy:\t\t98.44 % \n",
      "  validation loss:\t\t1.312672 \n",
      "  validation accuracy:\t\t68.36 % \n",
      "Epoch 17 of 300 took 27.244s \n",
      "  training loss:\t\t0.253868 \n",
      "  training accuracy:\t\t98.05 % \n",
      "  validation loss:\t\t1.449160 \n",
      "  validation accuracy:\t\t61.33 % \n",
      "Epoch 18 of 300 took 27.139s \n",
      "  training loss:\t\t0.190699 \n",
      "  training accuracy:\t\t99.41 % \n",
      "  validation loss:\t\t1.447413 \n",
      "  validation accuracy:\t\t54.69 % \n",
      "Epoch 19 of 300 took 27.121s \n",
      "  training loss:\t\t0.168647 \n",
      "  training accuracy:\t\t98.96 % \n",
      "  validation loss:\t\t1.228851 \n",
      "  validation accuracy:\t\t62.89 % \n",
      "Epoch 20 of 300 took 27.111s \n",
      "  training loss:\t\t0.121550 \n",
      "  training accuracy:\t\t99.80 % \n",
      "  validation loss:\t\t1.160693 \n",
      "  validation accuracy:\t\t64.84 % \n",
      "Epoch 21 of 300 took 27.128s \n",
      "  training loss:\t\t0.100991 \n",
      "  training accuracy:\t\t99.45 % \n",
      "  validation loss:\t\t1.142474 \n",
      "  validation accuracy:\t\t64.84 % \n",
      "Epoch 22 of 300 took 27.204s \n",
      "  training loss:\t\t0.082866 \n",
      "  training accuracy:\t\t99.54 % \n",
      "  validation loss:\t\t1.152193 \n",
      "  validation accuracy:\t\t66.02 % \n",
      "Epoch 23 of 300 took 27.213s \n",
      "  training loss:\t\t0.077419 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.198323 \n",
      "  validation accuracy:\t\t64.06 % \n",
      "Epoch 24 of 300 took 27.142s \n",
      "  training loss:\t\t0.072419 \n",
      "  training accuracy:\t\t99.93 % \n",
      "  validation loss:\t\t1.151133 \n",
      "  validation accuracy:\t\t69.14 % \n",
      "Epoch 25 of 300 took 27.089s \n",
      "  training loss:\t\t0.103225 \n",
      "  training accuracy:\t\t98.63 % \n",
      "  validation loss:\t\t1.257519 \n",
      "  validation accuracy:\t\t63.28 % \n",
      "Epoch 26 of 300 took 27.073s \n",
      "  training loss:\t\t0.158612 \n",
      "  training accuracy:\t\t97.72 % \n",
      "  validation loss:\t\t1.226253 \n",
      "  validation accuracy:\t\t66.02 % \n",
      "Epoch 27 of 300 took 27.162s \n",
      "  training loss:\t\t0.128309 \n",
      "  training accuracy:\t\t98.93 % \n",
      "  validation loss:\t\t1.278785 \n",
      "  validation accuracy:\t\t69.53 % \n",
      "Epoch 28 of 300 took 27.242s \n",
      "  training loss:\t\t0.275569 \n",
      "  training accuracy:\t\t96.43 % \n",
      "  validation loss:\t\t1.525471 \n",
      "  validation accuracy:\t\t62.89 % \n",
      "Epoch 29 of 300 took 27.217s \n",
      "  training loss:\t\t0.408329 \n",
      "  training accuracy:\t\t94.54 % \n",
      "  validation loss:\t\t1.632562 \n",
      "  validation accuracy:\t\t60.94 % \n",
      "Epoch 30 of 300 took 27.194s \n",
      "  training loss:\t\t0.448684 \n",
      "  training accuracy:\t\t93.30 % \n",
      "  validation loss:\t\t1.459592 \n",
      "  validation accuracy:\t\t62.50 % \n",
      "Epoch 31 of 300 took 27.159s \n",
      "  training loss:\t\t0.386431 \n",
      "  training accuracy:\t\t95.03 % \n",
      "  validation loss:\t\t1.568407 \n",
      "  validation accuracy:\t\t58.20 % \n",
      "Epoch 32 of 300 took 27.101s \n",
      "  training loss:\t\t0.345370 \n",
      "  training accuracy:\t\t94.38 % \n",
      "  validation loss:\t\t1.589074 \n",
      "  validation accuracy:\t\t55.08 % \n",
      "Epoch 33 of 300 took 27.136s \n",
      "  training loss:\t\t0.260935 \n",
      "  training accuracy:\t\t97.27 % \n",
      "  validation loss:\t\t1.355803 \n",
      "  validation accuracy:\t\t60.55 % \n",
      "Epoch 34 of 300 took 27.260s \n",
      "  training loss:\t\t0.186362 \n",
      "  training accuracy:\t\t98.21 % \n",
      "  validation loss:\t\t1.230859 \n",
      "  validation accuracy:\t\t65.62 % \n",
      "Epoch 35 of 300 took 27.090s \n",
      "  training loss:\t\t0.117520 \n",
      "  training accuracy:\t\t99.54 % \n",
      "  validation loss:\t\t1.243369 \n",
      "  validation accuracy:\t\t64.84 % \n",
      "Epoch 36 of 300 took 27.131s \n",
      "  training loss:\t\t0.074720 \n",
      "  training accuracy:\t\t99.87 % \n",
      "  validation loss:\t\t1.137276 \n",
      "  validation accuracy:\t\t67.19 % \n",
      "Epoch 37 of 300 took 27.161s \n",
      "  training loss:\t\t0.044483 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.080160 \n",
      "  validation accuracy:\t\t69.14 % \n",
      "Epoch 38 of 300 took 27.154s \n",
      "  training loss:\t\t0.035635 \n",
      "  training accuracy:\t\t99.93 % \n",
      "  validation loss:\t\t1.006271 \n",
      "  validation accuracy:\t\t70.70 % \n",
      "Epoch 39 of 300 took 27.111s \n",
      "  training loss:\t\t0.023717 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.919807 \n",
      "  validation accuracy:\t\t70.70 % \n",
      "Epoch 40 of 300 took 27.229s \n",
      "  training loss:\t\t0.016544 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.924489 \n",
      "  validation accuracy:\t\t70.70 % \n",
      "Epoch 41 of 300 took 27.241s \n",
      "  training loss:\t\t0.012822 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.952013 \n",
      "  validation accuracy:\t\t69.14 % \n",
      "Epoch 42 of 300 took 27.119s \n",
      "  training loss:\t\t0.010410 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.960165 \n",
      "  validation accuracy:\t\t70.31 % \n",
      "Epoch 43 of 300 took 27.149s \n",
      "  training loss:\t\t0.008281 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.961520 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 44 of 300 took 27.182s \n",
      "  training loss:\t\t0.006927 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.951185 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 45 of 300 took 27.155s \n",
      "  training loss:\t\t0.006226 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.949745 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 46 of 300 took 27.125s \n",
      "  training loss:\t\t0.005535 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.949869 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 47 of 300 took 27.272s \n",
      "  training loss:\t\t0.005030 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.947181 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 48 of 300 took 27.290s \n",
      "  training loss:\t\t0.004601 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.948354 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 49 of 300 took 27.255s \n",
      "  training loss:\t\t0.004199 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.947264 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 50 of 300 took 27.112s \n",
      "  training loss:\t\t0.004045 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.946891 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 51 of 300 took 27.187s \n",
      "  training loss:\t\t0.003680 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.946728 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 52 of 300 took 27.179s \n",
      "  training loss:\t\t0.003527 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.945460 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 53 of 300 took 27.208s \n",
      "  training loss:\t\t0.003283 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.944471 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 54 of 300 took 27.181s \n",
      "  training loss:\t\t0.003276 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.948190 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 55 of 300 took 27.115s \n",
      "  training loss:\t\t0.003088 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.953344 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 56 of 300 took 27.099s \n",
      "  training loss:\t\t0.002929 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.954262 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 57 of 300 took 27.060s \n",
      "  training loss:\t\t0.002834 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.950514 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 58 of 300 took 27.124s \n",
      "  training loss:\t\t0.002662 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.953575 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 59 of 300 took 27.206s \n",
      "  training loss:\t\t0.002584 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.954885 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 60 of 300 took 27.119s \n",
      "  training loss:\t\t0.002375 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.955013 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 61 of 300 took 27.157s \n",
      "  training loss:\t\t0.002383 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.954850 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 62 of 300 took 27.085s \n",
      "  training loss:\t\t0.002326 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.955774 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 63 of 300 took 27.190s \n",
      "  training loss:\t\t0.002215 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.954788 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 64 of 300 took 27.147s \n",
      "  training loss:\t\t0.002145 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.956463 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 65 of 300 took 27.198s \n",
      "  training loss:\t\t0.002072 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.957545 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 66 of 300 took 27.114s \n",
      "  training loss:\t\t0.001949 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.957785 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 67 of 300 took 27.135s \n",
      "  training loss:\t\t0.001958 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.962645 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 68 of 300 took 27.152s \n",
      "  training loss:\t\t0.001912 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.961566 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 69 of 300 took 27.222s \n",
      "  training loss:\t\t0.001907 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.960078 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 70 of 300 took 27.142s \n",
      "  training loss:\t\t0.001752 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.961356 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 71 of 300 took 27.093s \n",
      "  training loss:\t\t0.001779 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.960940 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 72 of 300 took 27.078s \n",
      "  training loss:\t\t0.001711 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.964348 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 73 of 300 took 27.170s \n",
      "  training loss:\t\t0.001676 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.965742 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 74 of 300 took 27.178s \n",
      "  training loss:\t\t0.001624 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.965732 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 75 of 300 took 27.168s \n",
      "  training loss:\t\t0.001626 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.970252 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 76 of 300 took 27.089s \n",
      "  training loss:\t\t0.001584 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.968805 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 77 of 300 took 27.132s \n",
      "  training loss:\t\t0.001495 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.969897 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 78 of 300 took 27.139s \n",
      "  training loss:\t\t0.001464 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.973147 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 79 of 300 took 27.224s \n",
      "  training loss:\t\t0.001437 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.975355 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 80 of 300 took 27.161s \n",
      "  training loss:\t\t0.001406 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.975671 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 81 of 300 took 27.101s \n",
      "  training loss:\t\t0.001337 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.975369 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 82 of 300 took 27.165s \n",
      "  training loss:\t\t0.001331 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.975870 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 83 of 300 took 27.137s \n",
      "  training loss:\t\t0.001346 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.976577 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 84 of 300 took 27.107s \n",
      "  training loss:\t\t0.001332 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.976503 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 85 of 300 took 27.111s \n",
      "  training loss:\t\t0.001276 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.977093 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 86 of 300 took 27.150s \n",
      "  training loss:\t\t0.001229 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.977742 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 87 of 300 took 27.156s \n",
      "  training loss:\t\t0.001171 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.978503 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 88 of 300 took 27.108s \n",
      "  training loss:\t\t0.001141 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.978776 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 89 of 300 took 27.113s \n",
      "  training loss:\t\t0.001157 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.978416 \n",
      "  validation accuracy:\t\t72.66 % \n",
      "Epoch 90 of 300 took 27.109s \n",
      "  training loss:\t\t0.001123 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.978095 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 91 of 300 took 27.130s \n",
      "  training loss:\t\t0.001085 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.976356 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 92 of 300 took 27.150s \n",
      "  training loss:\t\t0.001090 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.977870 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 93 of 300 took 27.104s \n",
      "  training loss:\t\t0.001091 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.978122 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 94 of 300 took 27.267s \n",
      "  training loss:\t\t0.001067 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.979319 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 95 of 300 took 27.152s \n",
      "  training loss:\t\t0.001056 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.979802 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 96 of 300 took 27.140s \n",
      "  training loss:\t\t0.001024 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.981209 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 97 of 300 took 27.100s \n",
      "  training loss:\t\t0.001002 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.982240 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 98 of 300 took 27.251s \n",
      "  training loss:\t\t0.000990 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.980958 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 99 of 300 took 27.739s \n",
      "  training loss:\t\t0.000971 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.981177 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 100 of 300 took 27.548s \n",
      "  training loss:\t\t0.000948 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.982945 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 101 of 300 took 27.199s \n",
      "  training loss:\t\t0.000926 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.985318 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 102 of 300 took 27.185s \n",
      "  training loss:\t\t0.000938 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.982400 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 103 of 300 took 27.131s \n",
      "  training loss:\t\t0.000896 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.982794 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 104 of 300 took 27.230s \n",
      "  training loss:\t\t0.000912 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.981863 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 105 of 300 took 27.141s \n",
      "  training loss:\t\t0.000879 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.981893 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 106 of 300 took 27.106s \n",
      "  training loss:\t\t0.000868 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.981043 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 107 of 300 took 27.107s \n",
      "  training loss:\t\t0.000854 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.980300 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 108 of 300 took 27.990s \n",
      "  training loss:\t\t0.000832 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.976713 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 109 of 300 took 27.809s \n",
      "  training loss:\t\t0.000816 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.976350 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 110 of 300 took 27.174s \n",
      "  training loss:\t\t0.000821 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.980334 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 111 of 300 took 27.099s \n",
      "  training loss:\t\t0.000803 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.982151 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 112 of 300 took 27.648s \n",
      "  training loss:\t\t0.000760 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.984736 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 113 of 300 took 27.225s \n",
      "  training loss:\t\t0.000754 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.981140 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 114 of 300 took 27.159s \n",
      "  training loss:\t\t0.000741 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.979511 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 115 of 300 took 27.242s \n",
      "  training loss:\t\t0.000747 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.987125 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 116 of 300 took 27.232s \n",
      "  training loss:\t\t0.000745 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.989180 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 117 of 300 took 27.534s \n",
      "  training loss:\t\t0.000754 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.990306 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 118 of 300 took 27.777s \n",
      "  training loss:\t\t0.000700 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.988451 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 119 of 300 took 27.766s \n",
      "  training loss:\t\t0.000717 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.991130 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 120 of 300 took 27.845s \n",
      "  training loss:\t\t0.000689 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.992151 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 121 of 300 took 27.612s \n",
      "  training loss:\t\t0.000675 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.992851 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 122 of 300 took 27.815s \n",
      "  training loss:\t\t0.000672 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.993364 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 123 of 300 took 27.708s \n",
      "  training loss:\t\t0.000664 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.993294 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 124 of 300 took 27.479s \n",
      "  training loss:\t\t0.000666 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.993380 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 125 of 300 took 27.879s \n",
      "  training loss:\t\t0.000638 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.993379 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 126 of 300 took 27.935s \n",
      "  training loss:\t\t0.000641 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.993941 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 127 of 300 took 27.805s \n",
      "  training loss:\t\t0.000615 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.995569 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 128 of 300 took 27.604s \n",
      "  training loss:\t\t0.000635 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.995593 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 129 of 300 took 27.425s \n",
      "  training loss:\t\t0.000604 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.994153 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 130 of 300 took 27.206s \n",
      "  training loss:\t\t0.000592 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.994851 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 131 of 300 took 27.195s \n",
      "  training loss:\t\t0.000605 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.994814 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 132 of 300 took 27.214s \n",
      "  training loss:\t\t0.000599 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.989992 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 133 of 300 took 27.262s \n",
      "  training loss:\t\t0.000575 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.986595 \n",
      "  validation accuracy:\t\t72.27 % \n",
      "Epoch 134 of 300 took 27.109s \n",
      "  training loss:\t\t0.000575 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.986522 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 135 of 300 took 27.234s \n",
      "  training loss:\t\t0.000567 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.989911 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 136 of 300 took 27.109s \n",
      "  training loss:\t\t0.000577 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.988832 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 137 of 300 took 27.075s \n",
      "  training loss:\t\t0.000550 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.981967 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 138 of 300 took 27.150s \n",
      "  training loss:\t\t0.000555 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.981856 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 139 of 300 took 27.116s \n",
      "  training loss:\t\t0.000555 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.984380 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 140 of 300 took 27.735s \n",
      "  training loss:\t\t0.000545 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.984112 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 141 of 300 took 27.624s \n",
      "  training loss:\t\t0.000521 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.984210 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 142 of 300 took 27.155s \n",
      "  training loss:\t\t0.000522 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.981541 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 143 of 300 took 27.155s \n",
      "  training loss:\t\t0.000511 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.980074 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 144 of 300 took 27.160s \n",
      "  training loss:\t\t0.000514 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.979426 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 145 of 300 took 27.202s \n",
      "  training loss:\t\t0.000502 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.979844 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 146 of 300 took 27.119s \n",
      "  training loss:\t\t0.000502 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.979285 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 147 of 300 took 27.046s \n",
      "  training loss:\t\t0.000480 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.985773 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 148 of 300 took 27.135s \n",
      "  training loss:\t\t0.000489 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.987567 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 149 of 300 took 27.160s \n",
      "  training loss:\t\t0.000471 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.989691 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 150 of 300 took 27.201s \n",
      "  training loss:\t\t0.000473 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.992175 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 151 of 300 took 27.196s \n",
      "  training loss:\t\t0.000468 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.995762 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 152 of 300 took 27.172s \n",
      "  training loss:\t\t0.000464 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.993879 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 153 of 300 took 27.181s \n",
      "  training loss:\t\t0.000464 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.994356 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 154 of 300 took 27.158s \n",
      "  training loss:\t\t0.000454 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.994195 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 155 of 300 took 27.102s \n",
      "  training loss:\t\t0.000443 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.996743 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 156 of 300 took 27.202s \n",
      "  training loss:\t\t0.000465 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.996965 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 157 of 300 took 27.242s \n",
      "  training loss:\t\t0.000437 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.997653 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 158 of 300 took 27.677s \n",
      "  training loss:\t\t0.000426 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.998040 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 159 of 300 took 27.703s \n",
      "  training loss:\t\t0.000432 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.000241 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 160 of 300 took 27.102s \n",
      "  training loss:\t\t0.000420 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.000018 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 161 of 300 took 27.149s \n",
      "  training loss:\t\t0.000421 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t0.998675 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 162 of 300 took 27.146s \n",
      "  training loss:\t\t0.000413 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.001418 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 163 of 300 took 27.193s \n",
      "  training loss:\t\t0.000406 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.005689 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 164 of 300 took 27.232s \n",
      "  training loss:\t\t0.000401 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006741 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 165 of 300 took 27.120s \n",
      "  training loss:\t\t0.000411 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006192 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 166 of 300 took 27.169s \n",
      "  training loss:\t\t0.000401 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006177 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 167 of 300 took 27.321s \n",
      "  training loss:\t\t0.000398 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.005467 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 168 of 300 took 27.625s \n",
      "  training loss:\t\t0.000385 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.004892 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 169 of 300 took 27.338s \n",
      "  training loss:\t\t0.000391 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.005055 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 170 of 300 took 27.104s \n",
      "  training loss:\t\t0.000397 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.005446 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 171 of 300 took 27.081s \n",
      "  training loss:\t\t0.000370 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006249 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 172 of 300 took 27.170s \n",
      "  training loss:\t\t0.000384 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006843 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 173 of 300 took 27.208s \n",
      "  training loss:\t\t0.000373 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.007636 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 174 of 300 took 27.203s \n",
      "  training loss:\t\t0.000370 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.007601 \n",
      "  validation accuracy:\t\t71.88 % \n",
      "Epoch 175 of 300 took 27.153s \n",
      "  training loss:\t\t0.000373 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.007650 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 176 of 300 took 27.174s \n",
      "  training loss:\t\t0.000355 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.007400 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 177 of 300 took 27.138s \n",
      "  training loss:\t\t0.000366 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.007180 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 178 of 300 took 27.262s \n",
      "  training loss:\t\t0.000355 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.007115 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 179 of 300 took 27.476s \n",
      "  training loss:\t\t0.000368 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006378 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 180 of 300 took 27.201s \n",
      "  training loss:\t\t0.000340 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.005730 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 181 of 300 took 27.086s \n",
      "  training loss:\t\t0.000340 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.005303 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 182 of 300 took 27.222s \n",
      "  training loss:\t\t0.000347 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.005027 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 183 of 300 took 27.131s \n",
      "  training loss:\t\t0.000345 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.004483 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 184 of 300 took 27.194s \n",
      "  training loss:\t\t0.000331 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.004271 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 185 of 300 took 27.194s \n",
      "  training loss:\t\t0.000336 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.004533 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 186 of 300 took 27.148s \n",
      "  training loss:\t\t0.000335 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.005264 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 187 of 300 took 27.135s \n",
      "  training loss:\t\t0.000341 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.005675 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 188 of 300 took 27.193s \n",
      "  training loss:\t\t0.000321 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006290 \n",
      "  validation accuracy:\t\t71.48 % \n",
      "Epoch 189 of 300 took 27.180s \n",
      "  training loss:\t\t0.000323 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006602 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 190 of 300 took 27.145s \n",
      "  training loss:\t\t0.000316 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006569 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 191 of 300 took 27.185s \n",
      "  training loss:\t\t0.000331 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006539 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 192 of 300 took 27.158s \n",
      "  training loss:\t\t0.000309 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.006713 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 193 of 300 took 27.175s \n",
      "  training loss:\t\t0.000307 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.007075 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 194 of 300 took 27.181s \n",
      "  training loss:\t\t0.000309 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.007595 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 195 of 300 took 27.103s \n",
      "  training loss:\t\t0.000309 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.008115 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 196 of 300 took 27.075s \n",
      "  training loss:\t\t0.000303 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.008377 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 197 of 300 took 27.151s \n",
      "  training loss:\t\t0.000298 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.008460 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 198 of 300 took 27.129s \n",
      "  training loss:\t\t0.000294 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.008503 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Epoch 199 of 300 took 27.224s \n",
      "  training loss:\t\t0.000289 \n",
      "  training accuracy:\t\t100.00 % \n",
      "  validation loss:\t\t1.008631 \n",
      "  validation accuracy:\t\t71.09 % \n",
      "Saving Model ...\n"
     ]
    }
   ],
   "source": [
    "#%%writefile attention2.py\n",
    "\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Train_feats_labs.plst')\n",
    "    lines = f1.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    labelz = [int(l.split()[1]) for l in lines] \n",
    "    #labelz = labelz[:20]\n",
    "    features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    f2 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Valid_feats_labs.plst')\n",
    "    lines = f2.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    val_labelz = [int(l.split()[1]) for l in lines] \n",
    "    #val_labelz = val_labelz[:20]\n",
    "    val_features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    n_samp = len(features)\n",
    "    maxlen=800 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "    nSpk = 98\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    Data = np.zeros((n_samp, maxlen, feat_dim), dtype='float32')\n",
    "    Mask = np.zeros((n_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "    \n",
    "    vn_samp = len(val_features)\n",
    "    val_Data = np.zeros((vn_samp, maxlen, feat_dim), dtype='float32')\n",
    "    val_Mask = np.zeros((vn_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "\n",
    "    for ind,f in enumerate(features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        Data[ind,:,:] = data\n",
    "        \n",
    "\n",
    "    for ind,f in enumerate(val_features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        val_Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        val_Data[ind,:,:] = data\n",
    "\n",
    "\n",
    "    return Data, Mask, np.asarray(labelz, dtype='int32'), val_Data, val_Mask, np.asarray(val_labelz, dtype='int32')\n",
    "\n",
    "def iterate_minibatches(inputs, mask, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    iplen = len(inputs)\n",
    "    pointer = 0\n",
    "    indices=np.arange(iplen)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    while pointer < iplen:\n",
    "    \n",
    "        if pointer <= iplen - batchsize:\n",
    "            excerpt = indices[pointer:pointer+batchsize]\n",
    "        else:\n",
    "            batchsize = iplen-pointer\n",
    "            excerpt = indices[pointer:pointer+batchsize]\n",
    "        \n",
    "        pointer+=batchsize\n",
    "\n",
    "        yield inputs[excerpt], mask[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 600\n",
    "F_DIM = 20\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 1\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "nSpk = 98\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "target = T.matrix(name='target_values', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None, F_DIM), input_var = X)\n",
    "n_batch,maxlen,_ = l_in.input_var.shape\n",
    "\n",
    "#get the batch size for the weights matrix\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "#initialize the gates\n",
    "\n",
    "#Compute Recurrent Embeddings\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask, backwards=True)\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "Recc_emb =  lasagne.layers.get_output(l_sum, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask})\n",
    "#print lasagne.layers.get_output(l_res, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "l_reshape1 = lasagne.layers.ReshapeLayer(l_sum, (-1, N_HIDDEN))\n",
    "\n",
    "l_attend = lasagne.layers.DenseLayer(l_reshape1, num_units=1, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "#print lasagne.layers.get_output(l_attend, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "l_attention = lasagne.layers.ReshapeLayer(l_attend, (n_batch, maxlen))\n",
    "attn_wts = lasagne.layers.get_output(l_attention, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask})\n",
    "attn_probs = T.nnet.softmax(attn_wts)#.eval({X:x_dummy, Mask:mask})\n",
    "#Calculate the last time-step of a recording using the mask.\n",
    "lstep = T.sum(Mask, axis=1)#.eval({Mask : mask})\n",
    "lstep = T.cast(lstep, 'int32')#.eval()\n",
    "lstep = lstep.T\n",
    "\n",
    "#print lasagne.layers.get_output(l_reshape1b, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "Wts = T.zeros([n_batch, maxlen], dtype='float32')\n",
    "\n",
    "def weighted_avg(w_t, l_t, W, R_emb):\n",
    "    W = T.set_subtensor(W[:l_t], w_t[:l_t])\n",
    "    W = W[None,:]\n",
    "    w_sum = T.dot(W, R_emb)\n",
    "    \n",
    "    #this W (1,600) is is then multiplied (dot product) with the recurrent embedding (600, 100)\n",
    "    #to produce a feature vector that is a weighted sum of all the embedding vectors of the recording\n",
    "    #also return the weights for analysis\n",
    "    \n",
    "    return w_sum\n",
    "    \n",
    "U_t,_ = theano.scan(fn=weighted_avg, sequences=[attn_probs, lstep, Wts, Recc_emb])\n",
    "l_in2 = lasagne.layers.InputLayer(shape=(None,None, None), input_var = U_t)\n",
    "n_batch1,l1,hdim = l_in2.input_var.shape\n",
    "\n",
    "l_reshape2 = lasagne.layers.ReshapeLayer(l_in2, (n_batch1*l1,N_HIDDEN))\n",
    "#print lasagne.layers.get_output(l_reshape2, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "#Finally this feature vector(s) gets passed to a dense layer which represents a softmax distribution over speakers\n",
    "l_Spk_softmax = lasagne.layers.DenseLayer(l_reshape2, num_units=98, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#print lasagne.layers.get_output(l_Spk_softmax, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "# lasagne.layers.get_output produces a variable for the output of the net\n",
    "\n",
    "network_output = lasagne.layers.get_output(l_Spk_softmax)\n",
    "\n",
    "labels = T.ivector(name='labels')\n",
    "\n",
    "val_prediction = lasagne.layers.get_output(l_Spk_softmax, deterministic=True)\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), labels), dtype=theano.config.floatX)\n",
    "#training accuracy\n",
    "train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), labels), dtype=theano.config.floatX)\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "total_cost = lasagne.objectives.categorical_crossentropy(network_output, labels)\n",
    "#total_cost = -(labels*T.log(network_MMMmoutput) + (1-labels)*T.log(1-network_output)) \n",
    "mean_cost = total_cost.mean()\n",
    "#accuracy function\n",
    "val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, labels)\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "#Get parameters of both encoder and decoder\n",
    "params1 = lasagne.layers.get_all_params([l_attend], trainable=True)\n",
    "params2 = lasagne.layers.get_all_params([l_Spk_softmax], trainable=True)\n",
    "all_parameters = params1 + params2\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)\n",
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-5,5) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,5)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.005)\n",
    "\n",
    "train_func = theano.function([X, Mask, labels], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, labels], [val_mcost, val_acc])\n",
    "\n",
    "\n",
    "#load the dataset\n",
    "Data, Msk, Targets, val_Data, val_Msk, val_tars = load_dataset()\n",
    "num_epochs=300\n",
    "epoch=0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "val_prev = np.inf\n",
    "\n",
    "#for epoch in range(num_epochs):\n",
    "while('true'):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(Data, Msk, Targets, 256):\n",
    "        t_data, t_mask, t_labs = batch\n",
    "        terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "        train_err += terr\n",
    "        tr_acc += tacc\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(val_Data, val_Msk, val_tars, 64, shuffle=False):\n",
    "        v_data, v_mask, v_tars = batch\n",
    "        err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    \n",
    "    #f_log = open('/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/training.log','a')\n",
    "    epoch+=1\n",
    "\n",
    "# Then we print the results for this epoch:\n",
    "    #flog = open('training.log','a')\n",
    "    print(\"Epoch {} of {} took {:.3f}s \".format(\n",
    "    epoch, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} % \".format(\n",
    "        tr_acc / train_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f} \".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} % \".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    #flog.write('\\n')\n",
    "    #flog.close()\n",
    "   \n",
    "    valE = val_err/val_batches\n",
    "    if valE > val_prev:\n",
    "        c+=1\n",
    "        val_prev=valE\n",
    "    else:\n",
    "        c=0\n",
    "        val_prev=valE\n",
    "    \n",
    "    if c==8:\n",
    "        break\n",
    "    \n",
    "    if epoch==num_epochs:\n",
    "        break\n",
    "        \n",
    "#Save the final model\n",
    "\n",
    "spth = '/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax'\n",
    "\n",
    "print('Saving Model ...')\n",
    "model_params1 = lasagne.layers.get_all_param_values(l_attention)\n",
    "model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)\n",
    "model_params = model_params1 + model_params2\n",
    "\n",
    "model1_name = 'Attn2_softmax_600' + '.pkl'\n",
    "model2_name = 'Attn2_softmax_600b' + '.pkl'\n",
    "\n",
    "vpth1 = os.path.join(spth, model1_name)\n",
    "vpth2 = os.path.join(spth, model2_name)\n",
    "\n",
    "fsave = open(vpth1,'wb')  \n",
    "fsave2 = open(vpth2,'wb')  \n",
    "\n",
    "cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(model_params2, fsave2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "fsave.close()\n",
    "fsave2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load trained model\n",
    "f_load1 = open('/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax/Attn2_softmax_700.pkl')\n",
    "f_load2 = open('/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax/Attn2_softmax_700b.pkl')\n",
    "\n",
    "Wts1 = cPickle.load(f_load1)\n",
    "Wts2 = cPickle.load(f_load2)\n",
    "\n",
    "lasagne.layers.set_all_param_values(l_Spk_softmax, Wts2)\n",
    "lasagne.layers.set_all_param_values(l_attend, Wts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_params1 = lasagne.layers.get_all_param_values(l_attend)\n",
    "model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)\n",
    "model_params = model_params1 + model_params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n",
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W_in_to_updategate, (20, 600))\n",
      "(W_hid_to_updategate, (600, 600))\n",
      "(b_updategate, (600,))\n",
      "(W_in_to_resetgate, (20, 600))\n",
      "(W_hid_to_resetgate, (600, 600))\n",
      "(b_resetgate, (600,))\n",
      "(W_in_to_hidden_update, (20, 600))\n",
      "(W_hid_to_hidden_update, (600, 600))\n",
      "(b_hidden_update, (600,))\n",
      "(W_in_to_updategate, (20, 600))\n",
      "(W_hid_to_updategate, (600, 600))\n",
      "(b_updategate, (600,))\n",
      "(W_in_to_resetgate, (20, 600))\n",
      "(W_hid_to_resetgate, (600, 600))\n",
      "(b_resetgate, (600,))\n",
      "(W_in_to_hidden_update, (20, 600))\n",
      "(W_hid_to_hidden_update, (600, 600))\n",
      "(b_hidden_update, (600,))\n",
      "(W, (600, 1))\n",
      "(b, (1,))\n",
      "(W, (600, 98))\n",
      "(b, (98,))\n",
      "----------------------------------------\n",
      "Starting training...\n",
      "Epoch 1 of 200 took 45.325s \n",
      "  training loss:\t\t4.324335 \n",
      "  training accuracy:\t\t5.60 % \n",
      "  validation loss:\t\t4.172675 \n",
      "  validation accuracy:\t\t3.52 % \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/misc/data15/reco/bhattgau/Rnn/Code/notebooks/speaker-softmax/attention2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMsk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mt_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_labs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0mterr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtacc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_labs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mterr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mtr_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtacc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    876\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    879\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run attention2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
