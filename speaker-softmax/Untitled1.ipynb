{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask = np.zeros((2,600), dtype='float32')\n",
    "\n",
    "mask[0,:433] = 1.0\n",
    "mask[1,:226] = 1.0\n",
    "\n",
    "#mask = mask[:, None]\n",
    "x_dummy = np.random.random((2,600,20))\n",
    "x_dummy = np.cast['float32'](x_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fuel.datasets import H5PYDataset\n",
    "\n",
    "train_set = H5PYDataset('/misc/data15/reco/bhattgau/Rnn/lists/rsr/rsr_train3.hdf5',\n",
    "                        which_sets=('train',), subset=slice(0,1))\n",
    "handle = train_set.open()\n",
    "data = train_set.get_data(handle, slice(0,1))\n",
    "\n",
    "x = data[0]\n",
    "mask = data[1]\n",
    "phr_targets = data[2]\n",
    "spk_targets = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n"
     ]
    }
   ],
   "source": [
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import ipdb\n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 100\n",
    "F_DIM = 60\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 1\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "nSpk = 194\n",
    "nPhr = 73\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = X)\n",
    "n_batch,_,_ = l_in.input_var.shape\n",
    "#print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "#initialize the gates\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask, only_return_final=True)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask, only_return_final=True,\n",
    "                                     backwards=True)\n",
    "\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "l_spk_softmax = lasagne.layers.DenseLayer(l_sum, num_units=nSpk, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "l_phr_softmax = lasagne.layers.DenseLayer(l_sum, num_units=nPhr, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "#print lasagne.layers.get_output(l_softmax, inputs={l_in: X, l_mask: Mask}).eval({X: d1, Mask: m1}).shape\n",
    "#l_softmax = lasagne.layers.ReshapeLayer(l_dense, (n_batch, MAX_LENGTH, N_HIDDEN))\n",
    "\n",
    "spk_labels = T.ivector(name='spk_labels')\n",
    "phr_labels = T.ivector('phr_labels')\n",
    "\n",
    "\n",
    "network_output_spk = lasagne.layers.get_output(l_spk_softmax)\n",
    "network_output_phr = lasagne.layers.get_output(l_phr_softmax)\n",
    "\n",
    "spk_cost = lasagne.objectives.categorical_crossentropy(network_output_spk, spk_labels)\n",
    "phr_cost = lasagne.objectives.categorical_crossentropy(network_output_phr, phr_labels)\n",
    "\n",
    "total_cost = spk_cost + phr_cost\n",
    "mean_cost = total_cost.mean()\n",
    "\n",
    "val_prediction_spk = lasagne.layers.get_output(l_spk_softmax, deterministic=True)\n",
    "val_prediction_phr = lasagne.layers.get_output(l_phr_softmax, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W_in_to_updategate, (60, 100))\n",
      "(W_hid_to_updategate, (100, 100))\n",
      "(b_updategate, (100,))\n",
      "(W_in_to_resetgate, (60, 100))\n",
      "(W_hid_to_resetgate, (100, 100))\n",
      "(b_resetgate, (100,))\n",
      "(W_in_to_hidden_update, (60, 100))\n",
      "(W_hid_to_hidden_update, (100, 100))\n",
      "(b_hidden_update, (100,))\n",
      "(W_in_to_updategate, (60, 100))\n",
      "(W_hid_to_updategate, (100, 100))\n",
      "(b_updategate, (100,))\n",
      "(W_in_to_resetgate, (60, 100))\n",
      "(W_hid_to_resetgate, (100, 100))\n",
      "(b_resetgate, (100,))\n",
      "(W_in_to_hidden_update, (60, 100))\n",
      "(W_hid_to_hidden_update, (100, 100))\n",
      "(b_hidden_update, (100,))\n",
      "(W, (100, 194))\n",
      "(b, (194,))\n",
      "(W, (100, 73))\n",
      "(b, (73,))\n",
      "----------------------------------------\n",
      "Starting training...\n",
      "Epoch 1 of 10 took 30.935s\n",
      "  training loss:\t\t9.550815\n",
      "  training accuracy (speaker):\t\t0.83 %\n",
      "  training accuracy (phrase):\t\t3.59 %\n",
      "  validation loss:\t\t9.157307\n",
      "  validation accuracy (speaker):\t\t0.50 %\n",
      "  validation accuracy:\t\t10.00 %\n",
      "Epoch 2 of 10 took 33.334s\n",
      "  training loss:\t\t7.770949\n",
      "  training accuracy (speaker):\t\t10.60 %\n",
      "  training accuracy (phrase):\t\t26.83 %\n",
      "  validation loss:\t\t8.389347\n",
      "  validation accuracy (speaker):\t\t2.50 %\n",
      "  validation accuracy:\t\t20.00 %\n",
      "Epoch 3 of 10 took 11.506s\n",
      "  training loss:\t\t6.465714\n",
      "  training accuracy (speaker):\t\t19.81 %\n",
      "  training accuracy (phrase):\t\t44.95 %\n",
      "  validation loss:\t\t7.793100\n",
      "  validation accuracy (speaker):\t\t5.50 %\n",
      "  validation accuracy:\t\t28.00 %\n",
      "Epoch 4 of 10 took 11.377s\n",
      "  training loss:\t\t5.370555\n",
      "  training accuracy (speaker):\t\t28.50 %\n",
      "  training accuracy (phrase):\t\t61.60 %\n",
      "  validation loss:\t\t7.393600\n",
      "  validation accuracy (speaker):\t\t6.00 %\n",
      "  validation accuracy:\t\t34.50 %\n",
      "Epoch 5 of 10 took 11.364s\n",
      "  training loss:\t\t4.419698\n",
      "  training accuracy (speaker):\t\t39.97 %\n",
      "  training accuracy (phrase):\t\t73.13 %\n",
      "  validation loss:\t\t7.124570\n",
      "  validation accuracy (speaker):\t\t7.50 %\n",
      "  validation accuracy:\t\t38.00 %\n",
      "Epoch 6 of 10 took 11.388s\n",
      "  training loss:\t\t3.581389\n",
      "  training accuracy (speaker):\t\t52.29 %\n",
      "  training accuracy (phrase):\t\t81.74 %\n",
      "  validation loss:\t\t6.964682\n",
      "  validation accuracy (speaker):\t\t6.00 %\n",
      "  validation accuracy:\t\t41.00 %\n",
      "Epoch 7 of 10 took 11.388s\n",
      "  training loss:\t\t2.840415\n",
      "  training accuracy (speaker):\t\t64.60 %\n",
      "  training accuracy (phrase):\t\t88.92 %\n",
      "  validation loss:\t\t6.868582\n",
      "  validation accuracy (speaker):\t\t6.50 %\n",
      "  validation accuracy:\t\t43.00 %\n",
      "Epoch 8 of 10 took 11.392s\n",
      "  training loss:\t\t2.195284\n",
      "  training accuracy (speaker):\t\t76.33 %\n",
      "  training accuracy (phrase):\t\t94.68 %\n",
      "  validation loss:\t\t6.867981\n",
      "  validation accuracy (speaker):\t\t9.00 %\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 9 of 10 took 11.384s\n",
      "  training loss:\t\t1.647251\n",
      "  training accuracy (speaker):\t\t86.04 %\n",
      "  training accuracy (phrase):\t\t97.54 %\n",
      "  validation loss:\t\t6.929220\n",
      "  validation accuracy (speaker):\t\t8.00 %\n",
      "  validation accuracy:\t\t39.50 %\n",
      "Epoch 10 of 10 took 11.394s\n",
      "  training loss:\t\t1.215323\n",
      "  training accuracy (speaker):\t\t92.43 %\n",
      "  training accuracy (phrase):\t\t99.31 %\n",
      "  validation loss:\t\t6.998225\n",
      "  validation accuracy (speaker):\t\t7.50 %\n",
      "  validation accuracy:\t\t40.50 %\n"
     ]
    }
   ],
   "source": [
    "val_spk_cost = lasagne.objectives.categorical_crossentropy(val_prediction_spk, spk_labels)\n",
    "val_phr_cost = lasagne.objectives.categorical_crossentropy(val_prediction_phr, phr_labels)\n",
    "\n",
    "val_cost = val_spk_cost + val_phr_cost\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc_spk = T.mean(T.eq(T.argmax(val_prediction_spk, axis=1), spk_labels), dtype=theano.config.floatX)\n",
    "val_acc_phr = T.mean(T.eq(T.argmax(val_prediction_phr, axis=1), phr_labels), dtype=theano.config.floatX)\n",
    "\n",
    "#training accuracy\n",
    "train_acc_spk = T.mean(T.eq(T.argmax(network_output_spk, axis=1), spk_labels), dtype=theano.config.floatX)\n",
    "train_acc_phr = T.mean(T.eq(T.argmax(network_output_phr, axis=1), phr_labels), dtype=theano.config.floatX)\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "#total_cost = -(labels*T.log(network_output) + (1-labels)*T.log(1-network_output)) \n",
    "#accuracy function\n",
    "\n",
    "\n",
    "#Get parameters of both encoder and decoder\n",
    "all_parameters = lasagne.layers.get_all_params([l_spk_softmax,l_phr_softmax], trainable=True)\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)\n",
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-3,3) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,3)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.005)\n",
    "\n",
    "train_func = theano.function([X, Mask, spk_labels, phr_labels], [mean_cost, train_acc_spk, train_acc_phr], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, spk_labels, phr_labels], [val_mcost, val_acc_spk, val_acc_phr])\n",
    "\n",
    "\n",
    "\n",
    "train_set = H5PYDataset('/misc/data15/reco/bhattgau/Rnn/lists/rsr/rsr_train3.hdf5',\n",
    "                        which_sets=('train',), subset=slice(0,2000))\n",
    "\n",
    "valid_set = H5PYDataset('/misc/data15/reco/bhattgau/Rnn/lists/rsr/rsr_train3.hdf5',\n",
    "                        which_sets=('test',), subset=slice(0,200))\n",
    "\n",
    "num_epochs=10\n",
    "\n",
    "trainerr=[]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc_spk = 0\n",
    "    tr_acc_phr = 0\n",
    "    \n",
    "    train_batches = 0\n",
    "    \n",
    "    t_state = train_set.open()\n",
    "    v_state = valid_set.open()\n",
    "\n",
    "    scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=256)\n",
    "    scheme1 = ShuffledScheme(examples=valid_set.num_examples, batch_size=100)\n",
    "\n",
    "\n",
    "    train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "    valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in train_stream.get_epoch_iterator():\n",
    "        t_data, t_mask, t_plabs,t_labs = data\n",
    "        \n",
    "        terr, tacc_spk, tacc_phr = train_func(t_data, t_mask, t_labs, t_plabs)\n",
    "        train_err += terr\n",
    "        tr_acc_spk += tacc_spk\n",
    "        tr_acc_phr += tacc_phr\n",
    "\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc_spk = 0\n",
    "    val_acc_phr = 0\n",
    "\n",
    "    val_batches = 0\n",
    "    \n",
    "    for data in valid_stream.get_epoch_iterator():\n",
    "        v_data, v_mask, v_ptars, v_tars = data\n",
    "\n",
    "        err, acc_spk, acc_phr = val_func(v_data, v_mask , v_tars, v_ptars)\n",
    "        val_err += err\n",
    "        val_acc_spk += acc_spk\n",
    "        val_acc_phr += acc_phr\n",
    "\n",
    "        val_batches += 1\n",
    "        \n",
    "    trainerr.append(train_err/train_batches)\n",
    "    \n",
    "    train_set.close(state=t_state)\n",
    "    valid_set.close(state=v_state)\n",
    "        \n",
    "# Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "    epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy (speaker):\\t\\t{:.2f} %\".format(\n",
    "        tr_acc_spk / train_batches * 100))\n",
    "    print(\"  training accuracy (phrase):\\t\\t{:.2f} %\".format(\n",
    "        tr_acc_phr / train_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy (speaker):\\t\\t{:.2f} %\".format(\n",
    "        val_acc_spk / val_batches * 100))\n",
    "    print(\"  validation accuracy (phrase):\\t\\t{:.2f} %\".format(\n",
    "        val_acc_phr / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../speaker_softmax.py\n",
    "\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cPickle\n",
    "import sys\n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "from utils import load_data\n",
    "\n",
    "cFile = sys.argv[1]\n",
    "\n",
    "f=open(cFile)\n",
    "lines = f.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "values = [v.split()[1] for v in lines]\n",
    "\n",
    "\n",
    "N_HIDDEN = int(values[0])\n",
    "F_DIM = int(values[1])\n",
    "LEARNING_RATE = np.cast['float32'](values[2])\n",
    "NUM_EPOCHS = int(values[3])\n",
    "NUM_SPEAKERS = int(values[4])\n",
    "MODEL_NAME = values[5]\n",
    "SAVE_PATH = values[6]\n",
    "#data\n",
    "TRAIN_FILE = values[7]\n",
    "VALID_FILE = values[8]\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "\n",
    "def build_rnn(net_input=None, mask_input=None):\n",
    "\n",
    "    print(\"Building network ...\")\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = X)\n",
    "    n_batch,_,_ = l_in.input_var.shape\n",
    "    #print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "    l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = Mask)\n",
    "    #print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "    #initialize the gates\n",
    "    l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask, only_return_final=True)\n",
    "    l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask, only_return_final=True,\n",
    "                                         backwards=True)\n",
    "\n",
    "    l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "    l_spk_softmax = lasagne.layers.DenseLayer(l_sum, num_units=nSpk, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    l_phr_softmax = lasagne.layers.DenseLayer(l_sum, num_units=nPhr, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_spk_softmax, l_phr_softmax\n",
    "\n",
    "\n",
    "#print lasagne.layers.get_output(l_softmax, inputs={l_in: X, l_mask: Mask}).eval({X: d1, Mask: m1}).shape\n",
    "#l_softmax = lasagne.layers.ReshapeLayer(l_dense, (n_batch, MAX_LENGTH, N_HIDDEN))\n",
    "\n",
    "def main():\n",
    "        \n",
    "    X = T.tensor3(name='input',dtype='float32')\n",
    "    MASK = T.matrix(name = 'mask', dtype='float32')\n",
    "    SPK_LABELS = T.ivector(name='spk_labels')\n",
    "    PHR_LABELS = T.ivector(name='phr_labels')\n",
    "\n",
    "\n",
    "    #load data\n",
    "    train_set = H5PYDataset(TRAIN_FILE,  which_sets=('train',))\n",
    "    valid_set = H5PYDataset(VALID_FILE, which_sets=('test',))\n",
    "    \n",
    "\n",
    "\n",
    "    spk_network, phr_network = build_rnn(net_input=X, mask_input= MASK)\n",
    "    \n",
    "    network_output_spk = lasagne.layers.get_output(spk_network)\n",
    "    network_output_phr = lasagne.layers.get_output(phr_network)\n",
    "\n",
    "    val_prediction_spk = lasagne.layers.get_output(spk_network, deterministic=True)\n",
    "    val_prediction_phr = lasagne.layers.get_output(phr_network, deterministic=True)\n",
    "    #needed for accuracy\n",
    "    \n",
    "    val_acc_spk = T.mean(T.eq(T.argmax(val_prediction_spk, axis=1), SPK_LABELS), dtype=theano.config.floatX)\n",
    "    val_acc_phr = T.mean(T.eq(T.argmax(val_prediction_phr, axis=1), PHR_LABELS), dtype=theano.config.floatX)\n",
    "\n",
    "    #training accuracy\n",
    "    train_acc_spk = T.mean(T.eq(T.argmax(network_output_spk, axis=1), SPK_LABELS), dtype=theano.config.floatX)\n",
    "    train_acc_phr = T.mean(T.eq(T.argmax(network_output_phr, axis=1), PHR_LABELS), dtype=theano.config.floatX)\n",
    "    \n",
    "    #cost function    \n",
    "    spk_cost = lasagne.objectives.categorical_crossentropy(network_output_spk, SPK_LABELS)\n",
    "    phr_cost = lasagne.objectives.categorical_crossentropy(network_output_phr, PHR_LABELS)\n",
    "    total_cost = spk_cost + phr_cost\n",
    "    mean_cost = total_cost.mean()\n",
    "    \n",
    "    #Validation cost\n",
    "    val_spk_cost = lasagne.objectives.categorical_crossentropy(val_prediction_spk, SPK_LABELS)\n",
    "    val_phr_cost = lasagne.objectives.categorical_crossentropy(val_prediction_phr, PHR_LABELS)\n",
    "    val_cost = val_spk_cost + val_phr_cost\n",
    "    val_mcost = val_cost.mean()\n",
    "\n",
    "    #Get parameters of both encoder and decoder\n",
    "    all_parameters = lasagne.layers.get_all_params([network], trainable=True)\n",
    "\n",
    "    print(\"Trainable Model Parameters\")\n",
    "    print(\"-\"*40)\n",
    "    for param in all_parameters:\n",
    "        print(param, param.get_value().shape)\n",
    "    print(\"-\"*40)\n",
    "    #add grad clipping to avoid exploding gradients\n",
    "    all_grads = [T.clip(g,-3,3) for g in T.grad(mean_cost, all_parameters)]\n",
    "    all_grads = lasagne.updates.total_norm_constraint(all_grads,3)\n",
    "\n",
    "    updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=LEARNING_RATE)\n",
    "\n",
    "    train_func = theano.function([X, MASK, LABELS], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "    val_func = theano.function([X, MASK, LABELS], [val_mcost, val_acc])\n",
    "\n",
    "    trainerr=[]\n",
    "    epoch=0 #set the epoch counter\n",
    "    \n",
    "    val_prev = np.inf\n",
    "    a_prev = -np.inf\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "        # We iterate over epochs:\n",
    "    while 'true':\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        tr_acc = 0\n",
    "        train_batches = 0\n",
    "\n",
    "        h1=train_set.open()\n",
    "        h2=valid_set.open()\n",
    "\n",
    "        scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=64)\n",
    "        scheme1 = SequentialScheme(examples=valid_set.num_examples, batch_size=128)\n",
    "\n",
    "\n",
    "        train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "        valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for data in train_stream.get_epoch_iterator():\n",
    "            t_data, t_mask, _, t_labs = data\n",
    "            terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "            train_err += terr\n",
    "            tr_acc += tacc\n",
    "            train_batches += 1\n",
    "\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "\n",
    "        for data in valid_stream.get_epoch_iterator():\n",
    "            v_data, v_mask, _, v_tars = data\n",
    "            err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        trainerr.append(train_err/train_batches)\n",
    "\n",
    "        epoch+=1\n",
    "        train_set.close(h1)\n",
    "        valid_set.close(h2)\n",
    "        \n",
    "        #Display\n",
    "        if display:\n",
    "            \n",
    "            print(\"Epoch {} of {} took {:.3f}s \".format(\n",
    "            epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "            print(\"  training accuracy:\\t\\t{:.2f} % \".format(\n",
    "                    tr_acc / train_batches * 100))\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "            print(\"  validation accuracy:\\t\\t{:.2f} % \".format(\n",
    "                    val_acc / val_batches * 100))\n",
    "\n",
    "        logfile = os.path.join(SAVE_PATH,MODEL_NAME,'logs',MODEL_NAME+'.log')\n",
    "        flog1 = open(logfile,'ab')\n",
    "\n",
    "        flog1.write(\"Epoch {} of {} took {:.3f}s\\n \".format(\n",
    "        epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "        flog1.write(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "        flog1.write(\"  training accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "            tr_acc / train_batches * 100))\n",
    "        flog1.write(\"  validation loss:\\t\\t{:.6f}\\n\".format(val_err / val_batches))\n",
    "        flog1.write(\"  validation accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "            val_acc / val_batches * 100))\n",
    "        flog1.write(\"\\n\")\n",
    "        flog1.close()\n",
    "\n",
    "        if epoch == NUM_EPOCHS:\n",
    "            break\n",
    "\n",
    "        valE = val_err/val_batches\n",
    "        valA = val_acc / val_batches\n",
    "\n",
    "        #save the max accuracy model\n",
    "        max_val = a_prev\n",
    "\n",
    "        #save model with highest accuracy\n",
    "        if valA > max_val:\n",
    "\n",
    "            model_params1 = lasagne.layers.get_all_param_values(network)\n",
    "\n",
    "            model1_name = MODEL_NAME+'_acc' + '.pkl'\n",
    "\n",
    "            vpth1 = os.path.join(SAVE_PATH, MODEL_NAME, 'weights',model1_name)\n",
    "\n",
    "            fsave = open(vpth1,'wb')  \n",
    "\n",
    "            cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            fsave.close()\n",
    "\n",
    "            max_val = valA\n",
    "\n",
    "        #Patience\n",
    "\n",
    "        if valE > val_prev:\n",
    "            c+=1\n",
    "\n",
    "            #save the model incase\n",
    "            model_params1 = lasagne.layers.get_all_param_values(network)\n",
    "\n",
    "            model1_name = MODEL_NAME + '_ofit'  + '.pkl'\n",
    "\n",
    "            vpth1 = os.path.join(SAVE_PATH, MODEL_NAME, 'weights',model1_name)\n",
    "\n",
    "            fsave = open(vpth1,'wb')  \n",
    "\n",
    "            cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            fsave.close()\n",
    "\n",
    "            val_prev=valE\n",
    "\n",
    "        else:\n",
    "            c=0\n",
    "            val_prev=valE\n",
    "\n",
    "        if c==5:\n",
    "            break\n",
    "\n",
    "    #Save the final model\n",
    "\n",
    "    print('Saving Model ...')\n",
    "    model_params = lasagne.layers.get_all_param_values(network)\n",
    "    model1_name = MODEL_NAME+'_final' + '.pkl'\n",
    "    vpth = os.path.join(SAVE_PATH, MODEL_NAME,'weights',model1_name)\n",
    "    fsave = open(vpth,'wb')  \n",
    "    cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    fsave.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spk_softmax_fuel.py\n"
     ]
    }
   ],
   "source": [
    "#Save the final model\n",
    "spth = '/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax'\n",
    "\n",
    "tfile = open('/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax/trainerr.npy','wb')\n",
    "trainerr = np.asarray(trainerr, dtype='float32')\n",
    "np.save(tfile,trainerr)\n",
    "\n",
    "print('Saving Model ...')\n",
    "model_params = lasagne.layers.get_all_param_values(l_softmax)\n",
    "model_name = 'basic_softmax_500' + '.pkl'\n",
    "vpth = os.path.join(spth, model_name)\n",
    "fsave = open(vpth,'wb')  \n",
    "cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "fsave.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /misc/data15/reco/bhattgau/Rnn/code/Code_/frame_softmax.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /misc/data15/reco/bhattgau/Rnn/code/Code_/frame_softmax.py\n",
    "\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cPickle\n",
    "import sys\n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "from utils import load_data\n",
    "\n",
    "cFile = sys.argv[1]\n",
    "\n",
    "f=open(cFile)\n",
    "lines = f.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "values = [v.split()[1] for v in lines]\n",
    "\n",
    "\n",
    "N_HIDDEN = int(values[0])\n",
    "F_DIM = int(values[1])\n",
    "LEARNING_RATE = np.cast['float32'](values[2])\n",
    "NUM_EPOCHS = int(values[3])\n",
    "NUM_SPEAKERS = int(values[4])\n",
    "MODEL_NAME = values[5]\n",
    "SAVE_PATH = values[6]\n",
    "#data\n",
    "TRAIN_FILE = values[7]\n",
    "VALID_FILE = values[8]\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "\n",
    "def build_rnn(net_input=None, mask_input=None):\n",
    "\n",
    "    print(\"Building network ...\")\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = net_input)\n",
    "    n_batch,_,_ = l_in.input_var.shape\n",
    "    #print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "    l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = mask_input)\n",
    "    #print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "    #initialize the gates\n",
    "    l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask)\n",
    "    l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, precompute_input=True, mask_input=l_mask, backwards=True)\n",
    "    l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "    l_reshape1 = lasagne.layers.ReshapeLayer(l_sum, (-1, N_HIDDEN))\n",
    "\n",
    "    l_softmax = lasagne.layers.DenseLayer(l_reshape1, num_units=NUM_SPEAKERS, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_softmax\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "        \n",
    "    X = T.tensor3(name='input',dtype='float32')\n",
    "    MASK = T.matrix(name = 'mask', dtype='float32')\n",
    "    LABELS = T.ivector(name='labels')\n",
    "    \n",
    "    #load data\n",
    "    train_set = H5PYDataset(TRAIN_FILE,  which_sets=('train',))\n",
    "    valid_set = H5PYDataset(VALID_FILE, which_sets=('test',))\n",
    "    \n",
    "    network = build_rnn(net_input=X, mask_input= MASK)\n",
    "    \n",
    "    network_output = lasagne.layers.get_output(network)\n",
    "\n",
    "    val_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    #needed for accuracy\n",
    "    val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), LABELS), dtype=theano.config.floatX)\n",
    "    #training accuracy\n",
    "    train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), LABELS), dtype=theano.config.floatX)\n",
    "\n",
    "    #T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "    #print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "    #cost function\n",
    "    total_cost = lasagne.objectives.categorical_crossentropy(network_output, LABELS)\n",
    "    #total_cost = -(labels*T.log(network_output) + (1-labels)*T.log(1-network_output)) \n",
    "    masked_cost = total_cost*MASK.flatten()\n",
    "    mean_cost = total_cost.mean()\n",
    "    #accuracy function\n",
    "    val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, LABELS)\n",
    "    val_cost = val_cost*MASK.flatten()\n",
    "    val_mcost = val_cost.mean()\n",
    "\n",
    "    #Get parameters of both encoder and decoder\n",
    "    all_parameters = lasagne.layers.get_all_params([network], trainable=True)\n",
    "\n",
    "    print(\"Trainable Model Parameters\")\n",
    "    print(\"-\"*40)\n",
    "    for param in all_parameters:\n",
    "        print(param, param.get_value().shape)\n",
    "    print(\"-\"*40)\n",
    "    #add grad clipping to avoid exploding gradients\n",
    "    all_grads = [T.clip(g,-3,3) for g in T.grad(mean_cost, all_parameters)]\n",
    "    all_grads = lasagne.updates.total_norm_constraint(all_grads,3)\n",
    "\n",
    "    updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=LEARNING_RATE)\n",
    "\n",
    "    train_func = theano.function([X, MASK, LABELS], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "    val_func = theano.function([X, MASK, LABELS], [val_mcost, val_acc])\n",
    "\n",
    "    trainerr=[]\n",
    "    epoch=0 #set the epoch counter\n",
    "    \n",
    "    val_prev = np.inf\n",
    "    a_prev = -np.inf\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "        # We iterate over epochs:\n",
    "    while 'true':\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        tr_acc = 0\n",
    "        train_batches = 0\n",
    "\n",
    "        h1=train_set.open()\n",
    "        h2=valid_set.open()\n",
    "\n",
    "        scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=64)\n",
    "        scheme1 = SequentialScheme(examples=valid_set.num_examples, batch_size=64)\n",
    "\n",
    "\n",
    "        train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "        valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for data in train_stream.get_epoch_iterator():\n",
    "            t_data, t_mask,t_labs = data\n",
    "            t_labs = t_labs.flatten()\n",
    "            terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "            train_err += terr\n",
    "            tr_acc += tacc\n",
    "            train_batches += 1\n",
    "\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "\n",
    "        for data in valid_stream.get_epoch_iterator():\n",
    "            v_data, v_mask, v_tars = data\n",
    "            v_tars = v_tars.flatten()\n",
    "            \n",
    "            err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        trainerr.append(train_err/train_batches)\n",
    "\n",
    "        epoch+=1\n",
    "        train_set.close(h1)\n",
    "        valid_set.close(h2)\n",
    "        \n",
    "        #Display\n",
    "        if display:\n",
    "            \n",
    "            print(\"Epoch {} of {} took {:.3f}s \".format(\n",
    "            epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "            print(\"  training accuracy:\\t\\t{:.2f} % \".format(\n",
    "                    tr_acc / train_batches * 100))\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "            print(\"  validation accuracy:\\t\\t{:.2f} % \".format(\n",
    "                    val_acc / val_batches * 100))\n",
    "\n",
    "        logfile = os.path.join(SAVE_PATH,MODEL_NAME,'logs',MODEL_NAME+'.log')\n",
    "        flog1 = open(logfile,'ab')\n",
    "\n",
    "        flog1.write(\"Epoch {} of {} took {:.3f}s\\n \".format(\n",
    "        epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "        flog1.write(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "        flog1.write(\"  training accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "            tr_acc / train_batches * 100))\n",
    "        flog1.write(\"  validation loss:\\t\\t{:.6f}\\n\".format(val_err / val_batches))\n",
    "        flog1.write(\"  validation accuracy:\\t\\t{:.2f} %\\n \".format(\n",
    "            val_acc / val_batches * 100))\n",
    "        flog1.write(\"\\n\")\n",
    "        flog1.close()\n",
    "\n",
    "        if epoch == NUM_EPOCHS:\n",
    "            break\n",
    "\n",
    "        valE = val_err/val_batches\n",
    "        valA = val_acc / val_batches\n",
    "\n",
    "        #save the max accuracy model\n",
    "        max_val = a_prev\n",
    "\n",
    "        #save model with highest accuracy\n",
    "        if valA > max_val:\n",
    "\n",
    "            model_params1 = lasagne.layers.get_all_param_values(network)\n",
    "\n",
    "            model1_name = MODEL_NAME+'_acc' + '.pkl'\n",
    "\n",
    "            vpth1 = os.path.join(SAVE_PATH, MODEL_NAME, 'weights',model1_name)\n",
    "\n",
    "            fsave = open(vpth1,'wb')  \n",
    "\n",
    "            cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            fsave.close()\n",
    "\n",
    "            max_val = valA\n",
    "\n",
    "        #Patience\n",
    "\n",
    "        if valE > val_prev:\n",
    "            c+=1\n",
    "\n",
    "            #save the model incase\n",
    "            model_params1 = lasagne.layers.get_all_param_values(network)\n",
    "\n",
    "            model1_name = MODEL_NAME + '_ofit'  + '.pkl'\n",
    "\n",
    "            vpth1 = os.path.join(SAVE_PATH, MODEL_NAME, 'weights',model1_name)\n",
    "\n",
    "            fsave = open(vpth1,'wb')  \n",
    "\n",
    "            cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            fsave.close()\n",
    "\n",
    "            val_prev=valE\n",
    "\n",
    "        else:\n",
    "            c=0\n",
    "            val_prev=valE\n",
    "\n",
    "        if c==5:\n",
    "            break\n",
    "\n",
    "    #Save the final model\n",
    "\n",
    "    print('Saving Model ...')\n",
    "    model_params = lasagne.layers.get_all_param_values(network)\n",
    "    model1_name = MODEL_NAME+'_final' + '.pkl'\n",
    "    vpth = os.path.join(SAVE_PATH, MODEL_NAME,'weights',model1_name)\n",
    "    fsave = open(vpth,'wb')  \n",
    "    cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    fsave.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
