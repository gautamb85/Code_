{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load attention3.py\n",
    "\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    f1 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Train_feats_labs.plst')\n",
    "    lines = f1.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    labelz = [int(l.split()[1]) for l in lines] \n",
    "    #labelz = labelz[:20]\n",
    "    features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    f2 = open('/misc/data15/reco/bhattgau/Rnn/Lists/spk_softmax/Valid_feats_labs.plst')\n",
    "    lines = f2.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    val_labelz = [int(l.split()[1]) for l in lines] \n",
    "    #val_labelz = val_labelz[:20]\n",
    "    val_features = [l.split()[0] for l in lines] \n",
    "    \n",
    "    n_samp = len(features)\n",
    "    maxlen=800 #pad all utterances to this length\n",
    "    feat_dim=20\n",
    "    nSpk = 98\n",
    "    dpth = '/misc/data15/reco/bhattgau/Rnn/Data/mfcc/Nobackup/VQ_VAD_HO_EPD/'\n",
    "\n",
    "    Data = np.zeros((n_samp, maxlen, feat_dim), dtype='float32')\n",
    "    Mask = np.zeros((n_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "    \n",
    "    vn_samp = len(val_features)\n",
    "    val_Data = np.zeros((vn_samp, maxlen, feat_dim), dtype='float32')\n",
    "    val_Mask = np.zeros((vn_samp,maxlen), dtype='float32')\n",
    "    #Targets = np.zeros((n_samp, nSpk), dtype='int32')\n",
    "\n",
    "    for ind,f in enumerate(features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        Data[ind,:,:] = data\n",
    "        \n",
    "\n",
    "    for ind,f in enumerate(val_features):\n",
    "        fname = os.path.join(dpth,f+'.fea')\n",
    "        fi = htkmfc.HTKFeat_read(fname)\n",
    "        data = fi.getall()[:,:20]\n",
    "        val_Mask[ind,:data.shape[0]] = 1.0\n",
    "        pad = maxlen - data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((pad,20), dtype='float32')))\n",
    "        val_Data[ind,:,:] = data\n",
    "\n",
    "\n",
    "    return Data, Mask, np.asarray(labelz, dtype='int32'), val_Data, val_Mask, np.asarray(val_labelz, dtype='int32')\n",
    "\n",
    "def iterate_minibatches(inputs, mask, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    iplen = len(inputs)\n",
    "    pointer = 0\n",
    "    indices=np.arange(iplen)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    while pointer < iplen:\n",
    "    \n",
    "        if pointer <= iplen - batchsize:\n",
    "            excerpt = indices[pointer:pointer+batchsize]\n",
    "        else:\n",
    "            batchsize = iplen-pointer\n",
    "            excerpt = indices[pointer:pointer+batchsize]\n",
    "        \n",
    "        pointer+=batchsize\n",
    "\n",
    "        yield inputs[excerpt], mask[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 600\n",
    "F_DIM = 20\n",
    "# Number of training sequences ain each batch\n",
    "N_BATCH = 1\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = .001\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "nSpk = 98\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "Mask = T.matrix(name = 'mask', dtype='float32')\n",
    "\n",
    "target = T.matrix(name='target_values', dtype='float32')\n",
    "\n",
    "print(\"Building network ...\")\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None, F_DIM), input_var = X)\n",
    "n_batch,maxlen,_ = l_in.input_var.shape\n",
    "\n",
    "#get the batch size for the weights matrix\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None), input_var = Mask)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "#initialize the gates\n",
    "\n",
    "#Compute Recurrent Embeddings\n",
    "l_forward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask)\n",
    "l_backward = lasagne.layers.GRULayer(l_in, N_HIDDEN, mask_input=l_mask, backwards=True)\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "Recc_emb =  lasagne.layers.get_output(l_sum, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask})\n",
    "#print lasagne.layers.get_output(l_res, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "l_reshape1 = lasagne.layers.ReshapeLayer(l_sum, (-1, N_HIDDEN))\n",
    "\n",
    "l_attend = lasagne.layers.DenseLayer(l_reshape1, num_units=1, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "#print lasagne.layers.get_output(l_attend, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "l_attention = lasagne.layers.ReshapeLayer(l_attend, (n_batch, maxlen))\n",
    "attn_wts = lasagne.layers.get_output(l_attention, inputs={l_in: X, l_mask: Mask})#.eval({X: x_dummy, Mask: mask})\n",
    "attn_probs = T.nnet.softmax(attn_wts)#.eval({X:x_dummy, Mask:mask})\n",
    "#Calculate the last time-step of a recording using the mask.\n",
    "lstep = T.sum(Mask, axis=1)#.eval({Mask : mask})\n",
    "lstep = T.cast(lstep, 'int32')#.eval()\n",
    "lstep = lstep.T\n",
    "\n",
    "#print lasagne.layers.get_output(l_reshape1b, inputs={l_in: X, l_mask: Mask}).eval({X: x_dummy, Mask: mask}).shape\n",
    "Wts = T.zeros([n_batch, maxlen], dtype='float32')\n",
    "\n",
    "def weighted_avg(w_t, l_t, W, R_emb):\n",
    "    W = T.set_subtensor(W[:l_t], w_t[:l_t])\n",
    "    W = W[None,:]\n",
    "    w_sum = T.dot(W, R_emb)\n",
    "    \n",
    "    #this W (1,600) is is then multiplied (dot product) with the recurrent embedding (600, 100)\n",
    "    #to produce a feature vector that is a weighted sum of all the embedding vectors of the recording\n",
    "    #also return the weights for analysis\n",
    "    \n",
    "    return w_sum\n",
    "    \n",
    "U_t,_ = theano.scan(fn=weighted_avg, sequences=[attn_probs, lstep, Wts, Recc_emb])\n",
    "l_in2 = lasagne.layers.InputLayer(shape=(None,None, None), input_var = U_t)\n",
    "n_batch1,l1,hdim = l_in2.input_var.shape\n",
    "\n",
    "l_reshape2 = lasagne.layers.ReshapeLayer(l_in2, (n_batch1*l1,N_HIDDEN))\n",
    "#print lasagne.layers.get_output(l_reshape2, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "\n",
    "#Finally this feature vector(s) gets passed to a dense layer which represents a softmax distribution over speakers\n",
    "l_Spk_softmax = lasagne.layers.DenseLayer(l_reshape2, num_units=98, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "#print lasagne.layers.get_output(l_Spk_softmax, inputs={l_in: X, l_mask: Mask, l_in2: U_t}).eval({X: x_dummy, Mask: mask}).shape\n",
    "# lasagne.layers.get_output produces a variable for the output of the net\n",
    "\n",
    "network_output = lasagne.layers.get_output(l_Spk_softmax)\n",
    "\n",
    "labels = T.ivector(name='labels')\n",
    "\n",
    "val_prediction = lasagne.layers.get_output(l_Spk_softmax, deterministic=True)\n",
    "#needed for accuracy\n",
    "#don't use the one hot vectors here\n",
    "#The one hot vectors are needed for the categorical cross-entropy\n",
    "val_acc = T.mean(T.eq(T.argmax(val_prediction, axis=1), labels), dtype=theano.config.floatX)\n",
    "#training accuracy\n",
    "train_acc = T.mean(T.eq(T.argmax(network_output, axis=1), labels), dtype=theano.config.floatX)\n",
    "\n",
    "#T.argmax(network_output, axis=1).eval({X: d1, Mask: m1})\n",
    "\n",
    "#print network_output.eval({X: d1, Mask: m1})[1][97]\n",
    "#cost function\n",
    "total_cost = lasagne.objectives.categorical_crossentropy(network_output, labels)\n",
    "#total_cost = -(labels*T.log(network_MMMmoutput) + (1-labels)*T.log(1-network_output)) \n",
    "mean_cost = total_cost.mean()\n",
    "#accuracy function\n",
    "val_cost = lasagne.objectives.categorical_crossentropy(val_prediction, labels)\n",
    "val_mcost = val_cost.mean()\n",
    "\n",
    "#Get parameters of both encoder and decoder\n",
    "params1 = lasagne.layers.get_all_params([l_attend], trainable=True)\n",
    "params2 = lasagne.layers.get_all_params([l_Spk_softmax], trainable=True)\n",
    "all_parameters = params1 + params2\n",
    "\n",
    "print(\"Trainable Model Parameters\")\n",
    "print(\"-\"*40)\n",
    "for param in all_parameters:\n",
    "    print(param, param.get_value().shape)\n",
    "print(\"-\"*40)\n",
    "#add grad clipping to avoid exploding gradients\n",
    "all_grads = [T.clip(g,-5,5) for g in T.grad(mean_cost, all_parameters)]\n",
    "all_grads = lasagne.updates.total_norm_constraint(all_grads,5)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=0.005)\n",
    "\n",
    "train_func = theano.function([X, Mask, labels], [mean_cost, train_acc], updates=updates)\n",
    "\n",
    "val_func = theano.function([X, Mask, labels], [val_mcost, val_acc])\n",
    "\n",
    "\n",
    "#load the dataset\n",
    "Data, Msk, Targets, val_Data, val_Msk, val_tars = load_dataset()\n",
    "num_epochs=300\n",
    "epoch=0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "val_prev = np.inf\n",
    "\n",
    "#for epoch in range(num_epochs):\n",
    "while('true'):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    tr_acc = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(Data, Msk, Targets, 256):\n",
    "        t_data, t_mask, t_labs = batch\n",
    "        terr, tacc = train_func(t_data, t_mask, t_labs)\n",
    "        train_err += terr\n",
    "        tr_acc += tacc\n",
    "        train_batches += 1\n",
    "        \n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(val_Data, val_Msk, val_tars, 64, shuffle=False):\n",
    "        v_data, v_mask, v_tars = batch\n",
    "        err, acc = val_func(v_data, v_mask ,v_tars)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    \n",
    "    #f_log = open('/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/training.log','a')\n",
    "    epoch+=1\n",
    "\n",
    "# Then we print the results for this epoch:\n",
    "    #flog = open('training.log','a')\n",
    "    print(\"Epoch {} of {} took {:.3f}s \".format(\n",
    "    epoch, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f} \".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} % \".format(\n",
    "        tr_acc / train_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f} \".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} % \".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    #flog.write('\\n')\n",
    "    #flog.close()\n",
    "   \n",
    "    valE = val_err/val_batches\n",
    "    if valE > val_prev:\n",
    "        c+=1\n",
    "        val_prev=valE\n",
    "    else:\n",
    "        c=0\n",
    "        val_prev=valE\n",
    "    \n",
    "    if c==8:\n",
    "        break\n",
    "    \n",
    "    if epoch==num_epochs:\n",
    "        break\n",
    "        \n",
    "#Save the final model\n",
    "\n",
    "spth = '/misc/data15/reco/bhattgau/Rnn/Projects/Rvector/Weights/basic-softmax'\n",
    "\n",
    "print('Saving Model ...')\n",
    "model_params1 = lasagne.layers.get_all_param_values(l_attention)\n",
    "model_params2 = lasagne.layers.get_all_param_values(l_Spk_softmax)\n",
    "model_params = model_params1 + model_params2\n",
    "\n",
    "model1_name = 'Attn2_softmax_600' + '.pkl'\n",
    "model2_name = 'Attn2_softmax_600b' + '.pkl'\n",
    "\n",
    "vpth1 = os.path.join(spth, model1_name)\n",
    "vpth2 = os.path.join(spth, model2_name)\n",
    "\n",
    "fsave = open(vpth1,'wb')  \n",
    "fsave2 = open(vpth2,'wb')  \n",
    "\n",
    "cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "cPickle.dump(model_params2, fsave2, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "fsave.close()\n",
    "fsave2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask = np.zeros((20,600), dtype='float32')\n",
    "mask[0,:433] = 1.0\n",
    "mask[1,:226] = 1.0\n",
    "\n",
    "#mask = mask[:, None]\n",
    "x_dummy = np.random.random((20,600,20))\n",
    "x_dummy = np.cast['float32'](x_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network ...\n"
     ]
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
