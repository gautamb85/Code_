{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../rsr_multitask.py\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cPickle\n",
    "import sys\n",
    "import subprocess \n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.converters.base import fill_hdf5_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cFile = sys.argv[1]\n",
    "\n",
    "f=open(cFile)\n",
    "lines = f.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "values = [v.split()[1] for v in lines]\n",
    "\n",
    "\n",
    "N_HIDDEN = int(values[0])\n",
    "F_DIM = int(values[1])\n",
    "LEARNING_RATE = np.cast['float32'](values[2])\n",
    "BATCH_SIZE = int(values[3])\n",
    "NUM_EPOCHS = int(values[4])\n",
    "NUM_SPEAKERS = int(values[5])\n",
    "NUM_PHRASES = int(values[6])\n",
    "MODEL_NAME = values[7]\n",
    "SAVE_PATH = values[8]\n",
    "#data\n",
    "TRAIN_FILE = values[9]\n",
    "VALID_FILE = values[10]\n",
    "SCRIPT = values[11]\n",
    "MSC = values[12]\n",
    "\n",
    "wtspth =os.path.join(SAVE_PATH,MODEL_NAME,'weights')\n",
    "logpth =os.path.join(SAVE_PATH,MODEL_NAME,'logs')\n",
    "\n",
    "#create a folder for logs and weights\n",
    "if os.path.exists(wtspth):\n",
    "    print 'weight folder exits\\n'\n",
    "else:\n",
    "    Command1 = \"mkdir -p\" +\" \"+ wtspth\n",
    "    process = subprocess.check_call(Command1.split())\n",
    "\n",
    "if os.path.exists(logpth):\n",
    "    print 'logs folder exists'\n",
    "else:\n",
    "    Command2 = \"mkdir -p\" +\" \"+ logpth\n",
    "    process2 = subprocess.check_call(Command2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_dummy = np.random.random((2,800,40))\n",
    "x_dummy = np.cast['float32'](x_dummy)\n",
    "mask = np.zeros((2,800), dtype='float32')\n",
    "mask[0,:344] = 1.0\n",
    "mask[1,:444] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 500)\n"
     ]
    }
   ],
   "source": [
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "F_DIM=40\n",
    "N_HIDDEN=500\n",
    "NUM_SPEAKERS=194\n",
    "NUM_PHRASES=30\n",
    "#def build_rnn(net_input=None, mask_input=None):\n",
    "\n",
    "    #print(\"Building network ...\")\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "MASK = T.matrix(name = 'mask', dtype='float32')\n",
    "    \n",
    "l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = X)\n",
    "n_batch,_,_ = l_in.input_var.shape\n",
    "#print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = MASK)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "#initialize the gates\n",
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "b=lasagne.init.Constant(0.))\n",
    "\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "W_cell=None, b=lasagne.init.Constant(0.),nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "\n",
    "l_forward = lasagne.layers.LSTMLayer(l_in, N_HIDDEN, mask_input=l_mask,\n",
    "                                     precompute_input=True, peepholes=True, learn_init=True)\n",
    "\n",
    "\n",
    "l_backward = lasagne.layers.LSTMLayer(l_in, N_HIDDEN, mask_input=l_mask,\n",
    "                                     precompute_input=True, peepholes=True, learn_init=True, backwards=True)\n",
    "\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "l_last = lasagne.layers.SliceLayer(l_sum,-1,1)\n",
    "print l_last.output_shape\n",
    "\n",
    "#to be ignored\n",
    "l_spk_softmax = lasagne.layers.DenseLayer(l_last, num_units=NUM_SPEAKERS, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "l_phr_softmax = lasagne.layers.DenseLayer(l_last, num_units=NUM_PHRASES, nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set network weights\n",
    "Wfile = open('/misc/data15/reco/bhattgau/Rnn/projects/Rvector/rsr_multitask_fbank_500/weights/rsr_multitask_fbank_500_minloss.pkl')\n",
    "Wts = cPickle.load(Wfile)\n",
    "lasagne.layers.set_all_param_values([l_spk_softmax, l_phr_softmax], Wts)\n",
    "\n",
    "\n",
    "Rvectors = lasagne.layers.get_output(l_sum)\n",
    "\n",
    "l_last = Rvectors[:,-1,:]\n",
    "\n",
    "feat_extractor = theano.function([X, MASK], [Rvectors, l_last])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 500)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wts[33].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 800, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M[0,-1,:] - m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    l_spk_softmax = lasagne.layers.DenseLayer(l_sum, num_units=NUM_SPEAKERS, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    l_phr_softmax = lasagne.layers.DenseLayer(l_sum, num_units=NUM_PHRASES, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_spk_softmax, l_phr_softmax\n",
    "\n",
    "\n",
    "#print lasagne.layers.get_output(l_softmax, inputs={l_in: X, l_mask: Mask}).eval({X: d1, Mask: m1}).shape\n",
    "#l_softmax = lasagne.layers.ReshapeLayer(l_dense, (n_batch, MAX_LENGTH, N_HIDDEN))\n",
    "\n",
    "def main():\n",
    "        \n",
    "    X = T.tensor3(name='input',dtype='float32')\n",
    "    MASK = T.matrix(name = 'mask', dtype='float32')\n",
    "    SPK_LABELS = T.ivector(name='spk_labels')\n",
    "    PHR_LABELS = T.ivector(name='phr_labels')\n",
    "\n",
    "    #load data\n",
    "    train_set = H5PYDataset(TRAIN_FILE,  which_sets=('train',))\n",
    "    valid_set = H5PYDataset(TRAIN_FILE, which_sets=('test',))\n",
    "\n",
    "    spk_network, phr_network = build_rnn(net_input=X, mask_input= MASK)\n",
    "    \n",
    "    network_output_spk = lasagne.layers.get_output(spk_network)\n",
    "    network_output_phr = lasagne.layers.get_output(phr_network)\n",
    "\n",
    "    val_prediction_spk = lasagne.layers.get_output(spk_network, deterministic=True)\n",
    "    val_prediction_phr = lasagne.layers.get_output(phr_network, deterministic=True)\n",
    "    #needed for accuracy\n",
    "    \n",
    "    val_acc_spk = T.mean(T.eq(T.argmax(val_prediction_spk, axis=1), SPK_LABELS), dtype=theano.config.floatX)\n",
    "    val_acc_phr = T.mean(T.eq(T.argmax(val_prediction_phr, axis=1), PHR_LABELS), dtype=theano.config.floatX)\n",
    "\n",
    "    #training accuracy\n",
    "    train_acc_spk = T.mean(T.eq(T.argmax(network_output_spk, axis=1), SPK_LABELS), dtype=theano.config.floatX)\n",
    "    train_acc_phr = T.mean(T.eq(T.argmax(network_output_phr, axis=1), PHR_LABELS), dtype=theano.config.floatX)\n",
    "    \n",
    "    #cost function    \n",
    "    spk_cost = lasagne.objectives.categorical_crossentropy(network_output_spk, SPK_LABELS)\n",
    "    phr_cost = lasagne.objectives.categorical_crossentropy(network_output_phr, PHR_LABELS)\n",
    "    total_cost = spk_cost + phr_cost\n",
    "    mean_cost = total_cost.mean()\n",
    "    \n",
    "    #Validation cost\n",
    "    val_spk_cost = lasagne.objectives.categorical_crossentropy(val_prediction_spk, SPK_LABELS)\n",
    "    val_phr_cost = lasagne.objectives.categorical_crossentropy(val_prediction_phr, PHR_LABELS)\n",
    "    val_cost = val_spk_cost + val_phr_cost\n",
    "    val_mcost = val_cost.mean()\n",
    "\n",
    "    #Get parameters of both encoder and decoder\n",
    "    all_parameters = lasagne.layers.get_all_params([spk_network, phr_network], trainable=True)\n",
    "\n",
    "    print(\"Trainable Model Parameters\")\n",
    "    print(\"-\"*40)\n",
    "    for param in all_parameters:\n",
    "        print(param, param.get_value().shape)\n",
    "    print(\"-\"*40)\n",
    "    #add grad clipping to avoid exploding gradients\n",
    "    all_grads = [T.clip(g,-3,3) for g in T.grad(mean_cost, all_parameters)]\n",
    "    #all_grads = lasagne.updates.total_norm_constraint(all_grads,3)\n",
    "\n",
    "    updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=LEARNING_RATE)\n",
    "\n",
    "    train_func = theano.function([X, MASK, SPK_LABELS, PHR_LABELS], [mean_cost, train_acc_spk, train_acc_phr], updates=updates)\n",
    "\n",
    "    val_func = theano.function([X, MASK, SPK_LABELS, PHR_LABELS], [val_mcost, val_acc_spk, val_acc_phr])\n",
    "\n",
    "    trainerr=[]\n",
    "    epoch=0 #set the epoch counter\n",
    "    \n",
    "    min_val_loss = np.inf\n",
    "    max_val = np.inf*(-1)\n",
    "    \n",
    "    patience=0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "        # We iterate over epochs:\n",
    "    while 'true':\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        tr_acc_spk = 0\n",
    "        tr_acc_phr = 0\n",
    "\n",
    "        train_batches = 0\n",
    "\n",
    "        h1=train_set.open()\n",
    "        h2=valid_set.open()\n",
    "\n",
    "        scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=BATCH_SIZE)\n",
    "        scheme1 = SequentialScheme(examples=valid_set.num_examples, batch_size=32)\n",
    "\n",
    "\n",
    "        train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "        valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for data in train_stream.get_epoch_iterator():\n",
    "            t_data, t_mask, t_plabs,_, t_labs,_ = data\n",
    "            terr, tacc_spk, tacc_phr = train_func(t_data, t_mask, t_labs, t_plabs)\n",
    "            train_err += terr\n",
    "            \n",
    "            tr_acc_spk += tacc_spk\n",
    "            tr_acc_phr += tacc_phr\n",
    "            \n",
    "            train_batches += 1\n",
    "\n",
    "        val_err = 0\n",
    "        val_acc_spk = 0\n",
    "        val_acc_phr = 0\n",
    "        \n",
    "        val_batches = 0\n",
    "\n",
    "        for data in valid_stream.get_epoch_iterator():\n",
    "            v_data, v_mask, v_ptars, _, v_tars, _ = data\n",
    "            err, acc_spk, acc_phr = val_func(v_data, v_mask ,v_tars, v_ptars)\n",
    "            val_err += err\n",
    "            val_acc_spk += acc_spk\n",
    "            val_acc_phr += acc_phr\n",
    "            \n",
    "            val_batches += 1\n",
    "\n",
    "        trainerr.append(train_err/train_batches)\n",
    "\n",
    "        epoch+=1\n",
    "        train_set.close(h1)\n",
    "        valid_set.close(h2)\n",
    "        \n",
    "        #Display\n",
    "        if display:\n",
    "            \n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "            print(\"  training accuracy (speaker):\\t\\t{:.2f} %\".format(\n",
    "                tr_acc_spk / train_batches * 100))\n",
    "            print(\"  training accuracy (phrase):\\t\\t{:.2f} %\".format(\n",
    "                tr_acc_phr / train_batches * 100))\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "            print(\"  validation accuracy (speaker):\\t\\t{:.2f} %\".format(\n",
    "                val_acc_spk / val_batches * 100))\n",
    "            print(\"  validation accuracy (phrase):\\t\\t{:.2f} %\".format(\n",
    "                val_acc_phr / val_batches * 100))\n",
    "\n",
    "        logfile = os.path.join(SAVE_PATH,MODEL_NAME,'logs',MODEL_NAME+'.log')\n",
    "        flog1 = open(logfile,'ab')\n",
    "        flog1.write(\"Running %s on %s\" % (SCRIPT,MSC))\n",
    "        flog1.write(\"Epoch {} of {} took {:.3f}s \\n\".format(\n",
    "        epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "        flog1.write(\"  training loss:\\t\\t{:.6f}\\n\".format(train_err / train_batches))\n",
    "        flog1.write(\"  training accuracy (speaker):\\t\\t{:.2f} %\\n\".format(\n",
    "            tr_acc_spk / train_batches * 100))\n",
    "        flog1.write(\"  training accuracy (phrase):\\t\\t{:.2f} %\\n\".format(\n",
    "            tr_acc_phr / train_batches * 100))\n",
    "        flog1.write(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        flog1.write(\"  validation accuracy (speaker):\\t\\t{:.2f} %\\n\".format(\n",
    "            val_acc_spk / val_batches * 100))\n",
    "        flog1.write(\"  validation accuracy (phrase):\\t\\t{:.2f} %\\n\".format(\n",
    "            val_acc_phr / val_batches * 100))\n",
    "        \n",
    "        flog1.write(\"\\n\")\n",
    "        flog1.close()\n",
    "\n",
    "        if epoch == NUM_EPOCHS:\n",
    "            break\n",
    " \n",
    "        valE = val_err/val_batches\n",
    "        \n",
    "        if valE <= min_val_loss:\n",
    "\n",
    "            model_params1 = lasagne.layers.get_all_param_values([spk_network, phr_network])\n",
    "\n",
    "            model1_name = MODEL_NAME+'_minloss' + '.pkl'\n",
    "\n",
    "            vpth1 = os.path.join(SAVE_PATH, MODEL_NAME, 'weights',model1_name)\n",
    "\n",
    "            fsave = open(vpth1,'wb')  \n",
    "\n",
    "            cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            fsave.close()\n",
    "\n",
    "            min_val_loss = valE\n",
    "\n",
    "        #Patience / Early stopping\n",
    "        else:\n",
    "            patience+=1\n",
    "        \n",
    "        if patience==10:\n",
    "            break\n",
    "            \n",
    "    #Save the final model\n",
    "\n",
    "    print('Saving Model ...')\n",
    "    model_params = lasagne.layers.get_all_param_values([spk_network, phr_network])\n",
    "    model1_name = MODEL_NAME+'_final' + '.pkl'\n",
    "    vpth = os.path.join(SAVE_PATH, MODEL_NAME,'weights',model1_name)\n",
    "    fsave = open(vpth,'wb')  \n",
    "    cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    fsave.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
