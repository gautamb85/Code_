{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../rsr_multitask.py\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cPickle\n",
    "import sys\n",
    "import subprocess \n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.converters.base import fill_hdf5_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cFile = sys.argv[1]\n",
    "\n",
    "f=open(cFile)\n",
    "lines = f.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "values = [v.split()[1] for v in lines]\n",
    "\n",
    "\n",
    "N_HIDDEN = int(values[0])\n",
    "F_DIM = int(values[1])\n",
    "LEARNING_RATE = np.cast['float32'](values[2])\n",
    "BATCH_SIZE = int(values[3])\n",
    "NUM_EPOCHS = int(values[4])\n",
    "NUM_SPEAKERS = int(values[5])\n",
    "NUM_PHRASES = int(values[6])\n",
    "MODEL_NAME = values[7]\n",
    "SAVE_PATH = values[8]\n",
    "#data\n",
    "TRAIN_FILE = values[9]\n",
    "VALID_FILE = values[10]\n",
    "SCRIPT = values[11]\n",
    "MSC = values[12]\n",
    "\n",
    "wtspth =os.path.join(SAVE_PATH,MODEL_NAME,'weights')\n",
    "logpth =os.path.join(SAVE_PATH,MODEL_NAME,'logs')\n",
    "\n",
    "#create a folder for logs and weights\n",
    "if os.path.exists(wtspth):\n",
    "    print 'weight folder exits\\n'\n",
    "else:\n",
    "    Command1 = \"mkdir -p\" +\" \"+ wtspth\n",
    "    process = subprocess.check_call(Command1.split())\n",
    "\n",
    "if os.path.exists(logpth):\n",
    "    print 'logs folder exists'\n",
    "else:\n",
    "    Command2 = \"mkdir -p\" +\" \"+ logpth\n",
    "    process2 = subprocess.check_call(Command2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/misc/data15/reco/bhattgau/Rnn/code/Code_/notebooks'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_dummy = np.random.random((2,800,40))\n",
    "x_dummy = np.cast['float32'](x_dummy)\n",
    "mask = np.zeros((2,800), dtype='float32')\n",
    "mask[0,:344] = 1.0\n",
    "mask[1,:444] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../extract_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../extract_features.py\n",
    "\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cPickle\n",
    "import sys\n",
    "import subprocess \n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "F_DIM=40\n",
    "N_HIDDEN=500\n",
    "NUM_SPEAKERS=194\n",
    "NUM_PHRASES=30\n",
    "#def build_rnn(net_input=None, mask_input=None):\n",
    "\n",
    "    #print(\"Building network ...\")\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "MASK = T.matrix(name = 'mask', dtype='float32')\n",
    "    \n",
    "l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = X)\n",
    "n_batch,_,_ = l_in.input_var.shape\n",
    "#print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = MASK)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "#initialize the gates\n",
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "b=lasagne.init.Constant(0.))\n",
    "\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "W_cell=None, b=lasagne.init.Constant(0.),nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "\n",
    "l_forward = lasagne.layers.LSTMLayer(l_in, N_HIDDEN, mask_input=l_mask,\n",
    "                                     precompute_input=True, peepholes=True, learn_init=True)\n",
    "\n",
    "\n",
    "l_backward = lasagne.layers.LSTMLayer(l_in, N_HIDDEN, mask_input=l_mask,\n",
    "                                     precompute_input=True, peepholes=True, learn_init=True, backwards=True)\n",
    "\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "l_last = lasagne.layers.SliceLayer(l_sum,-1,1)\n",
    "\n",
    "#to be ignored\n",
    "l_spk_softmax = lasagne.layers.DenseLayer(l_last, num_units=NUM_SPEAKERS, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "l_phr_softmax = lasagne.layers.DenseLayer(l_last, num_units=NUM_PHRASES, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "#set network weights\n",
    "Wfile = open('/misc/data15/reco/bhattgau/Rnn/projects/Rvector/rsr_multitask_fbank_500/weights/rsr_multitask_fbank_500_minloss.pkl')\n",
    "Wts = cPickle.load(Wfile)\n",
    "lasagne.layers.set_all_param_values([l_spk_softmax, l_phr_softmax], Wts)\n",
    "\n",
    "Rvectors = lasagne.layers.get_output(l_sum)\n",
    "\n",
    "l_last = Rvectors[:,-1,:]\n",
    "\n",
    "feat_extractor = theano.function([X, MASK], [Rvectors, l_last])    \n",
    "\n",
    "TRAIN_FILE='/misc/data15/reco/bhattgau/Rnn/data/rsr/rsr-names-features.hdf5'\n",
    "SAVE='/misc/data15/reco/bhattgau/Rnn/data/r-vectors/rsr_multitask_500fb'\n",
    "\n",
    "R_VEC_FR=[]\n",
    "R_VEC_UTT = []\n",
    "\n",
    "train_set = H5PYDataset(TRAIN_FILE,  which_sets=('train',))\n",
    "#valid_set = H5PYDataset(TRAIN_FILE, which_sets=('test',))\n",
    "\n",
    "h1=train_set.open()\n",
    "h2=valid_set.open()\n",
    "\n",
    "scheme = SequentialScheme(examples=train_set.num_examples, batch_size=1)\n",
    "\n",
    "train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "#valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "\n",
    "for data in train_stream.get_epoch_iterator():\n",
    "    t_data, t_mask, t_name = data\n",
    "    t_name = str(t_name)\n",
    "    t_name = t_name[2:-2]\n",
    "    #extract features\n",
    "    frame_f, utt_f = feat_extractor(t_data, t_mask)\n",
    "    lastP = np.sum(t_mask, axis=1)\n",
    "    frame_f = frame_f[:,:lastP,:]\n",
    "    frame_f = np.reshape(frame_f, (frame_f.shape[1], frame_f.shape[2]))\n",
    "    \n",
    "    fr_feat = os.path.join(SAVE,'frame',t_name+'.npy')\n",
    "    utt_feat = os.path.join(SAVE,'Utt',t_name +'.npy')\n",
    "    \n",
    "    ffile = open(fr_feat, 'wb')\n",
    "    ufile = open(utt_feat, 'wb')\n",
    "    \n",
    "    np.save(ffile, frame_f)\n",
    "    np.save(ufile, utt_f)\n",
    "\n",
    "    ffile.close()\n",
    "    ufile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/misc/data15/reco/bhattgau/Rnn/code/Code_/extract_features.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mt_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;31m#extract features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mframe_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutt_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeat_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mlastP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mframe_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlastP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/misc/home/reco/bhattaga/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    950\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "close failed in file object destructor:\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 116] Stale file handle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;31mIOError\u001b[0m: [Errno 116] Stale file handle"
     ]
    }
   ],
   "source": [
    "run ../extract_features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../extract_features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../extract_features.py\n",
    "\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cPickle\n",
    "import sys\n",
    "import subprocess \n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "F_DIM=40\n",
    "N_HIDDEN=500\n",
    "NUM_SPEAKERS=194\n",
    "NUM_PHRASES=30\n",
    "#def build_rnn(net_input=None, mask_input=None):\n",
    "\n",
    "    #print(\"Building network ...\")\n",
    "\n",
    "X = T.tensor3(name='input',dtype='float32')\n",
    "MASK = T.matrix(name = 'mask', dtype='float32')\n",
    "    \n",
    "l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = X)\n",
    "n_batch,_,_ = l_in.input_var.shape\n",
    "#print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = MASK)\n",
    "#print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "#initialize the gates\n",
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "b=lasagne.init.Constant(0.))\n",
    "\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "W_cell=None, b=lasagne.init.Constant(0.),nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "\n",
    "l_forward = lasagne.layers.LSTMLayer(l_in, N_HIDDEN, mask_input=l_mask,\n",
    "                                     precompute_input=True, peepholes=True, learn_init=True)\n",
    "\n",
    "\n",
    "l_backward = lasagne.layers.LSTMLayer(l_in, N_HIDDEN, mask_input=l_mask,\n",
    "                                     precompute_input=True, peepholes=True, learn_init=True, backwards=True)\n",
    "\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "l_last = lasagne.layers.SliceLayer(l_sum,-1,1)\n",
    "\n",
    "#to be ignored\n",
    "l_spk_softmax = lasagne.layers.DenseLayer(l_last, num_units=NUM_SPEAKERS, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "l_phr_softmax = lasagne.layers.DenseLayer(l_last, num_units=NUM_PHRASES, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "#set network weights\n",
    "Wfile = open('/misc/data15/reco/bhattgau/Rnn/projects/Rvector/rsr_multitask_fbank_500/weights/rsr_multitask_fbank_500_minloss.pkl')\n",
    "Wts = cPickle.load(Wfile)\n",
    "lasagne.layers.set_all_param_values([l_spk_softmax, l_phr_softmax], Wts)\n",
    "\n",
    "Rvectors = lasagne.layers.get_output(l_sum)\n",
    "\n",
    "l_last = Rvectors[:,-1,:]\n",
    "\n",
    "feat_extractor = theano.function([X, MASK], [Rvectors, l_last])    \n",
    "\n",
    "TRAIN_FILE='/misc/data15/reco/bhattgau/Rnn/data/rsr/rsr-names-features.hdf5'\n",
    "SAVE='/misc/data15/reco/bhattgau/Rnn/data/r-vectors/rsr_multitask_500fb'\n",
    "\n",
    "R_VEC_FR=[]\n",
    "R_VEC_UTT = []\n",
    "\n",
    "train_set = H5PYDataset(TRAIN_FILE,  which_sets=('train',))\n",
    "#valid_set = H5PYDataset(TRAIN_FILE, which_sets=('test',))\n",
    "\n",
    "h1=train_set.open()\n",
    "\n",
    "scheme = SequentialScheme(examples=train_set.num_examples, batch_size=1)\n",
    "\n",
    "train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "#valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "\n",
    "for data in train_stream.get_epoch_iterator():\n",
    "    t_data, t_mask, t_name = data\n",
    "    t_name = str(t_name)\n",
    "    t_name = t_name[2:-2]\n",
    "    #extract features\n",
    "    frame_f, utt_f = feat_extractor(t_data, t_mask)\n",
    "    lastP = np.sum(t_mask, axis=1)\n",
    "    frame_f = frame_f[:,:lastP,:]\n",
    "    frame_f = np.reshape(frame_f, (frame_f.shape[1], frame_f.shape[2]))\n",
    "    \n",
    "    fr_feat = os.path.join(SAVE,'frame',t_name+'.npy')\n",
    "    utt_feat = os.path.join(SAVE,'Utt',t_name +'.npy')\n",
    "    \n",
    "    ffile = open(fr_feat, 'wb')\n",
    "    ufile = open(utt_feat, 'wb')\n",
    "    \n",
    "    np.save(ffile, frame_f)\n",
    "    np.save(ufile, utt_f)\n",
    "\n",
    "    ffile.close()\n",
    "    ufile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    l_spk_softmax = lasagne.layers.DenseLayer(l_sum, num_units=NUM_SPEAKERS, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    l_phr_softmax = lasagne.layers.DenseLayer(l_sum, num_units=NUM_PHRASES, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_spk_softmax, l_phr_softmax\n",
    "\n",
    "\n",
    "#print lasagne.layers.get_output(l_softmax, inputs={l_in: X, l_mask: Mask}).eval({X: d1, Mask: m1}).shape\n",
    "#l_softmax = lasagne.layers.ReshapeLayer(l_dense, (n_batch, MAX_LENGTH, N_HIDDEN))\n",
    "\n",
    "def main():\n",
    "        \n",
    "    X = T.tensor3(name='input',dtype='float32')\n",
    "    MASK = T.matrix(name = 'mask', dtype='float32')\n",
    "    SPK_LABELS = T.ivector(name='spk_labels')\n",
    "    PHR_LABELS = T.ivector(name='phr_labels')\n",
    "\n",
    "    #load data\n",
    "    train_set = H5PYDataset(TRAIN_FILE,  which_sets=('train',))\n",
    "    valid_set = H5PYDataset(TRAIN_FILE, which_sets=('test',))\n",
    "\n",
    "    spk_network, phr_network = build_rnn(net_input=X, mask_input= MASK)\n",
    "    \n",
    "    network_output_spk = lasagne.layers.get_output(spk_network)\n",
    "    network_output_phr = lasagne.layers.get_output(phr_network)\n",
    "\n",
    "    val_prediction_spk = lasagne.layers.get_output(spk_network, deterministic=True)\n",
    "    val_prediction_phr = lasagne.layers.get_output(phr_network, deterministic=True)\n",
    "    #needed for accuracy\n",
    "    \n",
    "    val_acc_spk = T.mean(T.eq(T.argmax(val_prediction_spk, axis=1), SPK_LABELS), dtype=theano.config.floatX)\n",
    "    val_acc_phr = T.mean(T.eq(T.argmax(val_prediction_phr, axis=1), PHR_LABELS), dtype=theano.config.floatX)\n",
    "\n",
    "    #training accuracy\n",
    "    train_acc_spk = T.mean(T.eq(T.argmax(network_output_spk, axis=1), SPK_LABELS), dtype=theano.config.floatX)\n",
    "    train_acc_phr = T.mean(T.eq(T.argmax(network_output_phr, axis=1), PHR_LABELS), dtype=theano.config.floatX)\n",
    "    \n",
    "    #cost function    \n",
    "    spk_cost = lasagne.objectives.categorical_crossentropy(network_output_spk, SPK_LABELS)\n",
    "    phr_cost = lasagne.objectives.categorical_crossentropy(network_output_phr, PHR_LABELS)\n",
    "    total_cost = spk_cost + phr_cost\n",
    "    mean_cost = total_cost.mean()\n",
    "    \n",
    "    #Validation cost\n",
    "    val_spk_cost = lasagne.objectives.categorical_crossentropy(val_prediction_spk, SPK_LABELS)\n",
    "    val_phr_cost = lasagne.objectives.categorical_crossentropy(val_prediction_phr, PHR_LABELS)\n",
    "    val_cost = val_spk_cost + val_phr_cost\n",
    "    val_mcost = val_cost.mean()\n",
    "\n",
    "    #Get parameters of both encoder and decoder\n",
    "    all_parameters = lasagne.layers.get_all_params([spk_network, phr_network], trainable=True)\n",
    "\n",
    "    print(\"Trainable Model Parameters\")\n",
    "    print(\"-\"*40)\n",
    "    for param in all_parameters:\n",
    "        print(param, param.get_value().shape)\n",
    "    print(\"-\"*40)\n",
    "    #add grad clipping to avoid exploding gradients\n",
    "    all_grads = [T.clip(g,-3,3) for g in T.grad(mean_cost, all_parameters)]\n",
    "    #all_grads = lasagne.updates.total_norm_constraint(all_grads,3)\n",
    "\n",
    "    updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=LEARNING_RATE)\n",
    "\n",
    "    train_func = theano.function([X, MASK, SPK_LABELS, PHR_LABELS], [mean_cost, train_acc_spk, train_acc_phr], updates=updates)\n",
    "\n",
    "    val_func = theano.function([X, MASK, SPK_LABELS, PHR_LABELS], [val_mcost, val_acc_spk, val_acc_phr])\n",
    "\n",
    "    trainerr=[]\n",
    "    epoch=0 #set the epoch counter\n",
    "    \n",
    "    min_val_loss = np.inf\n",
    "    max_val = np.inf*(-1)\n",
    "    \n",
    "    patience=0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "        # We iterate over epochs:\n",
    "    while 'true':\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        tr_acc_spk = 0\n",
    "        tr_acc_phr = 0\n",
    "\n",
    "        train_batches = 0\n",
    "\n",
    "        h1=train_set.open()\n",
    "        h2=valid_set.open()\n",
    "\n",
    "        scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=BATCH_SIZE)\n",
    "        scheme1 = SequentialScheme(examples=valid_set.num_examples, batch_size=32)\n",
    "\n",
    "\n",
    "        train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "        valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for data in train_stream.get_epoch_iterator():\n",
    "            t_data, t_mask, t_plabs,_, t_labs,_ = data\n",
    "            terr, tacc_spk, tacc_phr = train_func(t_data, t_mask, t_labs, t_plabs)\n",
    "            train_err += terr\n",
    "            \n",
    "            tr_acc_spk += tacc_spk\n",
    "            tr_acc_phr += tacc_phr\n",
    "            \n",
    "            train_batches += 1\n",
    "\n",
    "        val_err = 0\n",
    "        val_acc_spk = 0\n",
    "        val_acc_phr = 0\n",
    "        \n",
    "        val_batches = 0\n",
    "\n",
    "        for data in valid_stream.get_epoch_iterator():\n",
    "            v_data, v_mask, v_ptars, _, v_tars, _ = data\n",
    "            err, acc_spk, acc_phr = val_func(v_data, v_mask ,v_tars, v_ptars)\n",
    "            val_err += err\n",
    "            val_acc_spk += acc_spk\n",
    "            val_acc_phr += acc_phr\n",
    "            \n",
    "            val_batches += 1\n",
    "\n",
    "        trainerr.append(train_err/train_batches)\n",
    "\n",
    "        epoch+=1\n",
    "        train_set.close(h1)\n",
    "        valid_set.close(h2)\n",
    "        \n",
    "        #Display\n",
    "        if display:\n",
    "            \n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "            print(\"  training accuracy (speaker):\\t\\t{:.2f} %\".format(\n",
    "                tr_acc_spk / train_batches * 100))\n",
    "            print(\"  training accuracy (phrase):\\t\\t{:.2f} %\".format(\n",
    "                tr_acc_phr / train_batches * 100))\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "            print(\"  validation accuracy (speaker):\\t\\t{:.2f} %\".format(\n",
    "                val_acc_spk / val_batches * 100))\n",
    "            print(\"  validation accuracy (phrase):\\t\\t{:.2f} %\".format(\n",
    "                val_acc_phr / val_batches * 100))\n",
    "\n",
    "        logfile = os.path.join(SAVE_PATH,MODEL_NAME,'logs',MODEL_NAME+'.log')\n",
    "        flog1 = open(logfile,'ab')\n",
    "        flog1.write(\"Running %s on %s\" % (SCRIPT,MSC))\n",
    "        flog1.write(\"Epoch {} of {} took {:.3f}s \\n\".format(\n",
    "        epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "        flog1.write(\"  training loss:\\t\\t{:.6f}\\n\".format(train_err / train_batches))\n",
    "        flog1.write(\"  training accuracy (speaker):\\t\\t{:.2f} %\\n\".format(\n",
    "            tr_acc_spk / train_batches * 100))\n",
    "        flog1.write(\"  training accuracy (phrase):\\t\\t{:.2f} %\\n\".format(\n",
    "            tr_acc_phr / train_batches * 100))\n",
    "        flog1.write(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        flog1.write(\"  validation accuracy (speaker):\\t\\t{:.2f} %\\n\".format(\n",
    "            val_acc_spk / val_batches * 100))\n",
    "        flog1.write(\"  validation accuracy (phrase):\\t\\t{:.2f} %\\n\".format(\n",
    "            val_acc_phr / val_batches * 100))\n",
    "        \n",
    "        flog1.write(\"\\n\")\n",
    "        flog1.close()\n",
    "\n",
    "        if epoch == NUM_EPOCHS:\n",
    "            break\n",
    " \n",
    "        valE = val_err/val_batches\n",
    "        \n",
    "        if valE <= min_val_loss:\n",
    "\n",
    "            model_params1 = lasagne.layers.get_all_param_values([spk_network, phr_network])\n",
    "\n",
    "            model1_name = MODEL_NAME+'_minloss' + '.pkl'\n",
    "\n",
    "            vpth1 = os.path.join(SAVE_PATH, MODEL_NAME, 'weights',model1_name)\n",
    "\n",
    "            fsave = open(vpth1,'wb')  \n",
    "\n",
    "            cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            fsave.close()\n",
    "\n",
    "            min_val_loss = valE\n",
    "\n",
    "        #Patience / Early stopping\n",
    "        else:\n",
    "            patience+=1\n",
    "        \n",
    "        if patience==10:\n",
    "            break\n",
    "            \n",
    "    #Save the final model\n",
    "\n",
    "    print('Saving Model ...')\n",
    "    model_params = lasagne.layers.get_all_param_values([spk_network, phr_network])\n",
    "    model1_name = MODEL_NAME+'_final' + '.pkl'\n",
    "    vpth = os.path.join(SAVE_PATH, MODEL_NAME,'weights',model1_name)\n",
    "    fsave = open(vpth,'wb')  \n",
    "    cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    fsave.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
