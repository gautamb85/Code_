{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight folder exits\n",
      "\n",
      "logs folder exists\n",
      "Building network ...\n",
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W, (400, 100))\n",
      "(b, (100,))\n",
      "(W, (100, 100))\n",
      "(b, (100,))\n",
      "(W, (100, 400))\n",
      "(b, (400,))\n",
      "----------------------------------------\n",
      "Starting training...\n",
      "Epoch 1 of 3 took 0.086s \n",
      "  training loss:\t\t1.319932 \n",
      "  validation loss:\t\t0.979781\n",
      "Epoch 2 of 3 took 0.075s \n",
      "  training loss:\t\t1.035115 \n",
      "  validation loss:\t\t0.985708\n",
      "Epoch 3 of 3 took 0.073s \n",
      "  training loss:\t\t0.994691 \n",
      "  validation loss:\t\t0.980856\n",
      "Saving Model ...\n"
     ]
    }
   ],
   "source": [
    "run ivec_dae.py config/ivec_dae.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rsr_multitask.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rsr_multitask.py\n",
    "import htkmfc\n",
    "import os\n",
    "import lasagne\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cPickle\n",
    "import sys\n",
    "import subprocess \n",
    "\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.schemes import ShuffledScheme, SequentialScheme\n",
    "from fuel.streams import DataStream\n",
    "\n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "from utils import load_data\n",
    "\n",
    "cFile = sys.argv[1]\n",
    "\n",
    "f=open(cFile)\n",
    "lines = f.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "values = [v.split()[1] for v in lines]\n",
    "\n",
    "\n",
    "N_HIDDEN = int(values[0])\n",
    "F_DIM = int(values[1])\n",
    "LEARNING_RATE = np.cast['float32'](values[2])\n",
    "BATCH_SIZE = int(values[3])\n",
    "NUM_EPOCHS = int(values[4])\n",
    "NUM_SPEAKERS = int(values[5])\n",
    "NUM_PHRASES = int(values[6])\n",
    "MODEL_NAME = values[7]\n",
    "SAVE_PATH = values[8]\n",
    "#data\n",
    "TRAIN_FILE = values[9]\n",
    "VALID_FILE = values[10]\n",
    "SCRIPT = values[11]\n",
    "MSC = values[12]\n",
    "\n",
    "wtspth =os.path.join(SAVE_PATH,MODEL_NAME,'weights')\n",
    "logpth =os.path.join(SAVE_PATH,MODEL_NAME,'logs')\n",
    "\n",
    "#create a folder for logs and weights\n",
    "if os.path.exists(wtspth):\n",
    "    print 'weight folder exits\\n'\n",
    "else:\n",
    "    Command1 = \"mkdir -p\" +\" \"+ wtspth\n",
    "    process = subprocess.check_call(Command1.split())\n",
    "\n",
    "if os.path.exists(logpth):\n",
    "    print 'logs folder exists'\n",
    "else:\n",
    "    Command2 = \"mkdir -p\" +\" \"+ logpth\n",
    "    process2 = subprocess.check_call(Command2.split())\n",
    "\n",
    "\n",
    "\n",
    "# Min/max sequence length\n",
    "MAX_LENGTH = 800\n",
    "\n",
    "def build_rnn(net_input=None, mask_input=None):\n",
    "\n",
    "    print(\"Building network ...\")\n",
    "\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH, F_DIM), input_var = net_input)\n",
    "    n_batch,_,_ = l_in.input_var.shape\n",
    "    #print lasagne.layers.get_output(l_in, inputs={l_in: X}).eval({X: x_dummy}).shape\n",
    "\n",
    "    l_mask = lasagne.layers.InputLayer(shape=(None, MAX_LENGTH), input_var = mask_input)\n",
    "    #print lasagne.layers.get_output(l_mask, inputs={l_mask: Mask}).eval({Mask: mask}).shape\n",
    "\n",
    "    #initialize the gates\n",
    "    gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    b=lasagne.init.Constant(0.))\n",
    "    \n",
    "    cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    W_cell=None, b=lasagne.init.Constant(0.),nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "\n",
    "    l_forward = lasagne.layers.LSTMLayer(l_in, N_HIDDEN, mask_input=l_mask,\n",
    "                                         precompute_input=True, peepholes=True, \n",
    "                                         learn_init=True, only_return_final=True)\n",
    "        \n",
    "    \n",
    "    l_backward = lasagne.layers.LSTMLayer(l_in, N_HIDDEN, mask_input=l_mask,\n",
    "                                         precompute_input=True, peepholes=True, \n",
    "                                         learn_init=True, only_return_final=True, backwards=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    l_sum = lasagne.layers.ElemwiseSumLayer([l_forward, l_backward])\n",
    "\n",
    "    l_spk_softmax = lasagne.layers.DenseLayer(l_sum, num_units=NUM_SPEAKERS, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    l_phr_softmax = lasagne.layers.DenseLayer(l_sum, num_units=NUM_PHRASES, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_spk_softmax, l_phr_softmax\n",
    "\n",
    "\n",
    "#print lasagne.layers.get_output(l_softmax, inputs={l_in: X, l_mask: Mask}).eval({X: d1, Mask: m1}).shape\n",
    "#l_softmax = lasagne.layers.ReshapeLayer(l_dense, (n_batch, MAX_LENGTH, N_HIDDEN))\n",
    "\n",
    "def main():\n",
    "        \n",
    "    X = T.tensor3(name='input',dtype='float32')\n",
    "    MASK = T.matrix(name = 'mask', dtype='float32')\n",
    "    SPK_LABELS = T.ivector(name='spk_labels')\n",
    "    PHR_LABELS = T.ivector(name='phr_labels')\n",
    "\n",
    "    #load data\n",
    "    train_set = H5PYDataset(TRAIN_FILE,  which_sets=('train',))\n",
    "    valid_set = H5PYDataset(TRAIN_FILE, which_sets=('test',))\n",
    "\n",
    "    spk_network, phr_network = build_rnn(net_input=X, mask_input= MASK)\n",
    "    \n",
    "    network_output_spk = lasagne.layers.get_output(spk_network)\n",
    "    network_output_phr = lasagne.layers.get_output(phr_network)\n",
    "\n",
    "    val_prediction_spk = lasagne.layers.get_output(spk_network, deterministic=True)\n",
    "    val_prediction_phr = lasagne.layers.get_output(phr_network, deterministic=True)\n",
    "    #needed for accuracy\n",
    "    \n",
    "    val_acc_spk = T.mean(T.eq(T.argmax(val_prediction_spk, axis=1), SPK_LABELS), dtype=theano.config.floatX)\n",
    "    val_acc_phr = T.mean(T.eq(T.argmax(val_prediction_phr, axis=1), PHR_LABELS), dtype=theano.config.floatX)\n",
    "\n",
    "    #training accuracy\n",
    "    train_acc_spk = T.mean(T.eq(T.argmax(network_output_spk, axis=1), SPK_LABELS), dtype=theano.config.floatX)\n",
    "    train_acc_phr = T.mean(T.eq(T.argmax(network_output_phr, axis=1), PHR_LABELS), dtype=theano.config.floatX)\n",
    "    \n",
    "    #cost function    \n",
    "    spk_cost = lasagne.objectives.categorical_crossentropy(network_output_spk, SPK_LABELS)\n",
    "    phr_cost = lasagne.objectives.categorical_crossentropy(network_output_phr, PHR_LABELS)\n",
    "    total_cost = spk_cost + phr_cost\n",
    "    mean_cost = total_cost.mean()\n",
    "    \n",
    "    #Validation cost\n",
    "    val_spk_cost = lasagne.objectives.categorical_crossentropy(val_prediction_spk, SPK_LABELS)\n",
    "    val_phr_cost = lasagne.objectives.categorical_crossentropy(val_prediction_phr, PHR_LABELS)\n",
    "    val_cost = val_spk_cost + val_phr_cost\n",
    "    val_mcost = val_cost.mean()\n",
    "\n",
    "    #Get parameters of both encoder and decoder\n",
    "    all_parameters = lasagne.layers.get_all_params([spk_network, phr_network], trainable=True)\n",
    "\n",
    "    print(\"Trainable Model Parameters\")\n",
    "    print(\"-\"*40)\n",
    "    for param in all_parameters:\n",
    "        print(param, param.get_value().shape)\n",
    "    print(\"-\"*40)\n",
    "    #add grad clipping to avoid exploding gradients\n",
    "    all_grads = [T.clip(g,-3,3) for g in T.grad(mean_cost, all_parameters)]\n",
    "    #all_grads = lasagne.updates.total_norm_constraint(all_grads,3)\n",
    "\n",
    "    updates = lasagne.updates.adam(all_grads, all_parameters, learning_rate=LEARNING_RATE)\n",
    "\n",
    "    train_func = theano.function([X, MASK, SPK_LABELS, PHR_LABELS], [mean_cost, train_acc_spk, train_acc_phr], updates=updates)\n",
    "\n",
    "    val_func = theano.function([X, MASK, SPK_LABELS, PHR_LABELS], [val_mcost, val_acc_spk, val_acc_phr])\n",
    "\n",
    "    trainerr=[]\n",
    "    epoch=0 #set the epoch counter\n",
    "    \n",
    "    min_val_loss = np.inf\n",
    "    max_val = np.inf*(-1)\n",
    "    \n",
    "    patience=0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "        # We iterate over epochs:\n",
    "    while 'true':\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        tr_acc_spk = 0\n",
    "        tr_acc_phr = 0\n",
    "\n",
    "        train_batches = 0\n",
    "\n",
    "        h1=train_set.open()\n",
    "        h2=valid_set.open()\n",
    "\n",
    "        scheme = ShuffledScheme(examples=train_set.num_examples, batch_size=BATCH_SIZE)\n",
    "        scheme1 = SequentialScheme(examples=valid_set.num_examples, batch_size=32)\n",
    "\n",
    "\n",
    "        train_stream = DataStream(dataset=train_set, iteration_scheme=scheme)\n",
    "        valid_stream = DataStream(dataset=valid_set, iteration_scheme=scheme1)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for data in train_stream.get_epoch_iterator():\n",
    "            t_data, t_mask, t_plabs,_, t_labs,_ = data\n",
    "            terr, tacc_spk, tacc_phr = train_func(t_data, t_mask, t_labs, t_plabs)\n",
    "            train_err += terr\n",
    "            \n",
    "            tr_acc_spk += tacc_spk\n",
    "            tr_acc_phr += tacc_phr\n",
    "            \n",
    "            train_batches += 1\n",
    "\n",
    "        val_err = 0\n",
    "        val_acc_spk = 0\n",
    "        val_acc_phr = 0\n",
    "        \n",
    "        val_batches = 0\n",
    "\n",
    "        for data in valid_stream.get_epoch_iterator():\n",
    "            v_data, v_mask, v_ptars, _, v_tars, _ = data\n",
    "            err, acc_spk, acc_phr = val_func(v_data, v_mask ,v_tars, v_ptars)\n",
    "            val_err += err\n",
    "            val_acc_spk += acc_spk\n",
    "            val_acc_phr += acc_phr\n",
    "            \n",
    "            val_batches += 1\n",
    "\n",
    "        trainerr.append(train_err/train_batches)\n",
    "\n",
    "        epoch+=1\n",
    "        train_set.close(h1)\n",
    "        valid_set.close(h2)\n",
    "        \n",
    "        #Display\n",
    "        if display:\n",
    "            \n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "            print(\"  training accuracy (speaker):\\t\\t{:.2f} %\".format(\n",
    "                tr_acc_spk / train_batches * 100))\n",
    "            print(\"  training accuracy (phrase):\\t\\t{:.2f} %\".format(\n",
    "                tr_acc_phr / train_batches * 100))\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "            print(\"  validation accuracy (speaker):\\t\\t{:.2f} %\".format(\n",
    "                val_acc_spk / val_batches * 100))\n",
    "            print(\"  validation accuracy (phrase):\\t\\t{:.2f} %\".format(\n",
    "                val_acc_phr / val_batches * 100))\n",
    "\n",
    "        logfile = os.path.join(SAVE_PATH,MODEL_NAME,'logs',MODEL_NAME+'.log')\n",
    "        flog1 = open(logfile,'ab')\n",
    "        flog1.write(\"Running %s on %s\" % (SCRIPT,MSC))\n",
    "        flog1.write(\"Epoch {} of {} took {:.3f}s \\n\".format(\n",
    "        epoch, NUM_EPOCHS, time.time() - start_time))\n",
    "        flog1.write(\"  training loss:\\t\\t{:.6f}\\n\".format(train_err / train_batches))\n",
    "        flog1.write(\"  training accuracy (speaker):\\t\\t{:.2f} %\\n\".format(\n",
    "            tr_acc_spk / train_batches * 100))\n",
    "        flog1.write(\"  training accuracy (phrase):\\t\\t{:.2f} %\\n\".format(\n",
    "            tr_acc_phr / train_batches * 100))\n",
    "        flog1.write(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        flog1.write(\"  validation accuracy (speaker):\\t\\t{:.2f} %\\n\".format(\n",
    "            val_acc_spk / val_batches * 100))\n",
    "        flog1.write(\"  validation accuracy (phrase):\\t\\t{:.2f} %\\n\".format(\n",
    "            val_acc_phr / val_batches * 100))\n",
    "        \n",
    "        flog1.write(\"\\n\")\n",
    "        flog1.close()\n",
    "\n",
    "        if epoch == NUM_EPOCHS:\n",
    "            break\n",
    " \n",
    "        valE = val_err/val_batches\n",
    "        \n",
    "        if valE <= min_val_loss:\n",
    "\n",
    "            model_params1 = lasagne.layers.get_all_param_values([spk_network, phr_network])\n",
    "\n",
    "            model1_name = MODEL_NAME+'_minloss' + '.pkl'\n",
    "\n",
    "            vpth1 = os.path.join(SAVE_PATH, MODEL_NAME, 'weights',model1_name)\n",
    "\n",
    "            fsave = open(vpth1,'wb')  \n",
    "\n",
    "            cPickle.dump(model_params1, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            fsave.close()\n",
    "\n",
    "            min_val_loss = valE\n",
    "\n",
    "        #Patience / Early stopping\n",
    "        else:\n",
    "            patience+=1\n",
    "        \n",
    "        if patience==10:\n",
    "            break\n",
    "            \n",
    "    #Save the final model\n",
    "\n",
    "    print('Saving Model ...')\n",
    "    model_params = lasagne.layers.get_all_param_values([spk_network, phr_network])\n",
    "    model1_name = MODEL_NAME+'_final' + '.pkl'\n",
    "    vpth = os.path.join(SAVE_PATH, MODEL_NAME,'weights',model1_name)\n",
    "    fsave = open(vpth,'wb')  \n",
    "    cPickle.dump(model_params, fsave, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    fsave.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight folder exits\n",
      "\n",
      "logs folder exists\n",
      "Building network ...\n",
      "Trainable Model Parameters\n",
      "----------------------------------------\n",
      "(W_in_to_ingate, (40, 50))\n",
      "(W_hid_to_ingate, (50, 50))\n",
      "(b_ingate, (50,))\n",
      "(W_in_to_forgetgate, (40, 50))\n",
      "(W_hid_to_forgetgate, (50, 50))\n",
      "(b_forgetgate, (50,))\n",
      "(W_in_to_cell, (40, 50))\n",
      "(W_hid_to_cell, (50, 50))\n",
      "(b_cell, (50,))\n",
      "(W_in_to_outgate, (40, 50))\n",
      "(W_hid_to_outgate, (50, 50))\n",
      "(b_outgate, (50,))\n",
      "(W_cell_to_ingate, (50,))\n",
      "(W_cell_to_forgetgate, (50,))\n",
      "(W_cell_to_outgate, (50,))\n",
      "(cell_init, (1, 50))\n",
      "(hid_init, (1, 50))\n",
      "(W_in_to_ingate, (40, 50))\n",
      "(W_hid_to_ingate, (50, 50))\n",
      "(b_ingate, (50,))\n",
      "(W_in_to_forgetgate, (40, 50))\n",
      "(W_hid_to_forgetgate, (50, 50))\n",
      "(b_forgetgate, (50,))\n",
      "(W_in_to_cell, (40, 50))\n",
      "(W_hid_to_cell, (50, 50))\n",
      "(b_cell, (50,))\n",
      "(W_in_to_outgate, (40, 50))\n",
      "(W_hid_to_outgate, (50, 50))\n",
      "(b_outgate, (50,))\n",
      "(W_cell_to_ingate, (50,))\n",
      "(W_cell_to_forgetgate, (50,))\n",
      "(W_cell_to_outgate, (50,))\n",
      "(cell_init, (1, 50))\n",
      "(hid_init, (1, 50))\n",
      "(W, (50, 194))\n",
      "(b, (194,))\n",
      "(W, (50, 30))\n",
      "(b, (30,))\n",
      "----------------------------------------\n",
      "Starting training...\n",
      "Epoch 1 of 3 took 10.468s\n",
      "  training loss:\t\t8.708508\n",
      "  training accuracy (speaker):\t\t0.63 %\n",
      "  training accuracy (phrase):\t\t5.18 %\n",
      "  validation loss:\t\t8.623956\n",
      "  validation accuracy (speaker):\t\t0.00 %\n",
      "  validation accuracy (phrase):\t\t6.25 %\n",
      "Epoch 2 of 3 took 10.420s\n",
      "  training loss:\t\t8.518374\n",
      "  training accuracy (speaker):\t\t0.90 %\n",
      "  training accuracy (phrase):\t\t12.23 %\n",
      "  validation loss:\t\t8.536782\n",
      "  validation accuracy (speaker):\t\t0.00 %\n",
      "  validation accuracy (phrase):\t\t7.81 %\n",
      "Epoch 3 of 3 took 10.428s\n",
      "  training loss:\t\t8.379722\n",
      "  training accuracy (speaker):\t\t1.41 %\n",
      "  training accuracy (phrase):\t\t19.88 %\n",
      "  validation loss:\t\t8.453432\n",
      "  validation accuracy (speaker):\t\t0.00 %\n",
      "  validation accuracy (phrase):\t\t12.50 %\n",
      "Saving Model ...\n"
     ]
    }
   ],
   "source": [
    "run rsr_multitask.py config/rsr_multitask.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
